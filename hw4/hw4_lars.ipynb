{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Applied ML\n",
    "\n",
    "\n",
    "### Loading the data\n",
    "First we load the data and vectorize it.\n",
    "The library functions contained in sklearn make this very straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we use the builtin function for loading data\n",
    "# sklearn already has a split in train/test, you can specify which data you want with the \"subset\" parameter\n",
    "# since we will perform that split ourselves, we load all data\n",
    "# we also remove all metadata\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the newsgroups are an sklearn \"bunch\"\n",
    "# it resembles a dictionary\n",
    "newsgroups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data contains the text for each article\n",
    "newsgroups.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many articles we have\n",
    "len(newsgroups.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# this will create a vector for every article\n",
    "# the output is a matrix\n",
    "matrix = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english').fit_transform(newsgroups.data)\n",
    "\n",
    "print(type(matrix))\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "\n",
    "We split the data into separate sets for training, testing and evaluating.\n",
    "Following the usual naming convention in machine learning, we call these datasets\n",
    "\n",
    " - X_train, y_train\n",
    " - X_test, y_test\n",
    " - X_val, y_val\n",
    " \n",
    "where X is the data and y contains the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# renaming\n",
    "X = matrix\n",
    "y = newsgroups.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now we do the split into train, test, val\n",
    "# it's 0.8, 0.1, 0.1\n",
    "\n",
    "num_samples = len(y)\n",
    "num_train = int(0.8 * num_samples)\n",
    "num_test = int(0.1 * num_samples)\n",
    "num_val = int(0.1 * num_samples)\n",
    "\n",
    "X_train = X[:num_train]\n",
    "X_test = X[num_train : -num_val]\n",
    "X_val = X[-num_val:]\n",
    "\n",
    "\n",
    "y_train = y[:num_train]\n",
    "y_test = y[num_train : -num_val]\n",
    "y_val = y[-num_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know whether the data is ordered\n",
    "# it would be possible that we have all articles from category 1 first, then category 2, etc\n",
    "# this would mean that our split is broken\n",
    "# we take a look at the labels\n",
    "# the scatterplot shows that there is no order, the labels don't increase linearly\n",
    "plt.figure(figsize=(10,1))\n",
    "plt.scatter(np.arange(len(y)), y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a RandomForest\n",
    "\n",
    "Before we start the grid search, we fit a random forest, to see how the syntax looks.\n",
    "\n",
    "We also want to check whether the classifier actually works, or if we have made some mistake. There are 20 categories. If the classifier is just guessing randomly, we would see an accuracy of about 1/20 = 5%. If the classifier does better than that, it is able to \"learn\" the data. That would mean we can continue to search for a good parametrization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=10)\n",
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the classifier to predict labels for the test set\n",
    "random_forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is already rather good. And this is for top-1 results. It would probably do much better if we would compute something like the top-2 or top-3 accuracy.\n",
    "\n",
    "The number of estimators is 10 by default. I would like to get the max depth that was used in this tree. Then we can use these parameters as the center of our grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately, we are apparently not supposed to read the depth\n",
    "# it is necessary to use properties with _ in their name\n",
    "depths = [estimator.tree_.max_depth for estimator in random_forest.estimators_]\n",
    "\n",
    "\n",
    "max_depth = max(depths)\n",
    "avg_depth = sum(depths)/len(depths)\n",
    "\n",
    "print(\"max depth is: \", max_depth)\n",
    "print(\"avg depth is: \", avg_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch, 1st approach\n",
    "\n",
    "We use the parameters given above to set up our gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = np.arange(10) + 5\n",
    "max_depths = 7 * np.logspace(1, 2, num=5, dtype=np.int)\n",
    "\n",
    "print(\"num estimators: \", n_estimators)\n",
    "print(\"max depths: \", max_depths)\n",
    "\n",
    "# we will use multithreading to process the grid on multiple cpus\n",
    "# the load balancing of sklearn is not very complex\n",
    "# it just splits the list and then collects the results\n",
    "# but the parametrizations for more estimators / depths take much longer\n",
    "# so we shuffle, as a simple load balancing\n",
    "np.random.shuffle(n_estimators)\n",
    "np.random.shuffle(max_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_indices = [-1] * X_train.shape[0]\n",
    "test_indices = [0] * X_test.shape[0]\n",
    "indices = train_indices + test_indices\n",
    "\n",
    "X_joint = vstack([X_train, X_test])\n",
    "y_joint = np.concatenate([y_train, y_test])\n",
    "\n",
    "pds = PredefinedSplit(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "rerun = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rerun:\n",
    "    \n",
    "    rfc = RandomForestClassifier()\n",
    "    clf_grid1 = GridSearchCV(rfc, param_grid={'n_estimators':n_estimators, 'max_depth':max_depths}, cv=pds, n_jobs=-1)\n",
    "    clf_grid1.fit(X_joint, y_joint)\n",
    "    \n",
    "    with open(\"data/clf_grid.pickle\", \"wb\") as file:\n",
    "        pickle.dump(clf_grid1, file)\n",
    "else:\n",
    "    with open(\"data/clf_grid.pickle\", \"rb\") as file:\n",
    "        clf_grid1 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussing results, for 1st GridSearch\n",
    "\n",
    "The results on both the training and the test set are very good. We reach well above 90%.\n",
    "\n",
    "But checking against the validation set shows that the results are not generalizable. We are not only overfitting the training data, the cross-validation of the parameters also resulted in training against the test set. We do note an improvement compared to the first evaluation, though.\n",
    "\n",
    "The overfitting on the test set is so strong that we need to question whether our split is actually working. But it seems to be set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on the train and test set, our classifier performs very well\n",
    "print(\"train score: \", clf_grid1.score(X_train, y_train))\n",
    "print(\"test score: \", clf_grid1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but on the val set, we perform badly\n",
    "print(\"test score: \", clf_grid1.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the predefined splot\n",
    "# it generates 1 set of train/test, that is good\n",
    "print(\"the split generates {} splits into train and test set\".format(pds.get_n_splits()))\n",
    "\n",
    "# let's check the indices\n",
    "# they also look good\n",
    "train, test = next(pds.split())\n",
    "print(\"the training indices go from {} to {}\".format(train.min(), train.max()))\n",
    "print(\"the testing indices go from {} to {}\".format(test.min(), test.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch, 2nd approach\n",
    "Our gridsearch is strongly overfitting.\n",
    "\n",
    "Overfitting is usually due to too high model complexity. So we start a second gridsearch with reduced complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators2 = n_estimators = np.arange(10) + 5\n",
    "max_depths2 = 2 * np.logspace(1, 2, num=5, dtype=np.int)\n",
    "\n",
    "\n",
    "print(\"num estimators: \", n_estimators2)\n",
    "print(\"max depths: \", max_depths2)\n",
    "\n",
    "np.random.shuffle(n_estimators2)\n",
    "np.random.shuffle(max_depths2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rerun:\n",
    "    \n",
    "    rfc = RandomForestClassifier()\n",
    "    clf_grid2 = GridSearchCV(rfc, param_grid={'n_estimators':n_estimators2, 'max_depth':max_depths2}, cv=pds, n_jobs=-1)\n",
    "    clf_grid2.fit(X_joint, y_joint)\n",
    "    \n",
    "    with open(\"data/clf_grid2.pickle\", \"wb\") as file:\n",
    "        pickle.dump(clf_grid2, file)\n",
    "else:\n",
    "    with open(\"data/clf_grid2.pickle\", \"rb\") as file:\n",
    "        clf_grid2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussing results for the 2nd gridsearch\n",
    "\n",
    "The performance is very similar to the first gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on the train and test set, our classifier performs very well\n",
    "print(\"train score: \", clf_grid2.score(X_train, y_train))\n",
    "print(\"test score: \", clf_grid2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but on the val set, we perform badly\n",
    "print(\"test score: \", clf_grid2.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting the baseline\n",
    "Between the first and second gridsearch, we changed the maximum depth. Which had only very little effect on the performance.\n",
    "\n",
    "We could also change the number of estimators, but this has a strong effect on the complexity and increases the time for training drastically.\n",
    "\n",
    "Therefore, we do a simple check first: We train just one random forest, with a higher number of estimators and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(n_estimators=100)\n",
    "rf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [estimator.tree_.max_depth for estimator in rf2.estimators_]\n",
    "\n",
    "\n",
    "max_depth = max(depths)\n",
    "avg_depth = sum(depths)/len(depths)\n",
    "\n",
    "print(\"max depth is: \", max_depth)\n",
    "print(\"avg depth is: \", avg_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch for the 3rd time\n",
    "\n",
    "Increasing the number of estimators improves both the training and the validation accuracy.\n",
    "So we need to perform a gridsearch for higher amounts of estimators.\n",
    "This will need a lot of time.\n",
    "We set the gridsearch up to sample many configurations and let it run over night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators3 = (40 * np.logspace(0, 1.5, num=20)).astype(np.int)\n",
    "max_depths3 = 250 + (np.arange(10)*50)\n",
    "\n",
    "print(\"num estimators: \", n_estimators3)\n",
    "print(\"max depths: \", max_depths3)\n",
    "\n",
    "np.random.shuffle(n_estimators3)\n",
    "np.random.shuffle(max_depths3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rerun:\n",
    "    \n",
    "    rfc = RandomForestClassifier()\n",
    "    clf_grid3 = GridSearchCV(rfc, param_grid={'n_estimators':n_estimators3, 'max_depth':max_depths3}, cv=pds, n_jobs=-1)\n",
    "    clf_grid3.fit(X_joint, y_joint)\n",
    "    \n",
    "    with open(\"data/clf_grid3.pickle\", \"wb\") as file:\n",
    "        pickle.dump(clf_grid3, file)\n",
    "else:\n",
    "    with open(\"data/clf_grid3.pickle\", \"rb\") as file:\n",
    "        clf_grid3 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
