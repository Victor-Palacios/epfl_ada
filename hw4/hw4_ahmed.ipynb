{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline\n",
    "\n",
    "Wednesday, November 22, 2017, 11:59PM\n",
    "\n",
    "## Important notes\n",
    "\n",
    "- When you push your Notebook to GitHub, all the cells must already have been evaluated.\n",
    "- Don't forget to add a textual description of your thought process and of any assumptions you've made.\n",
    "- Please write all your comments in English, and use meaningful variable names in your code.\n",
    "\n",
    "## Question 1: Propensity score matching\n",
    "\n",
    "In this exercise, you will apply [propensity score matching](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf), which we discussed in lecture 5 (\"Observational studies\"), in order to draw conclusions from an observational study.\n",
    "\n",
    "We will work with a by-now classic dataset from Robert LaLonde's study \"[Evaluating the Econometric Evaluations of Training Programs](http://people.hbs.edu/nashraf/LaLonde_1986.pdf)\" (1986).\n",
    "The study investigated the effect of a job training program (\"National Supported Work Demonstration\") on the real earnings of an individual, a couple of years after completion of the program.\n",
    "Your task is to determine the effectiveness of the \"treatment\" represented by the job training program.\n",
    "\n",
    "#### Dataset description\n",
    "\n",
    "- `treat`: 1 if the subject participated in the job training program, 0 otherwise\n",
    "- `age`: the subject's age\n",
    "- `educ`: years of education\n",
    "- `race`: categorical variable with three possible values: Black, Hispanic, or White\n",
    "- `married`: 1 if the subject was married at the time of the training program, 0 otherwise\n",
    "- `nodegree`: 1 if the subject has earned no school degree, 0 otherwise\n",
    "- `re74`: real earnings in 1974 (pre-treatment)\n",
    "- `re75`: real earnings in 1975 (pre-treatment)\n",
    "- `re78`: real earnings in 1978 (outcome)\n",
    "\n",
    "If you want to brush up your knowledge on propensity scores and observational studies, we highly recommend Rosenbaum's excellent book on the [\"Design of Observational Studies\"](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf). Even just reading the first chapter (18 pages) will help you a lot.\n",
    "\n",
    "#### 1. A naive analysis\n",
    "\n",
    "Compare the distribution of the outcome variable (`re78`) between the two groups, using plots and numbers.\n",
    "To summarize and compare the distributions, you may use the techniques we discussed in lectures 4 (\"Read the stats carefully\") and 6 (\"Data visualization\").\n",
    "\n",
    "What might a naive \"researcher\" conclude from this superficial analysis?\n",
    "\n",
    "#### 2. A closer look at the data\n",
    "\n",
    "You're not naive, of course (and even if you are, you've learned certain things in ADA), so you aren't content with a superficial analysis such as the above.\n",
    "You're aware of the dangers of observational studies, so you take a closer look at the data before jumping to conclusions.\n",
    "\n",
    "For each feature in the dataset, compare its distribution in the treated group with its distribution in the control group, using plots and numbers.\n",
    "As above, you may use the techniques we discussed in class for summarizing and comparing the distributions.\n",
    "\n",
    "What do you observe?\n",
    "Describe what your observations mean for the conclusions drawn by the naive \"researcher\" from his superficial analysis.\n",
    "\n",
    "#### 3. A propsensity score model\n",
    "\n",
    "Use logistic regression to estimate propensity scores for all points in the dataset.\n",
    "You may use `sklearn` to fit the logistic regression model and apply it to each data point to obtain propensity scores:\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "```\n",
    "\n",
    "Recall that the propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features (in this case, age, education, pre-treatment income, etc.).\n",
    "To brush up on propensity scores, you may read chapter 3.3 of the above-cited book by Rosenbaum or [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).\n",
    "\n",
    "Note: you do not need a train/test split here. Train and apply the model on the entire dataset. If you're wondering why this is the right thing to do in this situation, recall that the propensity score model is not used in order to make predictions about unseen data. Its sole purpose is to balance the dataset across treatment groups.\n",
    "(See p. 74 of Rosenbaum's book for an explanation why slight overfitting is even good for propensity scores.\n",
    "If you want even more information, read [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).)\n",
    "\n",
    "#### 4. Balancing the dataset via matching\n",
    "\n",
    "Use the propensity scores to match each data point from the treated group with exactly one data point from the control group, while ensuring that each data point from the control group is matched with at most one data point from the treated group.\n",
    "(Hint: you may explore the `networkx` package in Python for predefined matching functions.)\n",
    "\n",
    "Your matching should maximize the similarity between matched subjects, as captured by their propensity scores.\n",
    "In other words, the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects should be minimized.\n",
    "\n",
    "After matching, you have as many treated as you have control subjects.\n",
    "Compare the outcomes (`re78`) between the two groups (treated and control).\n",
    "\n",
    "Also, compare again the feature-value distributions between the two groups, as you've done in part 2 above, but now only for the matched subjects.\n",
    "What do you observe?\n",
    "Are you closer to being able to draw valid conclusions now than you were before?\n",
    "\n",
    "\n",
    "#### 5. Balancing the groups further\n",
    "\n",
    "Based on your comparison of feature-value distributions from part 4, are you fully satisfied with your matching?\n",
    "Would you say your dataset is sufficiently balanced?\n",
    "If not, in what ways could the \"balanced\" dataset you have obtained still not allow you to draw valid conclusions?\n",
    "\n",
    "Improve your matching by explicitly making sure that you match only subjects that have the same value for the problematic feature.\n",
    "Argue with numbers and plots that the two groups (treated and control) are now better balanced than after part 4.\n",
    "\n",
    "\n",
    "#### 6. A less naive analysis\n",
    "\n",
    "Compare the outcomes (`re78`) between treated and control subjects, as you've done in part 1, but now only for the matched dataset you've obtained from part 5.\n",
    "What do you conclude about the effectiveness of the job training program?\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## Question 2: Applied ML\n",
    "\n",
    "We are going to build a classifier of news to directly assign them to 20 news categories. Note that the pipeline that you will build in this exercise could be of great help during your project if you plan to work with text!\n",
    "\n",
    "1. Load the 20newsgroup dataset. It is, again, a classic dataset that can directly be loaded using sklearn ([link](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)).  \n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequencyâ€“inverse document frequency, is of great help when if comes to compute textual features. Indeed, it gives more importance to terms that are more specific to the considered articles (TF) but reduces the importance of terms that are very frequent in the entire corpus (IDF). Compute TF-IDF features for every article using [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Then, split your dataset into a training, a testing and a validation set (10% for validation and 10% for testing). Each observation should be paired with its corresponding label (the article category).\n",
    "\n",
    "\n",
    "2. Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSW1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9930.0460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSW2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3595.8940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NSW3</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24909.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NSW4</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7506.1460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NSW5</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.7899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  treat  age  educ  black  hispan  married  nodegree  re74  re75  \\\n",
       "0  NSW1      1   37    11      1       0        1         1   0.0   0.0   \n",
       "1  NSW2      1   22     9      0       1        0         1   0.0   0.0   \n",
       "2  NSW3      1   30    12      1       0        0         0   0.0   0.0   \n",
       "3  NSW4      1   27    11      1       0        0         1   0.0   0.0   \n",
       "4  NSW5      1   33     8      1       0        0         1   0.0   0.0   \n",
       "\n",
       "         re78  \n",
       "0   9930.0460  \n",
       "1   3595.8940  \n",
       "2  24909.4500  \n",
       "3   7506.1460  \n",
       "4    289.7899  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde = pd.read_csv(r'lalonde.csv')\n",
    "data_lalonde.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's do a little describe to see what we have here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.301303</td>\n",
       "      <td>27.363192</td>\n",
       "      <td>10.268730</td>\n",
       "      <td>0.395765</td>\n",
       "      <td>0.117264</td>\n",
       "      <td>0.415309</td>\n",
       "      <td>0.630293</td>\n",
       "      <td>4557.546569</td>\n",
       "      <td>2184.938207</td>\n",
       "      <td>6792.834483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.459198</td>\n",
       "      <td>9.881187</td>\n",
       "      <td>2.628325</td>\n",
       "      <td>0.489413</td>\n",
       "      <td>0.321997</td>\n",
       "      <td>0.493177</td>\n",
       "      <td>0.483119</td>\n",
       "      <td>6477.964479</td>\n",
       "      <td>3295.679043</td>\n",
       "      <td>7470.730792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>238.283425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1042.330000</td>\n",
       "      <td>601.548400</td>\n",
       "      <td>4759.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7888.498250</td>\n",
       "      <td>3248.987500</td>\n",
       "      <td>10893.592500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35040.070000</td>\n",
       "      <td>25142.240000</td>\n",
       "      <td>60307.930000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            treat         age        educ       black      hispan     married  \\\n",
       "count  614.000000  614.000000  614.000000  614.000000  614.000000  614.000000   \n",
       "mean     0.301303   27.363192   10.268730    0.395765    0.117264    0.415309   \n",
       "std      0.459198    9.881187    2.628325    0.489413    0.321997    0.493177   \n",
       "min      0.000000   16.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000   20.000000    9.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000   25.000000   11.000000    0.000000    0.000000    0.000000   \n",
       "75%      1.000000   32.000000   12.000000    1.000000    0.000000    1.000000   \n",
       "max      1.000000   55.000000   18.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree          re74          re75          re78  \n",
       "count  614.000000    614.000000    614.000000    614.000000  \n",
       "mean     0.630293   4557.546569   2184.938207   6792.834483  \n",
       "std      0.483119   6477.964479   3295.679043   7470.730792  \n",
       "min      0.000000      0.000000      0.000000      0.000000  \n",
       "25%      0.000000      0.000000      0.000000    238.283425  \n",
       "50%      1.000000   1042.330000    601.548400   4759.018500  \n",
       "75%      1.000000   7888.498250   3248.987500  10893.592500  \n",
       "max      1.000000  35040.070000  25142.240000  60307.930000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing really special to say here. These value do not mean much by themselves. They just show some basic information. Let's answer question 1 now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_treat = data_lalonde[\"treat\"] == 1\n",
    "index_no_treat = data_lalonde[\"treat\"] == 0\n",
    "data_lalonde_treat = data_lalonde[index_treat]\n",
    "data_lalonde_no_treat = data_lalonde[index_no_treat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just checking of there are no \"outliers\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde_treat.shape[0] + data_lalonde_no_treat.shape[0] == data_lalonde.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1563958dd8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCAAAAIMCAYAAAAkUt0SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f15640dbcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (18, 9)\n",
    "plt.figure()\n",
    "\n",
    "data_lalonde_treat.re78.plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f156392e6a0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCYAAAIMCAYAAAApTK1VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHkFJREFUeJzt3Xuw7fVZ3/HPI0eTEDWGckTk4kHL6BCrMR4zaVNtFGOw\naMBOm5JRS21G7Ii31hmF1Cn+wwydeu8YR0zSoMZQjImhxRvBS6YzNeTkouEihTEkgXA5mla8DQh5\n+sf5kdk9OQc2sNd69uX1mjmz1u+71tr7Ocxv1sCb36W6OwAAAAATPm16AAAAAGDvEiYAAACAMcIE\nAAAAMEaYAAAAAMYIEwAAAMAYYQIAAAAYI0wAAAAAY4QJAAAAYIwwAQAAAIwRJgAAAIAx+6YHeCZO\nPvnkPnDgwPQYAAAAwFHe+973/ll373+y9+3oMHHgwIEcOnRoegwAAADgKFX14c28z6kcAAAAwBhh\nAgAAABgjTAAAAABjhAkAAABgjDABAAAAjBEmAAAAgDHCBAAAADBGmAAAAADGCBMAAADAGGECAAAA\nGCNMAAAAAGOECQAAAGCMMAEAAACMESYAAACAMSsLE1X1xqp6sKpuOcZrP1hVXVUnb1i7vKruqqo7\nquoVq5oLAAAA2D5WecTEm5Kcd/RiVZ2R5BuSfGTD2jlJLkryguUzr6uqE1Y4GwAAALANrCxMdPe7\nknz8GC/9ZJIfStIb1i5Icm13P9zdH0pyV5IXr2o2AAAAYHtY6zUmquqCJPd29x8d9dJpST66Yfue\nZQ0AAADYxfat6xdV1YlJXpsjp3E8k59zSZJLkuTMM8/cgskAAACAKes8YuKLkpyV5I+q6u4kpyd5\nX1V9XpJ7k5yx4b2nL2uforuv7u6D3X1w//79Kx4ZAAAAWKW1hYnu/mB3f253H+juAzlyusaLuvv+\nJNcnuaiqnlVVZyU5O8nN65oNAAAAmLGyUzmq6i1JXpbk5Kq6J8kV3f2GY723u2+tquuS3Jbk0SSX\ndvdjq5pt2oHLbpge4Wm5+6rzp0cAAABgl1lZmOjuVz/J6weO2r4yyZWrmgcAAADYftZ6Vw4AAACA\njYQJAAAAYIwwAQAAAIwRJgAAAIAxwgQAAAAwRpgAAAAAxggTAAAAwBhhAgAAABgjTAAAAABjhAkA\nAABgjDABAAAAjBEmAAAAgDHCBAAAADBGmAAAAADGCBMAAADAGGECAAAAGCNMAAAAAGOECQAAAGCM\nMAEAAACMESYAAACAMcIEAAAAMEaYAAAAAMYIEwAAAMAYYQIAAAAYI0wAAAAAY4QJAAAAYIwwAQAA\nAIwRJgAAAIAxwgQAAAAwRpgAAAAAxggTAAAAwBhhAgAAABgjTAAAAABjhAkAAABgjDABAAAAjBEm\nAAAAgDHCBAAAADBGmAAAAADGCBMAAADAGGECAAAAGCNMAAAAAGOECQAAAGCMMAEAAACMESYAAACA\nMcIEAAAAMEaYAAAAAMYIEwAAAMAYYQIAAAAYI0wAAAAAY4QJAAAAYIwwAQAAAIwRJgAAAIAxwgQA\nAAAwRpgAAAAAxggTAAAAwBhhAgAAABgjTAAAAABjhAkAAABgzMrCRFW9saoerKpbNqz956r6k6r6\n46p6e1V9zobXLq+qu6rqjqp6xarmAgAAALaPVR4x8aYk5x21dmOSL+3uL0vyv5NcniRVdU6Si5K8\nYPnM66rqhBXOBgAAAGwDKwsT3f2uJB8/au13uvvRZfMPk5y+PL8gybXd/XB3fyjJXUlevKrZAAAA\ngO1h8hoT/ybJby7PT0vy0Q2v3bOsAQAAALvYSJioqv+Q5NEkb34an72kqg5V1aHDhw9v/XAAAADA\n2qw9TFTVv07yTUm+tbt7Wb43yRkb3nb6svYpuvvq7j7Y3Qf379+/0lkBAACA1VprmKiq85L8UJJX\ndvffbHjp+iQXVdWzquqsJGcnuXmdswEAAADrt29VP7iq3pLkZUlOrqp7klyRI3fheFaSG6sqSf6w\nu/9td99aVdcluS1HTvG4tLsfW9VsAAAAwPawsjDR3a8+xvIbnuD9Vya5clXzAAAAANvP5F05AAAA\ngD1OmAAAAADGCBMAAADAGGECAAAAGCNMAAAAAGOECQAAAGCMMAEAAACMESYAAACAMcIEAAAAMEaY\nAAAAAMYIEwAAAMAYYQIAAAAYI0wAAAAAY4QJAAAAYIwwAQAAAIwRJgAAAIAxwgQAAAAwRpgAAAAA\nxggTAAAAwBhhAgAAABgjTAAAAABjhAkAAABgjDABAAAAjBEmAAAAgDHCBAAAADBGmAAAAADGCBMA\nAADAGGECAAAAGCNMAAAAAGOECQAAAGCMMAEAAACMESYAAACAMcIEAAAAMEaYAAAAAMYIEwAAAMAY\nYQIAAAAYI0wAAAAAY4QJAAAAYIwwAQAAAIwRJgAAAIAxwgQAAAAwRpgAAAAAxggTAAAAwBhhAgAA\nABgjTAAAAABjhAkAAABgjDABAAAAjBEmAAAAgDHCBAAAADBGmAAAAADGCBMAAADAGGECAAAAGCNM\nAAAAAGOECQAAAGCMMAEAAACMESYAAACAMcIEAAAAMEaYAAAAAMasLExU1Rur6sGqumXD2klVdWNV\n3bk8Pn/Da5dX1V1VdUdVvWJVcwEAAADbxyqPmHhTkvOOWrssyU3dfXaSm5btVNU5SS5K8oLlM6+r\nqhNWOBsAAACwDawsTHT3u5J8/KjlC5Jcszy/JsmFG9av7e6Hu/tDSe5K8uJVzQYAAABsD+u+xsQp\n3X3f8vz+JKcsz09L8tEN77tnWQMAAAB2sbGLX3Z3J+mn+rmquqSqDlXVocOHD69gMgAAAGBd1h0m\nHqiqU5NkeXxwWb83yRkb3nf6svYpuvvq7j7Y3Qf379+/0mEBAACA1Vp3mLg+ycXL84uTvGPD+kVV\n9ayqOivJ2UluXvNsAAAAwJrtW9UPrqq3JHlZkpOr6p4kVyS5Ksl1VfWaJB9O8qok6e5bq+q6JLcl\neTTJpd392KpmAwAAALaHlYWJ7n71cV469zjvvzLJlauaBwAAANh+xi5+CQAAACBMAAAAAGOECQAA\nAGCMMAEAAACMESYAAACAMcIEAAAAMEaYAAAAAMYIEwAAAMAYYQIAAAAYI0wAAAAAY4QJAAAAYIww\nAQAAAIwRJgAAAIAxwgQAAAAwRpgAAAAAxggTAAAAwBhhAgAAABgjTAAAAABjhAkAAABgjDABAAAA\njBEmAAAAgDHCBAAAADBGmAAAAADGCBMAAADAGGECAAAAGCNMAAAAAGOECQAAAGCMMAEAAACMESYA\nAACAMcIEAAAAMEaYAAAAAMYIEwAAAMAYYQIAAAAYI0wAAAAAY4QJAAAAYIwwAQAAAIwRJgAAAIAx\nwgQAAAAwRpgAAAAAxggTAAAAwBhhAgAAABgjTAAAAABjhAkAAABgjDABAAAAjBEmAAAAgDHCBAAA\nADBGmAAAAADGCBMAAADAGGECAAAAGCNMAAAAAGOECQAAAGCMMAEAAACMESYAAACAMcIEAAAAMEaY\nAAAAAMYIEwAAAMAYYQIAAAAYI0wAAAAAY0bCRFX9u6q6tapuqaq3VNWzq+qkqrqxqu5cHp8/MRsA\nAACwPmsPE1V1WpLvS3Kwu780yQlJLkpyWZKbuvvsJDct2wAAAMAuNnUqx74kz6mqfUlOTPKxJBck\nuWZ5/ZokFw7NBgAAAKzJpsJEVf2DrfqF3X1vkh9L8pEk9yX5i+7+nSSndPd9y9vuT3LKVv1OAAAA\nYHva7BETr6uqm6vqu6vqec/kFy7XjrggyVlJPj/Jc6vq2za+p7s7SR/n85dU1aGqOnT48OFnMgoA\nAAAwbFNhoru/Osm3JjkjyXur6leq6uVP83d+fZIPdffh7v67JG9L8o+SPFBVpybJ8vjgcWa5ursP\ndvfB/fv3P80RAAAAgO1g09eY6O47k/xIkh9O8k+S/ExV/UlV/bOn+Ds/kuQlVXViVVWSc5PcnuT6\nJBcv77k4yTue4s8FAAAAdph9m3lTVX1Zku9Icn6SG5N8c3e/r6o+P8n/ypGjHjalu99dVW9N8r4k\njyZ5f5Krk3xmkuuq6jVJPpzkVU/lLwIAAADsPJsKE0n+S5LXJ3ltd//t44vd/bGq+pGn+ku7+4ok\nVxy1/HCOHD0BAAAA7BGbDRPnJ/nb7n4sSarq05I8u7v/prt/aWXTAQAAALvaZq8x8c4kz9mwfeKy\nBgAAAPC0bTZMPLu7/+rxjeX5iasZCQAAANgrNhsm/rqqXvT4RlV9ZZK/fYL3AwAAADypzV5j4geS\n/GpVfSxJJfm8JP9yZVMBAAAAe8KmwkR3v6eqviTJFy9Ld3T3361uLAAAAGAv2OwRE0nyVUkOLJ95\nUVWlu39xJVMBAAAAe8KmwkRV/VKSL0rygSSPLcudRJgAAAAAnrbNHjFxMMk53d2rHAYAAADYWzZ7\nV45bcuSClwAAAABbZrNHTJyc5LaqujnJw48vdvcrVzIVAAAAsCdsNkz86CqHAAAAAPamzd4u9A+q\n6guSnN3d76yqE5OcsNrRAAAAgN1uU9eYqKrvTPLWJD+/LJ2W5NdXNRQAAACwN2z24peXJnlpkoeS\npLvvTPK5qxoKAAAA2Bs2GyYe7u5HHt+oqn1J3DoUAAAAeEY2Gyb+oKpem+Q5VfXyJL+a5L+vbiwA\nAABgL9hsmLgsyeEkH0zyXUl+I8mPrGooAAAAYG/Y7F05PpHkF5Y/AAAAAFtiU2Giqj6UY1xToru/\ncMsnAgAAAPaMTYWJJAc3PH92kn+R5KStHwcAAADYSzZ1jYnu/vMNf+7t7p9Kcv6KZwMAAAB2uc2e\nyvGiDZufliNHUGz2aAsAAACAY9psXPjxDc8fTXJ3kldt+TQAAADAnrLZu3J87aoHAQAAAPaezZ7K\n8e+f6PXu/omtGQcAAADYS57KXTm+Ksn1y/Y3J7k5yZ2rGAoAAADYGzYbJk5P8qLu/sskqaofTXJD\nd3/bqgYDAAAAdr9N3S40ySlJHtmw/ciyBgAAAPC0bfaIiV9McnNVvX3ZvjDJNasZCQAAANgrNntX\njiur6jeTfPWy9B3d/f7VjQUAAADsBZs9lSNJTkzyUHf/dJJ7quqsFc0EAAAA7BGbChNVdUWSH05y\n+bL06Ul+eVVDAQAAAHvDZo+Y+JYkr0zy10nS3R9L8lmrGgoAAADYGzYbJh7p7k7SSVJVz13dSAAA\nAMBesdkwcV1V/XySz6mq70zyziS/sLqxAAAAgL1gs3fl+LGqenmSh5J8cZL/2N03rnQyAAAAYNd7\n0jBRVSckeWd3f20SMQIAAADYMk96Kkd3P5bkE1X1vDXMAwAAAOwhmzqVI8lfJflgVd2Y5c4cSdLd\n37eSqQAAAIA9YbNh4m3LHwAAAIAt84RhoqrO7O6PdPc16xoIAAAA2Due7BoTv/74k6r6tRXPAgAA\nAOwxTxYmasPzL1zlIAAAAMDe82Rhoo/zHAAAAOAZe7KLX355VT2UI0dOPGd5nmW7u/uzVzodAAAA\nsKs9YZjo7hPWNQgAAACw9zzZqRwAAAAAKyNMAAAAAGOECQAAAGCMMAEAAACMESYAAACAMcIEAAAA\nMEaYAAAAAMYIEwAAAMAYYQIAAAAYI0wAAAAAY4QJAAAAYMxImKiqz6mqt1bVn1TV7VX1D6vqpKq6\nsaruXB6fPzEbAAAAsD5TR0z8dJLf6u4vSfLlSW5PclmSm7r77CQ3LdsAAADALrb2MFFVz0vyNUne\nkCTd/Uh3/98kFyS5ZnnbNUkuXPdsAAAAwHpNHDFxVpLDSf5rVb2/ql5fVc9Nckp337e85/4kpwzM\nBgAAAKzRRJjYl+RFSX6uu78iyV/nqNM2uruT9LE+XFWXVNWhqjp0+PDhlQ8LAAAArM5EmLgnyT3d\n/e5l+605EioeqKpTk2R5fPBYH+7uq7v7YHcf3L9//1oGBgAAAFZj7WGiu+9P8tGq+uJl6dwktyW5\nPsnFy9rFSd6x7tkAAACA9do39Hu/N8mbq+ozkvxpku/IkUhyXVW9JsmHk7xqaDYAAABgTUbCRHd/\nIMnBY7x07rpnAQAAAOZMXGMCAAAAIIkwAQAAAAwSJgAAAIAxwgQAAAAwRpgAAAAAxggTAAAAwBhh\nAgAAABgjTAAAAABjhAkAAABgjDABAAAAjBEmAAAAgDHCBAAAADBGmAAAAADGCBMAAADAGGECAAAA\nGCNMAAAAAGOECQAAAGCMMAEAAACMESYAAACAMcIEAAAAMEaYAAAAAMYIEwAAAMAYYQIAAAAYI0wA\nAAAAY4QJAAAAYIwwAQAAAIwRJgAAAIAxwgQAAAAwRpgAAAAAxggTAAAAwBhhAgAAABgjTAAAAABj\nhAkAAABgjDABAAAAjBEmAAAAgDHCBAAAADBGmAAAAADGCBMAAADAGGECAAAAGCNMAAAAAGOECQAA\nAGCMMAEAAACMESYAAACAMcIEAAAAMEaYAAAAAMYIEwAAAMAYYQIAAAAYI0wAAAAAY4QJAAAAYIww\nAQAAAIwRJgAAAIAxwgQAAAAwRpgAAAAAxggTAAAAwBhhAgAAABgjTAAAAABjhAkAAABgjDABAAAA\njBkLE1V1QlW9v6r+x7J9UlXdWFV3Lo/Pn5oNAAAAWI/JIya+P8ntG7YvS3JTd5+d5KZlGwAAANjF\nRsJEVZ2e5Pwkr9+wfEGSa5bn1yS5cN1zAQAAAOs1dcTETyX5oSSf2LB2Snfftzy/P8kpa58KAAAA\nWKu1h4mq+qYkD3b3e4/3nu7uJH2cz19SVYeq6tDhw4dXNSYAAACwBhNHTLw0ySur6u4k1yb5uqr6\n5SQPVNWpSbI8PnisD3f31d19sLsP7t+/f10zAwAAACuw9jDR3Zd39+ndfSDJRUl+t7u/Lcn1SS5e\n3nZxknesezYAAABgvSbvynG0q5K8vKruTPL1yzYAAACwi+2b/OXd/ftJfn95/udJzp2cBwAAAFiv\n7XTEBAAAALDHCBMAAADAGGECAAAAGCNMAAAAAGOECQAAAGCMMAEAAACMESYAAACAMcIEAAAAMEaY\nAAAAAMYIEwAAAMAYYQIAAAAYI0wAAAAAY4QJAAAAYIwwAQAAAIwRJgAAAIAxwgQAAAAwRpgAAAAA\nxggTAAAAwBhhAgAAABgjTAAAAABjhAkAAABgjDABAAAAjNk3PQA7x4HLbpge4Sm7+6rzp0cAAADg\nCThiAgAAABgjTAAAAABjhAkAAABgjDABAAAAjBEmAAAAgDHCBAAAADBGmAAAAADGCBMAAADAGGEC\nAAAAGCNMAAAAAGOECQAAAGCMMAEAAACM2Tc9AMCkA5fdMD3C03L3VedPjwAAAFvCERMAAADAGGEC\nAAAAGONUDmDL7NTTIgAAgDmOmAAAAADGCBMAAADAGGECAAAAGCNMAAAAAGOECQAAAGCMMAEAAACM\ncbtQdrWdevvKu686f3oEAACAtXDEBAAAADBGmAAAAADGCBMAAADAGGECAAAAGCNMAAAAAGOECQAA\nAGCMMAEAAACMESYAAACAMcIEAAAAMEaYAAAAAMYIEwAAAMAYYQIAAAAYs/YwUVVnVNXvVdVtVXVr\nVX3/sn5SVd1YVXcuj89f92wAAADAek0cMfFokh/s7nOSvCTJpVV1TpLLktzU3WcnuWnZBgAAAHax\ntYeJ7r6vu9+3PP/LJLcnOS3JBUmuWd52TZIL1z0bAAAAsF6j15ioqgNJviLJu5Oc0t33LS/dn+SU\nobEAAACANdk39Yur6jOT/FqSH+juh6rqk691d1dVH+dzlyS5JEnOPPPMdYwKa3fgshumRwAAAFiL\nkSMmqurTcyRKvLm737YsP1BVpy6vn5rkwWN9truv7u6D3X1w//796xkYAAAAWImJu3JUkjckub27\nf2LDS9cnuXh5fnGSd6x7NgAAAGC9Jk7leGmSb0/ywar6wLL22iRXJbmuql6T5MNJXjUwG8COsFNP\n97n7qvOnRwAAYJtZe5jo7v+ZpI7z8rnrnAUAAACYNXpXDgAAAGBvG7srBwCwOk73AQB2CkdMAAAA\nAGOECQAAAGCMMAEAAACMcY0JANZmJ173wDUPAGD32In/LpLs/n8fccQEAAAAMEaYAAAAAMYIEwAA\nAMAYYQIAAAAYI0wAAAAAY4QJAAAAYIwwAQAAAIwRJgAAAIAxwgQAAAAwZt/0AAAAO9mBy26YHmHP\nuPuq86dHAGAFHDEBAAAAjBEmAAAAgDFO5QAAgBXaqaf7OHUGWBdHTAAAAABjhAkAAABgjDABAAAA\njBEmAAAAgDHCBAAAADBGmAAAAADGuF0oADyBnXqbPwCAncIREwAAAMAYYQIAAAAYI0wAAAAAY4QJ\nAAAAYIwwAQAAAIwRJgAAAIAxbhcKAMCO4Pa967UT/3nffdX50yMAT4MjJgAAAIAxwgQAAAAwxqkc\nAMC2sRMPHQcAnhlHTAAAAABjhAkAAABgjDABAAAAjBEmAAAAgDHCBAAAADBGmAAAAADGCBMAAADA\nGGECAAAAGCNMAAAAAGP2TQ8AAACwlx247IbpEZ6Wu686f3oEdglHTAAAAABjhAkAAABgjDABAAAA\njBEmAAAAgDHCBAAAADBGmAAAAADGuF0oAACwK+zU227CXueICQAAAGCMMAEAAACMESYAAACAMcIE\nAAAAMEaYAAAAAMZsu7tyVNV5SX46yQlJXt/dVw2PBAAAwFHcBYWtsq2OmKiqE5L8bJJvTHJOkldX\n1TmzUwEAAACrsq3CRJIXJ7mru/+0ux9Jcm2SC4ZnAgAAAFZku4WJ05J8dMP2PcsaAAAAsAttu2tM\nPJmquiTJJcvmX1XVHZPzPE0nJ/mz6SFgC9iX2Q3sx+wW9mV2A/sxu8WW7sv1n7bqJ63dF2zmTdst\nTNyb5IwN26cva5/U3VcnuXqdQ221qjrU3Qen54Bnyr7MbmA/ZrewL7Mb2I/ZLezLT812O5XjPUnO\nrqqzquozklyU5PrhmQAAAIAV2VZHTHT3o1X1PUl+O0duF/rG7r51eCwAAABgRbZVmEiS7v6NJL8x\nPceK7ehTUWAD+zK7gf2Y3cK+zG5gP2a3sC8/BdXd0zMAAAAAe9R2u8YEAAAAsIcIE2tUVedV1R1V\ndVdVXTY9DxxLVd1dVR+sqg9U1aFl7aSqurGq7lwen7/h/Zcv+/QdVfWKDetfufycu6rqZ6qqJv4+\n7A1V9caqerCqbtmwtmX7bVU9q6r+27L+7qo6sM6/H3vHcfblH62qe5fv5Q9U1T/d8Jp9mW2nqs6o\nqt+rqtuq6taq+v5l3fcyO8oT7Mu+l7eYMLEmVXVCkp9N8o1Jzkny6qo6Z3YqOK6v7e4XbrjF0WVJ\nburus5PctGxn2YcvSvKCJOcled2yryfJzyX5ziRnL3/OW+P87D1vyqfuY1u5374myf/p7r+f5CeT\n7Ny7ibPdvSnH/r78yeV7+YXL9bjsy2xnjyb5we4+J8lLkly67K++l9lpjrcvJ76Xt5QwsT4vTnJX\nd/9pdz+S5NokFwzPBJt1QZJrlufXJLlww/q13f1wd38oyV1JXlxVpyb57O7+wz5yIZtf3PAZ2HLd\n/a4kHz9qeSv3240/661JznUUEKtwnH35eOzLbEvdfV93v295/pdJbk9yWnwvs8M8wb58PPblp0mY\nWJ/Tknx0w/Y9eeKdGqZ0kndW1Xur6pJl7ZTuvm95fn+SU5bnx9uvT1ueH70O67SV++0nP9Pdjyb5\niyR/bzVjwzF9b1X98XKqx+OHv9uX2faWw9K/Ism743uZHeyofTnxvbylhAngaP+4u1+YI6cdXVpV\nX7PxxaXyup0PO4r9lh3u55J8YZIXJrkvyY/PjgObU1WfmeTXkvxAdz+08TXfy+wkx9iXfS9vMWFi\nfe5NcsaG7dOXNdhWuvve5fHBJG/PkdOQHlgOQcvy+ODy9uPt1/cuz49eh3Xayv32k5+pqn1Jnpfk\nz1c2OWzQ3Q9092Pd/Ykkv5Aj38uJfZltrKo+PUf+Q+7N3f22Zdn3MjvOsfZl38tbT5hYn/ckObuq\nzqqqz8iRi6JcPzwT/H+q6rlV9VmPP0/yDUluyZF99eLlbRcnecfy/PokFy1XEz4rRy7kc/NymOZD\nVfWS5Ry5f7XhM7AuW7nfbvxZ/zzJ7y7/tw9W7vH/kFt8S458Lyf2ZbapZb97Q5Lbu/snNrzke5kd\n5Xj7su/lrbdveoC9orsfrarvSfLbSU5I8sbuvnV4LDjaKUnevlxvZ1+SX+nu36qq9yS5rqpek+TD\nSV6VJN19a1Vdl+S2HLlq8aXd/djys747R64u/5wkv7n8gZWoqrckeVmSk6vqniRXJLkqW7ffviHJ\nL1XVXTlyYcKL1vDXYg86zr78sqp6YY4c9n53ku9K7Mtsay9N8u1JPlhVH1jWXhvfy+w8x9uXX+17\neWvVHowxAAAAwDbhVA4AAABgjDABAAAAjBEmAAAAgDHCBAAAADBGmAAAAADGCBMAAADAGGECAAAA\nGCNMAAAAAGP+H2zkv835ay66AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1563fd2da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lalonde_no_treat.re78.plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe not informative enough. We see that there are a few \"outliers\" in the treat case but generally speaking there are still more people on the left. Let's try to put everything on the same histogram so as to make the information clearer with respect to the scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBgAAAIMCAYAAABfdh/eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nALoJDAAAAEA3gQEAAADo9v8BxfuVFagJ5HsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f156387f9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 65000, 66)\n",
    "\n",
    "plt.hist(data_lalonde_treat.re78.tolist(), bins, alpha=0.5, label=\"treat\")\n",
    "plt.hist(data_lalonde_no_treat.re78.tolist(), bins, alpha=0.5, label=\"no treat\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't tell us much about the data to be honest. We clearly see that there are some outliers who make a lot of money on the far right but they are a minority so we cannot say something about that. Let's describe the two dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>185.0</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.816216</td>\n",
       "      <td>10.345946</td>\n",
       "      <td>0.843243</td>\n",
       "      <td>0.059459</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.708108</td>\n",
       "      <td>2095.573689</td>\n",
       "      <td>1532.055314</td>\n",
       "      <td>6349.143530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.155019</td>\n",
       "      <td>2.010650</td>\n",
       "      <td>0.364558</td>\n",
       "      <td>0.237124</td>\n",
       "      <td>0.392722</td>\n",
       "      <td>0.455867</td>\n",
       "      <td>4886.620353</td>\n",
       "      <td>3219.250870</td>\n",
       "      <td>7867.402218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>485.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4232.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1291.468000</td>\n",
       "      <td>1817.284000</td>\n",
       "      <td>9642.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35040.070000</td>\n",
       "      <td>25142.240000</td>\n",
       "      <td>60307.930000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       treat         age        educ       black      hispan     married  \\\n",
       "count  185.0  185.000000  185.000000  185.000000  185.000000  185.000000   \n",
       "mean     1.0   25.816216   10.345946    0.843243    0.059459    0.189189   \n",
       "std      0.0    7.155019    2.010650    0.364558    0.237124    0.392722   \n",
       "min      1.0   17.000000    4.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.0   20.000000    9.000000    1.000000    0.000000    0.000000   \n",
       "50%      1.0   25.000000   11.000000    1.000000    0.000000    0.000000   \n",
       "75%      1.0   29.000000   12.000000    1.000000    0.000000    0.000000   \n",
       "max      1.0   48.000000   16.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree          re74          re75          re78  \n",
       "count  185.000000    185.000000    185.000000    185.000000  \n",
       "mean     0.708108   2095.573689   1532.055314   6349.143530  \n",
       "std      0.455867   4886.620353   3219.250870   7867.402218  \n",
       "min      0.000000      0.000000      0.000000      0.000000  \n",
       "25%      0.000000      0.000000      0.000000    485.229800  \n",
       "50%      1.000000      0.000000      0.000000   4232.309000  \n",
       "75%      1.000000   1291.468000   1817.284000   9642.999000  \n",
       "max      1.000000  35040.070000  25142.240000  60307.930000  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde_treat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.0</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.030303</td>\n",
       "      <td>10.235431</td>\n",
       "      <td>0.202797</td>\n",
       "      <td>0.142191</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.596737</td>\n",
       "      <td>5619.236506</td>\n",
       "      <td>2466.484443</td>\n",
       "      <td>6984.169742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.786653</td>\n",
       "      <td>2.855238</td>\n",
       "      <td>0.402552</td>\n",
       "      <td>0.349654</td>\n",
       "      <td>0.500419</td>\n",
       "      <td>0.491126</td>\n",
       "      <td>6788.750796</td>\n",
       "      <td>3291.996183</td>\n",
       "      <td>7294.161791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2547.047000</td>\n",
       "      <td>1086.726000</td>\n",
       "      <td>4975.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9277.128000</td>\n",
       "      <td>3881.419000</td>\n",
       "      <td>11688.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25862.320000</td>\n",
       "      <td>18347.230000</td>\n",
       "      <td>25564.670000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       treat         age        educ       black      hispan     married  \\\n",
       "count  429.0  429.000000  429.000000  429.000000  429.000000  429.000000   \n",
       "mean     0.0   28.030303   10.235431    0.202797    0.142191    0.512821   \n",
       "std      0.0   10.786653    2.855238    0.402552    0.349654    0.500419   \n",
       "min      0.0   16.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.0   19.000000    9.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.0   25.000000   11.000000    0.000000    0.000000    1.000000   \n",
       "75%      0.0   35.000000   12.000000    0.000000    0.000000    1.000000   \n",
       "max      0.0   55.000000   18.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree          re74          re75          re78  \n",
       "count  429.000000    429.000000    429.000000    429.000000  \n",
       "mean     0.596737   5619.236506   2466.484443   6984.169742  \n",
       "std      0.491126   6788.750796   3291.996183   7294.161791  \n",
       "min      0.000000      0.000000      0.000000      0.000000  \n",
       "25%      0.000000      0.000000      0.000000    220.181300  \n",
       "50%      1.000000   2547.047000   1086.726000   4975.505000  \n",
       "75%      1.000000   9277.128000   3881.419000  11688.820000  \n",
       "max      1.000000  25862.320000  18347.230000  25564.670000  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde_no_treat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that the value has nothing special to do with treat or no treat. Somehow we see that the selection was far from uniform at random. In the treat group there are more people with no degree than with degree. The \"black\" variable is interesting also. We intuitively make associate the fact that there are a lot more black people to more difficult social conditions. We can also note that there is a three years difference in mean age between the two groups so, in a sense, we guess that people in the no treat group have more experience on average and this should imply the higher mean salary in the no treat group. We can also say that the age parameter has a lot of impact on the nodegree and the married parameters. Somehow, being 3 years older implies a higher probability of being married and a higher probability of having a degree.\n",
    "\n",
    "Now we want to measure the propensity scores using logistic regression in order to be able to improve our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# We won't include re78 since this is the salary after the subject is treated\n",
    "features = [\"age\", \"educ\", \"black\", \"hispan\", \"married\", \"nodegree\", \"re74\", \"re75\"]\n",
    "X = data_lalonde[features]\n",
    "y = data_lalonde.treat\n",
    "\n",
    "logistic.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80781758957654726"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for a start. We are really far from 0.5 which could mean that we are really far from a randomized experiment. Bad this score is not really informative, what we want here is the propensity score measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "propensity = [b for [a,b] in logistic.predict_proba(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our propensity scores for all \"nodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.44335042297442523,\n",
       " 0.14465953267416379,\n",
       " 0.72235463271079448,\n",
       " 0.66415051691188443,\n",
       " 0.69828561085601226]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensity[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want now is to do matching : for example, if we take the first person in our dataset (the one with propensity score 0.443...), we would like to match them with another person whose propensity score is very close to 0.44 but which is in the opposite group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to separate the dataset into the treat and no treat groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_propensity = data_lalonde.copy()\n",
    "data_propensity[\"propensity\"] = propensity\n",
    "\n",
    "data_treat_propensity = data_propensity[data_propensity.treat == 1]\n",
    "data_no_treat_propensity = data_propensity[data_propensity.treat == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go and create a bipartite graph. So our first strategy (we will find another one if this one doesn't work) is that the weight of the edge between the two nodes is the absolute difference in propensity scores between them. And then we will use networkx to find a min-weight matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 20)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "G.add_nodes_from(range(data_treat_propensity.shape[0]), bipartite=0)\n",
    "G.add_nodes_from(range(data_no_treat_propensity.shape[0]), bipartite=1)\n",
    "\n",
    "for i, row_i in data_treat_propensity.iterrows():\n",
    "    for j, row_j in data_no_treat_propensity.iterrows():\n",
    "        # Since the method we use is based on max matching we use negative value to get min matching\n",
    "        G.add_edge(i, j, weight=-abs(row_i[\"propensity\"] - row_j[\"propensity\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compute the matching (it takes around a minute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_propensity_max_card = nx.max_weight_matching(G, maxcardinality=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the min weight matching without max cardinality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{11: 565,\n",
       " 24: 595,\n",
       " 28: 585,\n",
       " 30: 576,\n",
       " 33: 570,\n",
       " 42: 582,\n",
       " 46: 560,\n",
       " 52: 573,\n",
       " 103: 558,\n",
       " 104: 572,\n",
       " 558: 103,\n",
       " 560: 46,\n",
       " 565: 11,\n",
       " 570: 33,\n",
       " 572: 104,\n",
       " 573: 52,\n",
       " 576: 30,\n",
       " 582: 42,\n",
       " 585: 28,\n",
       " 595: 24}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_propensity = nx.max_weight_matching(G)\n",
    "matching_propensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the problem, as we can see, is that we end up with too few datapoints in the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between 33 and 570 is 0.0\n",
      "Difference between 42 and 582 is 0.0\n",
      "Difference between 11 and 565 is 0.0\n",
      "Difference between 46 and 560 is 0.0\n",
      "Difference between 52 and 573 is 0.0\n",
      "Difference between 24 and 595 is 0.0\n",
      "Difference between 28 and 585 is 0.0\n",
      "Difference between 30 and 576 is 0.0\n",
      "Mean difference in prop score is 0.0\n"
     ]
    }
   ],
   "source": [
    "def observe_matching(matching, verbose):\n",
    "    sum_differences = 0\n",
    "    for (a, b) in matching.items():\n",
    "        if a < data_treat_propensity.shape[0] / 2:\n",
    "            propensity_score_a = data_propensity.loc[a][\"propensity\"]\n",
    "            propensity_score_b = data_propensity.loc[b][\"propensity\"]\n",
    "            diff = abs(propensity_score_a - propensity_score_b)\n",
    "            if verbose:\n",
    "                print(\"Difference between {} and {} is {}\".format(a, b, diff))\n",
    "            sum_differences += diff\n",
    "    print(\"Mean difference in prop score is {}\".format(sum_differences / len(matching) / 2))\n",
    "    \n",
    "observe_matching(matching_propensity, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the problem with using min matching without max cardinality. The algorithm will compute the \"real\" min matching and it will only match the points with the same propensity scores for that. We will continue with the matchings we obtained from max cardinality matchings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean difference in prop score is 0.02293329747279737\n"
     ]
    }
   ],
   "source": [
    "observe_matching(matching_propensity_max_card, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matching takes into account all datapoints from the treat (the smallest) group. So it has 185 matchings. In general those matchings are pretty good, very similar points are matched with each other. But if we inspect the values we see that there are some \"outliers\" matchings since all points have to be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 104.,    4.,    6.,    1.,    7.,    7.,    2.,    0.,   24.,   30.]),\n",
       " array([ 0.        ,  0.05796474,  0.11592949,  0.17389423,  0.23185898,\n",
       "         0.28982372,  0.34778847,  0.40575321,  0.46371795,  0.5216827 ,\n",
       "         0.57964744]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1554bace10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "propensity_differences_matchings = list(filter(lambda x : x[0] < 185, \n",
    "                                               map(lambda matching: (matching[0], abs(data_propensity.loc[matching[0]][\"propensity\"] - data_propensity.loc[matching[1]][\"propensity\"])), \n",
    "                                                   matching_propensity_max_card.items())))\n",
    "propensity_differences_matchings = [prop for (index, prop) in propensity_differences_matchings]\n",
    "plt.hist(propensity_differences_matchings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this histogram we see that there are some bothering matchings where the differences in propensity scores are more than 0.5. A third alternative may be to apply a threshold after the matching to get rid of those values but let's already see how far we can go with our 185-matchings set\n",
    "\n",
    "Now let's look at the re78 values for both groups but we will take into account only the matched datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_treat = [a for (a, b) in matching_propensity_max_card.items() if a < data_treat_propensity.shape[0]]\n",
    "group_no_treat = [b for (a, b) in matching_propensity_max_card.items() if a < data_treat_propensity.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFclJREFUeJzt3X2QVfWd5/H3dxFFiauIpIuIDGj5hIY0SUNAEiNhZY2Z\nUlNJ1FTNpLOjkgedqJONAacqlpapkDU1UTLJZqkkho2ahdFRqZSZKSQk65r40GiHURCJBiYoDz0o\nYEZdRb/zRx+x6Qfv7b636b7H96uq6577O+fc+7mIH07/+pzTkZlIkhrffxrqAJKk+rDQJakkLHRJ\nKgkLXZJKwkKXpJKw0CWpJCx0SSqJioUeESdFRHuXrz0RcWVEHBURKyNiY/E45kAEliT1LvpzYVFE\njACeBT4IXAY8n5mLImIBMCYzvzY4MSVJlfS30OcB12bm7IjYAJyZmVsjYjzwq8w86e32P/roo3PS\npEk1BZakd5o1a9b8W2aOq7TdQf183YuAnxXLTZm5tVjeBjT1tkNEzAfmA0ycOJG2trZ+vqUkvbNF\nxOZqtqv6h6IRcTBwLvAP3ddl52F+r4f6mbkkM1sys2XcuIr/wEiSBqg/Z7l8DHg0M7cXz7cXUy0U\njzvqHU6SVL3+FPpneGu6BWAF0FostwL31CuUJKn/qppDj4jRwFnA57sMLwKWR8TFwGbggvrHk9To\nXnvtNbZs2cIrr7wy1FGGvVGjRjFhwgRGjhw5oP2rKvTM/HdgbLexncDcAb2rpHeMLVu2cPjhhzNp\n0iQiYqjjDFuZyc6dO9myZQuTJ08e0Gt4paikQfXKK68wduxYy7yCiGDs2LE1fSdjoUsadJZ5dWr9\nc7LQJakk+nthkSTV5Dsrn6rr61111olvu37Xrl3cfvvtfOlLX6rL+910003Mnz+fww47rC6vV0+N\nc4S++ps9vySpgl27dvH973+/x/jevXsH9Ho33XQTL730Uq2xBkXjFLokDcCCBQt4+umnaW5uZvr0\n6Xz4wx/m3HPPZcqUKQDceuutzJgxg+bmZj7/+c/z+uuvA/DFL36RlpYWTj31VK699loAFi9ezHPP\nPcecOXOYM2fOkH2mvljokkpt0aJFHH/88bS3t3PjjTfy6KOPcvPNN/PUU0+xfv16li1bxgMPPEB7\nezsjRozgtttuA+Ab3/gGbW1trF27ll//+tesXbuWL3/5y7znPe9h9erVrF69eog/WU/OoUt6R5kx\nY8a+87xXrVrFmjVrmD59OgAvv/wy7373uwFYvnw5S5YsYe/evWzdupV169YxderUIctdDQtd0jvK\n6NGj9y1nJq2trXzzm/v/TO4Pf/gD3/72t3nkkUcYM2YMn/vc5xriSlenXCSV2uGHH86LL77Y67q5\nc+dyxx13sGNH570Fn3/+eTZv3syePXsYPXo0RxxxBNu3b+cXv/hFVa831DxCl3RAVTrNsN7Gjh3L\n7NmzOe200zj00ENpanrrVzdMmTKFG264gXnz5vHGG28wcuRIvve97zFz5kymTZvGySefzLHHHsvs\n2bP37TN//nzOPvvsfXPpw0m/fmNRrVpaWnLAv+Cit9MU5yysLZCkQbd+/XpOOeWUoY7RMHr784qI\nNZnZUmlfp1wkqSQsdEkqCQtdkkrCQpekkrDQJakkLHRJKgnPQ5d0YNX7TqmDfPry3XffzYknnrjv\nZl61am9v57nnnuOcc86py+t15RG6JL2Nu+++m3Xr1vW6biC34G1vb+fee++tNVavLHRJpbZp0yZO\nOeUULr30Uk499VTmzZvHyy+/DHSW68yZM5k6dSqf+MQneOGFF/bb9ze/+Q0rVqzgq1/9Ks3NzTz9\n9NOceeaZXHnllbS0tHDzzTfT0dHBJz/5SaZPn8706dN54IEHAHj44YeZNWsW06ZN4/TTT2fDhg28\n+uqrfP3rX2fZsmU0NzezbNmyun5WC11S6W3cuJHLLruMJ554giOPPJI777wTgM9+9rN861vfYu3a\ntbz3ve/luuuu22+/008/nXPPPZcbb7yR9vZ2jj/+eABeffVV2tra+MpXvsIVV1zBVVddxSOPPMKd\nd97JJZdcAsDJJ5/M/fffz2OPPcb111/PNddcw8EHH8z111/PhRdeSHt7OxdeeGFdP2dVc+gRcSTw\nQ+A0IIG/AjYAy4BJwCbggsx8oY+XkKQhM3nyZJqbmwH4wAc+wKZNm9i9eze7du3iIx/5CACtra18\n+tOfrur1uhbxfffdt9+UzJ49e/jTn/7E7t27aW1tZePGjUQEr732Wh0/Ue+q/aHozcA/ZeanIuJg\n4DDgGmBVZi6KiAXAAuBrg5RTkgbskEMO2bc8YsSIfVMuA9X1FrxvvPEGDz74IKNGjdpvm8svv5w5\nc+Zw1113sWnTJs4888ya3rMaFadcIuII4AzgRwCZ+Wpm7gLOA5YWmy0Fzh+skJJUb0cccQRjxozh\n/vvvB+CnP/3pvqP1rirdLnfevHl897vf3fe8vb0dgN27d3PMMccA8JOf/KTq16tFNUfok4EO4JaI\neB+wBrgCaMrMrcU224Cm3naOiPnAfICJEyfWHFhSgxtGd0ldunQpX/jCF3jppZc47rjjuOWWW3ps\nc9FFF3HppZeyePFi7rjjjh7rFy9ezGWXXcbUqVPZu3cvZ5xxBj/4wQ+4+uqraW1t5YYbbuDjH//4\nvu3nzJnDokWLaG5uZuHChXWdR694+9yIaAEeBGZn5kMRcTOwB/jrzDyyy3YvZOaYt3stb58rvfN4\n+9z+Gezb524BtmTmQ8XzO4D3A9sjYnzxZuOBHf1KLUmqq4qFnpnbgD9GxEnF0FxgHbACaC3GWoF7\nBiWhJKkq1Z7l8tfAbcUZLs8A/43OfwyWR8TFwGbggsGJKKnRZSYRMdQxhr1af4NcVYWeme1Ab/M3\nc2t6d0mlN2rUKHbu3MnYsWMt9beRmezcubPH6Y/94c25JA2qCRMmsGXLFjo6OoY6yrA3atQoJkyY\nMOD9LXRJg2rkyJFMnjx5qGO8I3gvF0kqCQtdkkqiYaZcfvvMzh5js+YMQRBJGqY8QpekkrDQJakk\nLHRJKgkLXZJKwkKXpJKw0CWpJCx0SSoJC12SSsJCl6SSsNAlqSQsdEkqCQtdkkrCQpekkrDQJakk\nLHRJKgkLXZJKwkKXpJKo6jcWRcQm4EXgdWBvZrZExFHAMmASsAm4IDNfGJyYkqRK+nOEPiczmzOz\npXi+AFiVmScAq4rnkqQhUsuUy3nA0mJ5KXB+7XEkSQNVbaEncF9ErImI+cVYU2ZuLZa3AU297RgR\n8yOiLSLaOjo6aowrSepLVXPowIcy89mIeDewMiKe7LoyMzMisrcdM3MJsASgpaWl120kSbWr6gg9\nM58tHncAdwEzgO0RMR6geNwxWCElSZVVLPSIGB0Rh7+5DMwDHgdWAK3FZq3APYMVUpJUWTVTLk3A\nXRHx5va3Z+Y/RcQjwPKIuBjYDFwweDElSZVULPTMfAZ4Xy/jO4G5gxFKktR/XikqSSVhoUtSSVjo\nklQSFroklYSFLkklYaFLUklY6JJUEha6JJWEhS5JJWGhS1JJWOiSVBIWuiSVhIUuSSVhoUtSSVjo\nklQSFroklYSFLkklYaFLUklY6JJUEha6JJWEhS5JJWGhS1JJVF3oETEiIh6LiJ8Xz4+KiJURsbF4\nHDN4MSVJlfTnCP0KYH2X5wuAVZl5ArCqeC5JGiJVFXpETAA+Dvywy/B5wNJieSlwfn2jSZL6o9oj\n9JuAq4E3uow1ZebWYnkb0NTbjhExPyLaIqKto6Nj4EklSW+rYqFHxJ8DOzJzTV/bZGYC2ce6JZnZ\nkpkt48aNG3hSSdLbOqiKbWYD50bEOcAo4D9HxK3A9ogYn5lbI2I8sGMwg0qS3l7FI/TMXJiZEzJz\nEnAR8MvM/AtgBdBabNYK3DNoKSVJFdVyHvoi4KyI2Aj8l+K5JGmIVDPlsk9m/gr4VbG8E5hb/0iS\npIHwSlFJKgkLXZJKwkKXpJKw0CWpJCx0SSoJC12SSsJCl6SSsNAlqSQsdEkqCQtdkkrCQpekkrDQ\nJakkLHRJKgkLXZJKwkKXpJKw0CWpJCx0SSoJC12SSsJCl6SSsNAlqSQsdEkqiYqFHhGjIuLhiPhd\nRDwREdcV40dFxMqI2Fg8jhn8uJKkvlRzhP7/gY9m5vuAZuDsiJgJLABWZeYJwKriuSRpiFQs9Oz0\np+LpyOIrgfOApcX4UuD8QUkoSapKVXPoETEiItqBHcDKzHwIaMrMrcUm24CmQcooSapCVYWema9n\nZjMwAZgREad1W590HrX3EBHzI6ItIto6OjpqDixJ6l2/znLJzF3AauBsYHtEjAcoHnf0sc+SzGzJ\nzJZx48bVmleS1IdqznIZFxFHFsuHAmcBTwIrgNZis1bgnsEKKUmq7KAqthkPLI2IEXT+A7A8M38e\nEb8FlkfExcBm4IJBzClJqqBioWfmWmBaL+M7gbmDEUqS1H9eKSpJJWGhS1JJWOiSVBIWuiSVhIUu\nSSVhoUtSSVjoklQSFroklYSFLkklYaFLUklY6JJUEha6JJWEhS5JJWGhS1JJWOiSVBIWuiSVhIUu\nSSVhoUtSSVjoklQSFroklYSFLkklYaFLUklULPSIODYiVkfEuoh4IiKuKMaPioiVEbGxeBwz+HEl\nSX2p5gh9L/CVzJwCzAQui4gpwAJgVWaeAKwqnkuShkjFQs/MrZn5aLH8IrAeOAY4D1habLYUOH+w\nQkqSKuvXHHpETAKmAQ8BTZm5tVi1DWiqazJJUr9UXegR8S7gTuDKzNzTdV1mJpB97Dc/Itoioq2j\no6OmsJKkvlVV6BExks4yvy0z/7EY3h4R44v144Edve2bmUsysyUzW8aNG1ePzJKkXlRzlksAPwLW\nZ+bfdVm1AmgtlluBe+ofT5JUrYOq2GY28JfAv0REezF2DbAIWB4RFwObgQsGJ6IkqRoVCz0z/x8Q\nfayeW984kqSB8kpRSSoJC12SSsJCl6SSsNAlqSQsdEkqCQtdkkrCQpekkrDQJakkLHRJKgkLXZJK\nopp7uQxb31n5VI+xq846cQiSSNLQ8whdkkrCQpekkrDQJakkGnoOfVCt/mbPsTkLD3wOSaqSR+iS\nVBIWuiSVRPmmXLpPlfQyTTLg0x2dhpE0jHmELkklYaFLUklY6JJUEha6JJVExUKPiB9HxI6IeLzL\n2FERsTIiNhaPYwY3piSpkmrOcvkJ8PfA/+4ytgBYlZmLImJB8fxr9Y/39mb+65Keg8eNPdAxJGlY\nqHiEnpn/F3i+2/B5wNJieSlwfp1zSZL6aaBz6E2ZubVY3gY01SmPJGmAar6wKDMzIrKv9RExH5gP\nMHHixFrfrt96u4joQL+f92iXdCAM9Ah9e0SMByged/S1YWYuycyWzGwZN27cAN9OklTJQAt9BdBa\nLLcC99QnjiRpoCpOuUTEz4AzgaMjYgtwLbAIWB4RFwObgQsGM+RQ+O0zO3uMzerlDJoDPaUjSX2p\nWOiZ+Zk+Vs2tcxZJUg28UlSSSqJ8t8/tpreLjx6cOL/HWPepk5mDlqgP3ppXUo08QpekkrDQJakk\nSj/l0ptqp2G66+3MFw78tVKS1CuP0CWpJCx0SSqJ0k259DotUoVeb8Vbp9f5zsqe0zlDfn8Xz6qR\nSscjdEkqCQtdkkqidFMujaLHhUz/2nOq6MG93opXUvU8QpekkrDQJakknHKpUb3Ojqlab2endNfL\n2Srdp3iu8r+8VDoeoUtSSVjoklQSFroklYQzqUPkQM+9d3+/3/ayzSy8elRqZB6hS1JJWOiSVBJO\nuRwA9Zxe6X7zsVnHje25UTWnNg5T3U+vBK+OlarlEboklURNhR4RZ0fEhoj4fUQsqFcoSVL/DXjK\nJSJGAN8DzgK2AI9ExIrMXFevcKpsoPd/r/r1f/Tfe4zNuvjb+z3vbZqkGo0ylVLtNFCPq3Eb5POp\nUxmm+2o5Qp8B/D4zn8nMV4H/A5xXn1iSpP6qpdCPAf7Y5fmWYkySNAQiMwe2Y8SngLMz85Li+V8C\nH8zMy7ttNx9483ewnQRsGGDWo4F/G+C+Q83sQ6NRszdqbjD7YPmzzBxXaaNaTlt8Fji2y/MJxdh+\nMnMJUPN5exHRlpkttb7OUDD70GjU7I2aG8w+1GqZcnkEOCEiJkfEwcBFwIr6xJIk9deAj9Azc29E\nXA78MzAC+HFmPlG3ZJKkfqnpStHMvBe4t05ZKjnAv0mirsw+NBo1e6PmBrMPqQH/UFSSNLx46b8k\nlURDFPpwuMVARPw4InZExONdxo6KiJURsbF4HNNl3cIi74aI+K9dxj8QEf9SrFscEVGMHxIRy4rx\nhyJiUh2zHxsRqyNiXUQ8ERFXNEL+iBgVEQ9HxO+K3Nc1Qu5un2FERDwWET9vpOwRsal4z/aIaGuw\n7EdGxB0R8WRErI+IWY2SvWaZOay/6PyB69PAccDBwO+AKUOQ4wzg/cDjXcb+B7CgWF4AfKtYnlLk\nPASYXOQfUax7GJgJBPAL4GPF+JeAHxTLFwHL6ph9PPD+Yvlw4Kki47DOX7zHu4rlkcBDxXsP69zd\nPsPfALcDP2+wvzObgKO7jTVK9qXAJcXywcCRjZK95s8+1AGq+I8zC/jnLs8XAguHKMsk9i/0DcD4\nYnk8sKG3jHSeCTSr2ObJLuOfAf5X122K5YPovMAhBulz3EPnPXgaJj9wGPAo8MFGyU3ntRmrgI/y\nVqE3SvZN9Cz0YZ8dOAL4Q/fXaoTs9fhqhCmX4XyLgabM3FosbwOaiuW+Mh9TLHcf32+fzNwL7AZ6\nudl5bYpvD6fRebQ77PMXUxbtwA5gZWY2RO7CTcDVwBtdxholewL3RcSa6Lzau1GyTwY6gFuKqa4f\nRsToBsles0Yo9IaQnf9cD+tThiLiXcCdwJWZuafruuGaPzNfz8xmOo92Z0TEad3WD8vcEfHnwI7M\nXNPXNsM1e+FDxZ/7x4DLIuKMriuHcfaD6Jwa/Z+ZOQ34dzqnWPYZxtlr1giFXtUtBobI9ogYD1A8\n7ijG+8r8bLHcfXy/fSLiIDq/dazbvXEjYiSdZX5bZv5jo+XPzF3AauDsBsk9Gzg3IjbReSfSj0bE\nrQ2Sncx8tnjcAdxF591VGyH7FmBL8Z0cwB10FnwjZK9ZIxT6cL7FwAqgtVhupXNu+s3xi4qfhk8G\nTgAeLr7l2xMRM4ufmH+22z5vvtangF8WRxI1K97rR8D6zPy7RskfEeMi4shi+VA65/2fHO65ATJz\nYWZOyMxJdP6d/WVm/kUjZI+I0RFx+JvLwDzg8UbInpnbgD9GxEnF0FxgXSNkr4uhnsSv5gs4h84z\nM54G/naIMvwM2Aq8RudRwMV0zputAjYC9wFHddn+b4u8Gyh+Ol6Mt9D5P8fTwN/z1sVdo4B/AH5P\n50/Xj6tj9g/R+S3mWqC9+DpnuOcHpgKPFbkfB75ejA/r3L18jjN564eiwz47nWeU/a74euLN/+ca\nIXvx2s1AW/H35m5gTKNkr/XLK0UlqSQaYcpFklQFC12SSsJCl6SSsNAlqSQsdEkqCQtdkkrCQpek\nkrDQJakk/gMAzLqQLDKAPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f15575a6b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 65000, 66)\n",
    "plt.hist(data_lalonde.loc[group_treat].re78.tolist(), bins, alpha=0.5, label=\"treat\")\n",
    "plt.hist(data_lalonde.loc[group_no_treat].re78.tolist(), bins, alpha=0.5, label=\"no treat\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is our distribution. Maybe we can argue we see things in a better way on this histogram. The numer of samples is exactly the same for both groups (the number of samples in the treat group) and the two groups were obtained using the min matching method on the bipartite graph. Here we clearly see that the \"extreme\" outliers belong to the treat group. But however, there are not so many of them. We can also see that the first bin (the least salary) contains more people from the non treated than the treated group (however, if we look at the re78 column in out dataset we see that there are a lot of 0 values so either the dataset has errors or those people are simply unemployed). So the only thing we might try to guess from this is that the treat works well on a few people: the blue outliers. (but maybe those people anyway had more determination and this determination is what caused them to take the treat so the variable is something that we do not have in our dataset: the determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tweak things to see if we can get better insights. The first thing we want to try is to simply remove the matchings where the difference is too big. We will end up with much less datapoints but we might get something out of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_matching(matching, threshold=0.4):\n",
    "    def difference_matching(x, y):\n",
    "        return abs(data_propensity.loc[x][\"propensity\"] - data_propensity.loc[y][\"propensity\"])\n",
    "\n",
    "    matching_propensity_max_card_pairs = filter(lambda x : x[0] < 185, matching_propensity_max_card.items())\n",
    "    matching_propensity_max_card_pairs = map(lambda x : (x[0], x[1], difference_matching(x[0], x[1])), matching_propensity_max_card_pairs)\n",
    "    matching_propensity_max_card_pairs = filter(lambda x : x[2] <= threshold, matching_propensity_max_card_pairs)\n",
    "    return list(matching_propensity_max_card_pairs)\n",
    "\n",
    "def plot_matching_hist(matching, threshold=0.4):\n",
    "    bins = np.linspace(0, 65000, 66)\n",
    "    matchings = filter_matching(matching, threshold)\n",
    "    group_treat = [a for (a, b, prop) in matchings]\n",
    "    group_no_treat = [b for (a, b, prop) in matchings]\n",
    "    plt.hist(data_lalonde.loc[group_treat].re78.tolist(), bins, alpha=0.5, label=\"treat\")\n",
    "    plt.hist(data_lalonde.loc[group_no_treat].re78.tolist(), bins, alpha=0.5, label=\"no treat\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we get with the threshold at 0.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f154fd69f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And at 0.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f154fbfe828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0.1 (very similar propensity scores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f155415d1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0.05 (very very similar propensity scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f154fbf8f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0 (Exactly the same propensity scores) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWNySVJj/B2PsPPOgAhBiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f154fbddf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general thing we always see no matter the threshold is that the treat group always has those outliers who have really big salaries (except in the threshold 0 case where they do not appear)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we might try to get rid of some columns to see if we can get better insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Applied ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroup_train = fetch_20newsgroups(subset=\"train\")\n",
    "pprint(list(newsgroup_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroup_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7532, 130107)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82906596444740432"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "newsgroup_test = fetch_20newsgroups(subset=\"test\")\n",
    "vectors_test = vectorizer.transform(newsgroup_test.data)\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectors, newsgroup_train.target)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroup_test.target, pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup = fetch_20newsgroups(subset=\"all\", remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroup.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  3, 17, ...,  3,  1,  7])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroup.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 134410)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroup.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ '/home/ahmed/scikit_learn_data/20news_home/20news-bydate-test/rec.sport.hockey/54367',\n",
       "       '/home/ahmed/scikit_learn_data/20news_home/20news-bydate-train/comp.sys.ibm.pc.hardware/60215',\n",
       "       '/home/ahmed/scikit_learn_data/20news_home/20news-bydate-train/talk.politics.mideast/76120',\n",
       "       ...,\n",
       "       '/home/ahmed/scikit_learn_data/20news_home/20news-bydate-train/comp.sys.ibm.pc.hardware/60695',\n",
       "       '/home/ahmed/scikit_learn_data/20news_home/20news-bydate-train/comp.graphics/38319',\n",
       "       '/home/ahmed/scikit_learn_data/20news_home/20news-bydate-test/rec.autos/103195'],\n",
       "      dtype='<U92')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroup.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectors\n",
    "y = newsgroup.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_fit, X_test, y_fit, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Here we put 0.11111111 because 0.1111111... * 0.9 = 0.1 which will give 80% / 10% / 10%\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_fit, y_fit, test_size=0.111111112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a little preview depending on the number of estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45835543766578252"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6408488063660478"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65729442970822283"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As if the score increases depending on the number of estimators. Let's perform a grid search on the number of trees to see what is happening with more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64827586206896548"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=200)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_total = X_fit.shape[0]\n",
    "n_val = int(fit_total * 0.1111111)\n",
    "n_train = fit_total - n_val\n",
    "ps_array = [0] * n_train + [1] * n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n",
      "[CV] max_depth=1, n_estimators=1 .....................................\n",
      "[CV] ...................... max_depth=1, n_estimators=1, total=   0.1s\n",
      "[CV] max_depth=1, n_estimators=1 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... max_depth=1, n_estimators=1, total=   0.1s\n",
      "[CV] max_depth=1, n_estimators=2 .....................................\n",
      "[CV] ...................... max_depth=1, n_estimators=2, total=   0.1s\n",
      "[CV] max_depth=1, n_estimators=2 .....................................\n",
      "[CV] ...................... max_depth=1, n_estimators=2, total=   0.1s\n",
      "[CV] max_depth=1, n_estimators=4 .....................................\n",
      "[CV] ...................... max_depth=1, n_estimators=4, total=   0.2s\n",
      "[CV] max_depth=1, n_estimators=4 .....................................\n",
      "[CV] ...................... max_depth=1, n_estimators=4, total=   0.1s\n",
      "[CV] max_depth=1, n_estimators=10 ....................................\n",
      "[CV] ..................... max_depth=1, n_estimators=10, total=   0.2s\n",
      "[CV] max_depth=1, n_estimators=10 ....................................\n",
      "[CV] ..................... max_depth=1, n_estimators=10, total=   0.1s\n",
      "[CV] max_depth=1, n_estimators=21 ....................................\n",
      "[CV] ..................... max_depth=1, n_estimators=21, total=   0.6s\n",
      "[CV] max_depth=1, n_estimators=21 ....................................\n",
      "[CV] ..................... max_depth=1, n_estimators=21, total=   0.2s\n",
      "[CV] max_depth=1, n_estimators=46 ....................................\n",
      "[CV] ..................... max_depth=1, n_estimators=46, total=   0.6s\n",
      "[CV] max_depth=1, n_estimators=46 ....................................\n",
      "[CV] ..................... max_depth=1, n_estimators=46, total=   0.4s\n",
      "[CV] max_depth=1, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=1, n_estimators=100, total=   1.2s\n",
      "[CV] max_depth=1, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=1, n_estimators=100, total=   0.8s\n",
      "[CV] max_depth=1, n_estimators=215 ...................................\n",
      "[CV] .................... max_depth=1, n_estimators=215, total=   2.6s\n",
      "[CV] max_depth=1, n_estimators=215 ...................................\n",
      "[CV] .................... max_depth=1, n_estimators=215, total=   1.7s\n",
      "[CV] max_depth=1, n_estimators=464 ...................................\n",
      "[CV] .................... max_depth=1, n_estimators=464, total=   5.4s\n",
      "[CV] max_depth=1, n_estimators=464 ...................................\n",
      "[CV] .................... max_depth=1, n_estimators=464, total=   3.7s\n",
      "[CV] max_depth=1, n_estimators=1000 ..................................\n",
      "[CV] ................... max_depth=1, n_estimators=1000, total=  11.5s\n",
      "[CV] max_depth=1, n_estimators=1000 ..................................\n",
      "[CV] ................... max_depth=1, n_estimators=1000, total=   7.6s\n",
      "[CV] max_depth=2, n_estimators=1 .....................................\n",
      "[CV] ...................... max_depth=2, n_estimators=1, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=1 .....................................\n",
      "[CV] ...................... max_depth=2, n_estimators=1, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=2 .....................................\n",
      "[CV] ...................... max_depth=2, n_estimators=2, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=2 .....................................\n",
      "[CV] ...................... max_depth=2, n_estimators=2, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=4 .....................................\n",
      "[CV] ...................... max_depth=2, n_estimators=4, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=4 .....................................\n",
      "[CV] ...................... max_depth=2, n_estimators=4, total=   0.1s\n",
      "[CV] max_depth=2, n_estimators=10 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=10, total=   0.2s\n",
      "[CV] max_depth=2, n_estimators=10 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=10, total=   0.2s\n",
      "[CV] max_depth=2, n_estimators=21 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=21, total=   0.4s\n",
      "[CV] max_depth=2, n_estimators=21 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=21, total=   0.3s\n",
      "[CV] max_depth=2, n_estimators=46 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=46, total=   0.6s\n",
      "[CV] max_depth=2, n_estimators=46 ....................................\n",
      "[CV] ..................... max_depth=2, n_estimators=46, total=   0.6s\n",
      "[CV] max_depth=2, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=2, n_estimators=100, total=   1.4s\n",
      "[CV] max_depth=2, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=2, n_estimators=100, total=   1.2s\n",
      "[CV] max_depth=2, n_estimators=215 ...................................\n",
      "[CV] .................... max_depth=2, n_estimators=215, total=   2.5s\n",
      "[CV] max_depth=2, n_estimators=215 ...................................\n",
      "[CV] .................... max_depth=2, n_estimators=215, total=   2.3s\n",
      "[CV] max_depth=2, n_estimators=464 ...................................\n",
      "[CV] .................... max_depth=2, n_estimators=464, total=   5.4s\n",
      "[CV] max_depth=2, n_estimators=464 ...................................\n",
      "[CV] .................... max_depth=2, n_estimators=464, total=   4.9s\n",
      "[CV] max_depth=2, n_estimators=1000 ..................................\n",
      "[CV] ................... max_depth=2, n_estimators=1000, total=  12.6s\n",
      "[CV] max_depth=2, n_estimators=1000 ..................................\n",
      "[CV] ................... max_depth=2, n_estimators=1000, total=  11.5s\n",
      "[CV] max_depth=4, n_estimators=1 .....................................\n",
      "[CV] ...................... max_depth=4, n_estimators=1, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=1 .....................................\n",
      "[CV] ...................... max_depth=4, n_estimators=1, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=2 .....................................\n",
      "[CV] ...................... max_depth=4, n_estimators=2, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=2 .....................................\n",
      "[CV] ...................... max_depth=4, n_estimators=2, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=4 .....................................\n",
      "[CV] ...................... max_depth=4, n_estimators=4, total=   0.1s\n",
      "[CV] max_depth=4, n_estimators=4 .....................................\n",
      "[CV] ...................... max_depth=4, n_estimators=4, total=   0.2s\n",
      "[CV] max_depth=4, n_estimators=10 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=10, total=   0.2s\n",
      "[CV] max_depth=4, n_estimators=10 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=10, total=   0.3s\n",
      "[CV] max_depth=4, n_estimators=21 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=21, total=   0.4s\n",
      "[CV] max_depth=4, n_estimators=21 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=21, total=   0.4s\n",
      "[CV] max_depth=4, n_estimators=46 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=46, total=   0.7s\n",
      "[CV] max_depth=4, n_estimators=46 ....................................\n",
      "[CV] ..................... max_depth=4, n_estimators=46, total=   0.9s\n",
      "[CV] max_depth=4, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=4, n_estimators=100, total=   1.3s\n",
      "[CV] max_depth=4, n_estimators=100 ...................................\n",
      "[CV] .................... max_depth=4, n_estimators=100, total=   1.8s\n",
      "[CV] max_depth=4, n_estimators=215 ...................................\n",
      "[CV] .................... max_depth=4, n_estimators=215, total=   2.7s\n",
      "[CV] max_depth=4, n_estimators=215 ...................................\n",
      "[CV] .................... max_depth=4, n_estimators=215, total=   3.7s\n",
      "[CV] max_depth=4, n_estimators=464 ...................................\n",
      "[CV] .................... max_depth=4, n_estimators=464, total=   5.7s\n",
      "[CV] max_depth=4, n_estimators=464 ...................................\n",
      "[CV] .................... max_depth=4, n_estimators=464, total=   8.1s\n",
      "[CV] max_depth=4, n_estimators=1000 ..................................\n",
      "[CV] ................... max_depth=4, n_estimators=1000, total=  12.3s\n",
      "[CV] max_depth=4, n_estimators=1000 ..................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... max_depth=4, n_estimators=1000, total=  17.3s\n",
      "[CV] max_depth=10, n_estimators=1 ....................................\n",
      "[CV] ..................... max_depth=10, n_estimators=1, total=   0.1s\n",
      "[CV] max_depth=10, n_estimators=1 ....................................\n",
      "[CV] ..................... max_depth=10, n_estimators=1, total=   0.1s\n",
      "[CV] max_depth=10, n_estimators=2 ....................................\n",
      "[CV] ..................... max_depth=10, n_estimators=2, total=   0.1s\n",
      "[CV] max_depth=10, n_estimators=2 ....................................\n",
      "[CV] ..................... max_depth=10, n_estimators=2, total=   0.2s\n",
      "[CV] max_depth=10, n_estimators=4 ....................................\n",
      "[CV] ..................... max_depth=10, n_estimators=4, total=   0.2s\n",
      "[CV] max_depth=10, n_estimators=4 ....................................\n",
      "[CV] ..................... max_depth=10, n_estimators=4, total=   0.3s\n",
      "[CV] max_depth=10, n_estimators=10 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=10, total=   0.3s\n",
      "[CV] max_depth=10, n_estimators=10 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=10, total=   0.5s\n",
      "[CV] max_depth=10, n_estimators=21 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=21, total=   0.4s\n",
      "[CV] max_depth=10, n_estimators=21 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=21, total=   0.9s\n",
      "[CV] max_depth=10, n_estimators=46 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=46, total=   0.8s\n",
      "[CV] max_depth=10, n_estimators=46 ...................................\n",
      "[CV] .................... max_depth=10, n_estimators=46, total=   2.0s\n",
      "[CV] max_depth=10, n_estimators=100 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=100, total=   1.6s\n",
      "[CV] max_depth=10, n_estimators=100 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=100, total=   4.3s\n",
      "[CV] max_depth=10, n_estimators=215 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=215, total=   3.5s\n",
      "[CV] max_depth=10, n_estimators=215 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=215, total=   9.3s\n",
      "[CV] max_depth=10, n_estimators=464 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=464, total=   7.3s\n",
      "[CV] max_depth=10, n_estimators=464 ..................................\n",
      "[CV] ................... max_depth=10, n_estimators=464, total=  19.8s\n",
      "[CV] max_depth=10, n_estimators=1000 .................................\n",
      "[CV] .................. max_depth=10, n_estimators=1000, total=  16.0s\n",
      "[CV] max_depth=10, n_estimators=1000 .................................\n",
      "[CV] .................. max_depth=10, n_estimators=1000, total=  43.4s\n",
      "[CV] max_depth=21, n_estimators=1 ....................................\n",
      "[CV] ..................... max_depth=21, n_estimators=1, total=   0.1s\n",
      "[CV] max_depth=21, n_estimators=1 ....................................\n",
      "[CV] ..................... max_depth=21, n_estimators=1, total=   0.2s\n",
      "[CV] max_depth=21, n_estimators=2 ....................................\n",
      "[CV] ..................... max_depth=21, n_estimators=2, total=   0.2s\n",
      "[CV] max_depth=21, n_estimators=2 ....................................\n",
      "[CV] ..................... max_depth=21, n_estimators=2, total=   0.3s\n",
      "[CV] max_depth=21, n_estimators=4 ....................................\n",
      "[CV] ..................... max_depth=21, n_estimators=4, total=   0.2s\n",
      "[CV] max_depth=21, n_estimators=4 ....................................\n",
      "[CV] ..................... max_depth=21, n_estimators=4, total=   0.5s\n",
      "[CV] max_depth=21, n_estimators=10 ...................................\n",
      "[CV] .................... max_depth=21, n_estimators=10, total=   0.4s\n",
      "[CV] max_depth=21, n_estimators=10 ...................................\n",
      "[CV] .................... max_depth=21, n_estimators=10, total=   1.1s\n",
      "[CV] max_depth=21, n_estimators=21 ...................................\n",
      "[CV] .................... max_depth=21, n_estimators=21, total=   0.7s\n",
      "[CV] max_depth=21, n_estimators=21 ...................................\n",
      "[CV] .................... max_depth=21, n_estimators=21, total=   3.6s\n",
      "[CV] max_depth=21, n_estimators=46 ...................................\n",
      "[CV] .................... max_depth=21, n_estimators=46, total=   1.4s\n",
      "[CV] max_depth=21, n_estimators=46 ...................................\n",
      "[CV] .................... max_depth=21, n_estimators=46, total=   5.2s\n",
      "[CV] max_depth=21, n_estimators=100 ..................................\n",
      "[CV] ................... max_depth=21, n_estimators=100, total=   2.5s\n",
      "[CV] max_depth=21, n_estimators=100 ..................................\n",
      "[CV] ................... max_depth=21, n_estimators=100, total=  10.7s\n",
      "[CV] max_depth=21, n_estimators=215 ..................................\n",
      "[CV] ................... max_depth=21, n_estimators=215, total=   5.3s\n",
      "[CV] max_depth=21, n_estimators=215 ..................................\n",
      "[CV] ................... max_depth=21, n_estimators=215, total=  23.5s\n",
      "[CV] max_depth=21, n_estimators=464 ..................................\n",
      "[CV] ................... max_depth=21, n_estimators=464, total=  11.4s\n",
      "[CV] max_depth=21, n_estimators=464 ..................................\n",
      "[CV] ................... max_depth=21, n_estimators=464, total=  49.9s\n",
      "[CV] max_depth=21, n_estimators=1000 .................................\n",
      "[CV] .................. max_depth=21, n_estimators=1000, total=  24.4s\n",
      "[CV] max_depth=21, n_estimators=1000 .................................\n",
      "[CV] .................. max_depth=21, n_estimators=1000, total= 1.8min\n",
      "[CV] max_depth=46, n_estimators=1 ....................................\n",
      "[CV] ..................... max_depth=46, n_estimators=1, total=   0.1s\n",
      "[CV] max_depth=46, n_estimators=1 ....................................\n",
      "[CV] ..................... max_depth=46, n_estimators=1, total=   0.4s\n",
      "[CV] max_depth=46, n_estimators=2 ....................................\n",
      "[CV] ..................... max_depth=46, n_estimators=2, total=   0.2s\n",
      "[CV] max_depth=46, n_estimators=2 ....................................\n",
      "[CV] ..................... max_depth=46, n_estimators=2, total=   0.8s\n",
      "[CV] max_depth=46, n_estimators=4 ....................................\n",
      "[CV] ..................... max_depth=46, n_estimators=4, total=   0.3s\n",
      "[CV] max_depth=46, n_estimators=4 ....................................\n",
      "[CV] ..................... max_depth=46, n_estimators=4, total=   1.4s\n",
      "[CV] max_depth=46, n_estimators=10 ...................................\n",
      "[CV] .................... max_depth=46, n_estimators=10, total=   0.6s\n",
      "[CV] max_depth=46, n_estimators=10 ...................................\n",
      "[CV] .................... max_depth=46, n_estimators=10, total=   3.3s\n",
      "[CV] max_depth=46, n_estimators=21 ...................................\n",
      "[CV] .................... max_depth=46, n_estimators=21, total=   1.2s\n",
      "[CV] max_depth=46, n_estimators=21 ...................................\n",
      "[CV] .................... max_depth=46, n_estimators=21, total=   6.6s\n",
      "[CV] max_depth=46, n_estimators=46 ...................................\n",
      "[CV] .................... max_depth=46, n_estimators=46, total=   2.3s\n",
      "[CV] max_depth=46, n_estimators=46 ...................................\n",
      "[CV] .................... max_depth=46, n_estimators=46, total=  14.8s\n",
      "[CV] max_depth=46, n_estimators=100 ..................................\n",
      "[CV] ................... max_depth=46, n_estimators=100, total=   4.9s\n",
      "[CV] max_depth=46, n_estimators=100 ..................................\n",
      "[CV] ................... max_depth=46, n_estimators=100, total=  32.5s\n",
      "[CV] max_depth=46, n_estimators=215 ..................................\n",
      "[CV] ................... max_depth=46, n_estimators=215, total=  10.3s\n",
      "[CV] max_depth=46, n_estimators=215 ..................................\n",
      "[CV] ................... max_depth=46, n_estimators=215, total= 1.1min\n",
      "[CV] max_depth=46, n_estimators=464 ..................................\n",
      "[CV] ................... max_depth=46, n_estimators=464, total=  22.1s\n",
      "[CV] max_depth=46, n_estimators=464 ..................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... max_depth=46, n_estimators=464, total= 2.5min\n",
      "[CV] max_depth=46, n_estimators=1000 .................................\n",
      "[CV] .................. max_depth=46, n_estimators=1000, total=  47.6s\n",
      "[CV] max_depth=46, n_estimators=1000 .................................\n",
      "[CV] .................. max_depth=46, n_estimators=1000, total= 5.3min\n",
      "[CV] max_depth=100, n_estimators=1 ...................................\n",
      "[CV] .................... max_depth=100, n_estimators=1, total=   0.2s\n",
      "[CV] max_depth=100, n_estimators=1 ...................................\n",
      "[CV] .................... max_depth=100, n_estimators=1, total=   0.8s\n",
      "[CV] max_depth=100, n_estimators=2 ...................................\n",
      "[CV] .................... max_depth=100, n_estimators=2, total=   0.2s\n",
      "[CV] max_depth=100, n_estimators=2 ...................................\n",
      "[CV] .................... max_depth=100, n_estimators=2, total=   1.5s\n",
      "[CV] max_depth=100, n_estimators=4 ...................................\n",
      "[CV] .................... max_depth=100, n_estimators=4, total=   0.5s\n",
      "[CV] max_depth=100, n_estimators=4 ...................................\n",
      "[CV] .................... max_depth=100, n_estimators=4, total=   3.0s\n",
      "[CV] max_depth=100, n_estimators=10 ..................................\n",
      "[CV] ................... max_depth=100, n_estimators=10, total=   1.0s\n",
      "[CV] max_depth=100, n_estimators=10 ..................................\n",
      "[CV] ................... max_depth=100, n_estimators=10, total=   7.7s\n",
      "[CV] max_depth=100, n_estimators=21 ..................................\n",
      "[CV] ................... max_depth=100, n_estimators=21, total=   1.9s\n",
      "[CV] max_depth=100, n_estimators=21 ..................................\n",
      "[CV] ................... max_depth=100, n_estimators=21, total=  17.3s\n",
      "[CV] max_depth=100, n_estimators=46 ..................................\n",
      "[CV] ................... max_depth=100, n_estimators=46, total=   4.0s\n",
      "[CV] max_depth=100, n_estimators=46 ..................................\n",
      "[CV] ................... max_depth=100, n_estimators=46, total=  35.7s\n",
      "[CV] max_depth=100, n_estimators=100 .................................\n",
      "[CV] .................. max_depth=100, n_estimators=100, total=   8.8s\n",
      "[CV] max_depth=100, n_estimators=100 .................................\n",
      "[CV] .................. max_depth=100, n_estimators=100, total= 1.3min\n",
      "[CV] max_depth=100, n_estimators=215 .................................\n",
      "[CV] .................. max_depth=100, n_estimators=215, total=  18.9s\n",
      "[CV] max_depth=100, n_estimators=215 .................................\n",
      "[CV] .................. max_depth=100, n_estimators=215, total= 2.8min\n",
      "[CV] max_depth=100, n_estimators=464 .................................\n",
      "[CV] .................. max_depth=100, n_estimators=464, total=  41.2s\n",
      "[CV] max_depth=100, n_estimators=464 .................................\n",
      "[CV] .................. max_depth=100, n_estimators=464, total= 6.0min\n",
      "[CV] max_depth=100, n_estimators=1000 ................................\n",
      "[CV] ................. max_depth=100, n_estimators=1000, total= 1.5min\n",
      "[CV] max_depth=100, n_estimators=1000 ................................\n",
      "[CV] ................. max_depth=100, n_estimators=1000, total=13.1min\n",
      "[CV] max_depth=215, n_estimators=1 ...................................\n",
      "[CV] .................... max_depth=215, n_estimators=1, total=   0.2s\n",
      "[CV] max_depth=215, n_estimators=1 ...................................\n",
      "[CV] .................... max_depth=215, n_estimators=1, total=   1.4s\n",
      "[CV] max_depth=215, n_estimators=2 ...................................\n",
      "[CV] .................... max_depth=215, n_estimators=2, total=   0.4s\n",
      "[CV] max_depth=215, n_estimators=2 ...................................\n",
      "[CV] .................... max_depth=215, n_estimators=2, total=   2.6s\n",
      "[CV] max_depth=215, n_estimators=4 ...................................\n",
      "[CV] .................... max_depth=215, n_estimators=4, total=   0.7s\n",
      "[CV] max_depth=215, n_estimators=4 ...................................\n",
      "[CV] .................... max_depth=215, n_estimators=4, total=   5.3s\n",
      "[CV] max_depth=215, n_estimators=10 ..................................\n",
      "[CV] ................... max_depth=215, n_estimators=10, total=   1.4s\n",
      "[CV] max_depth=215, n_estimators=10 ..................................\n",
      "[CV] ................... max_depth=215, n_estimators=10, total=  13.3s\n",
      "[CV] max_depth=215, n_estimators=21 ..................................\n",
      "[CV] ................... max_depth=215, n_estimators=21, total=   2.9s\n",
      "[CV] max_depth=215, n_estimators=21 ..................................\n",
      "[CV] ................... max_depth=215, n_estimators=21, total=  28.3s\n",
      "[CV] max_depth=215, n_estimators=46 ..................................\n",
      "[CV] ................... max_depth=215, n_estimators=46, total=   5.9s\n",
      "[CV] max_depth=215, n_estimators=46 ..................................\n",
      "[CV] ................... max_depth=215, n_estimators=46, total= 1.0min\n",
      "[CV] max_depth=215, n_estimators=100 .................................\n",
      "[CV] .................. max_depth=215, n_estimators=100, total=  12.8s\n",
      "[CV] max_depth=215, n_estimators=100 .................................\n",
      "[CV] .................. max_depth=215, n_estimators=100, total= 2.2min\n",
      "[CV] max_depth=215, n_estimators=215 .................................\n",
      "[CV] .................. max_depth=215, n_estimators=215, total=  27.8s\n",
      "[CV] max_depth=215, n_estimators=215 .................................\n",
      "[CV] .................. max_depth=215, n_estimators=215, total= 4.8min\n",
      "[CV] max_depth=215, n_estimators=464 .................................\n",
      "[CV] .................. max_depth=215, n_estimators=464, total=  58.7s\n",
      "[CV] max_depth=215, n_estimators=464 .................................\n",
      "[CV] .................. max_depth=215, n_estimators=464, total=10.2min\n",
      "[CV] max_depth=215, n_estimators=1000 ................................\n",
      "[CV] ................. max_depth=215, n_estimators=1000, total= 2.1min\n",
      "[CV] max_depth=215, n_estimators=1000 ................................\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-34a2afc0d2f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclf_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_estimators_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_depth_space\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPredefinedSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mclf_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 564\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 326\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    348\u001b[0m                                            self.min_impurity_split)\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "n_estimators_space = np.logspace(0, 3, num=10, dtype='int')\n",
    "max_depth_space = np.logspace(0, 3, num=10, dtype='int')\n",
    "rfc = RandomForestClassifier()\n",
    "clf_grid = GridSearchCV(rfc, param_grid={'n_estimators':n_estimators_space, 'max_depth':max_depth_space}, verbose=2, cv=PredefinedSplit(ps_array))\n",
    "clf_grid.fit(X_fit, y_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16961"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_val = int(X_fit.shape[0] * 0.1111111)\n",
    "len([0] * n_val + [1] * (X_fit.shape[0] - n_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16961, 134410)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fit.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
