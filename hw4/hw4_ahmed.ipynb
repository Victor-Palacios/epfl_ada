{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline\n",
    "\n",
    "Wednesday, November 22, 2017, 11:59PM\n",
    "\n",
    "## Important notes\n",
    "\n",
    "- When you push your Notebook to GitHub, all the cells must already have been evaluated.\n",
    "- Don't forget to add a textual description of your thought process and of any assumptions you've made.\n",
    "- Please write all your comments in English, and use meaningful variable names in your code.\n",
    "\n",
    "## Question 1: Propensity score matching\n",
    "\n",
    "In this exercise, you will apply [propensity score matching](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf), which we discussed in lecture 5 (\"Observational studies\"), in order to draw conclusions from an observational study.\n",
    "\n",
    "We will work with a by-now classic dataset from Robert LaLonde's study \"[Evaluating the Econometric Evaluations of Training Programs](http://people.hbs.edu/nashraf/LaLonde_1986.pdf)\" (1986).\n",
    "The study investigated the effect of a job training program (\"National Supported Work Demonstration\") on the real earnings of an individual, a couple of years after completion of the program.\n",
    "Your task is to determine the effectiveness of the \"treatment\" represented by the job training program.\n",
    "\n",
    "#### Dataset description\n",
    "\n",
    "- `treat`: 1 if the subject participated in the job training program, 0 otherwise\n",
    "- `age`: the subject's age\n",
    "- `educ`: years of education\n",
    "- `race`: categorical variable with three possible values: Black, Hispanic, or White\n",
    "- `married`: 1 if the subject was married at the time of the training program, 0 otherwise\n",
    "- `nodegree`: 1 if the subject has earned no school degree, 0 otherwise\n",
    "- `re74`: real earnings in 1974 (pre-treatment)\n",
    "- `re75`: real earnings in 1975 (pre-treatment)\n",
    "- `re78`: real earnings in 1978 (outcome)\n",
    "\n",
    "If you want to brush up your knowledge on propensity scores and observational studies, we highly recommend Rosenbaum's excellent book on the [\"Design of Observational Studies\"](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf). Even just reading the first chapter (18 pages) will help you a lot.\n",
    "\n",
    "#### 1. A naive analysis\n",
    "\n",
    "Compare the distribution of the outcome variable (`re78`) between the two groups, using plots and numbers.\n",
    "To summarize and compare the distributions, you may use the techniques we discussed in lectures 4 (\"Read the stats carefully\") and 6 (\"Data visualization\").\n",
    "\n",
    "What might a naive \"researcher\" conclude from this superficial analysis?\n",
    "\n",
    "#### 2. A closer look at the data\n",
    "\n",
    "You're not naive, of course (and even if you are, you've learned certain things in ADA), so you aren't content with a superficial analysis such as the above.\n",
    "You're aware of the dangers of observational studies, so you take a closer look at the data before jumping to conclusions.\n",
    "\n",
    "For each feature in the dataset, compare its distribution in the treated group with its distribution in the control group, using plots and numbers.\n",
    "As above, you may use the techniques we discussed in class for summarizing and comparing the distributions.\n",
    "\n",
    "What do you observe?\n",
    "Describe what your observations mean for the conclusions drawn by the naive \"researcher\" from his superficial analysis.\n",
    "\n",
    "#### 3. A propsensity score model\n",
    "\n",
    "Use logistic regression to estimate propensity scores for all points in the dataset.\n",
    "You may use `sklearn` to fit the logistic regression model and apply it to each data point to obtain propensity scores:\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "```\n",
    "\n",
    "Recall that the propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features (in this case, age, education, pre-treatment income, etc.).\n",
    "To brush up on propensity scores, you may read chapter 3.3 of the above-cited book by Rosenbaum or [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).\n",
    "\n",
    "Note: you do not need a train/test split here. Train and apply the model on the entire dataset. If you're wondering why this is the right thing to do in this situation, recall that the propensity score model is not used in order to make predictions about unseen data. Its sole purpose is to balance the dataset across treatment groups.\n",
    "(See p. 74 of Rosenbaum's book for an explanation why slight overfitting is even good for propensity scores.\n",
    "If you want even more information, read [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).)\n",
    "\n",
    "#### 4. Balancing the dataset via matching\n",
    "\n",
    "Use the propensity scores to match each data point from the treated group with exactly one data point from the control group, while ensuring that each data point from the control group is matched with at most one data point from the treated group.\n",
    "(Hint: you may explore the `networkx` package in Python for predefined matching functions.)\n",
    "\n",
    "Your matching should maximize the similarity between matched subjects, as captured by their propensity scores.\n",
    "In other words, the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects should be minimized.\n",
    "\n",
    "After matching, you have as many treated as you have control subjects.\n",
    "Compare the outcomes (`re78`) between the two groups (treated and control).\n",
    "\n",
    "Also, compare again the feature-value distributions between the two groups, as you've done in part 2 above, but now only for the matched subjects.\n",
    "What do you observe?\n",
    "Are you closer to being able to draw valid conclusions now than you were before?\n",
    "\n",
    "\n",
    "#### 5. Balancing the groups further\n",
    "\n",
    "Based on your comparison of feature-value distributions from part 4, are you fully satisfied with your matching?\n",
    "Would you say your dataset is sufficiently balanced?\n",
    "If not, in what ways could the \"balanced\" dataset you have obtained still not allow you to draw valid conclusions?\n",
    "\n",
    "Improve your matching by explicitly making sure that you match only subjects that have the same value for the problematic feature.\n",
    "Argue with numbers and plots that the two groups (treated and control) are now better balanced than after part 4.\n",
    "\n",
    "\n",
    "#### 6. A less naive analysis\n",
    "\n",
    "Compare the outcomes (`re78`) between treated and control subjects, as you've done in part 1, but now only for the matched dataset you've obtained from part 5.\n",
    "What do you conclude about the effectiveness of the job training program?\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## Question 2: Applied ML\n",
    "\n",
    "We are going to build a classifier of news to directly assign them to 20 news categories. Note that the pipeline that you will build in this exercise could be of great help during your project if you plan to work with text!\n",
    "\n",
    "1. Load the 20newsgroup dataset. It is, again, a classic dataset that can directly be loaded using sklearn ([link](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)).  \n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequencyâ€“inverse document frequency, is of great help when if comes to compute textual features. Indeed, it gives more importance to terms that are more specific to the considered articles (TF) but reduces the importance of terms that are very frequent in the entire corpus (IDF). Compute TF-IDF features for every article using [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Then, split your dataset into a training, a testing and a validation set (10% for validation and 10% for testing). Each observation should be paired with its corresponding label (the article category).\n",
    "\n",
    "\n",
    "2. Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSW1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9930.0460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSW2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3595.8940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NSW3</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24909.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NSW4</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7506.1460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NSW5</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.7899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  treat  age  educ  black  hispan  married  nodegree  re74  re75  \\\n",
       "0  NSW1      1   37    11      1       0        1         1   0.0   0.0   \n",
       "1  NSW2      1   22     9      0       1        0         1   0.0   0.0   \n",
       "2  NSW3      1   30    12      1       0        0         0   0.0   0.0   \n",
       "3  NSW4      1   27    11      1       0        0         1   0.0   0.0   \n",
       "4  NSW5      1   33     8      1       0        0         1   0.0   0.0   \n",
       "\n",
       "         re78  \n",
       "0   9930.0460  \n",
       "1   3595.8940  \n",
       "2  24909.4500  \n",
       "3   7506.1460  \n",
       "4    289.7899  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde = pd.read_csv(r'lalonde.csv')\n",
    "data_lalonde.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's do a little describe to see what we have here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.301303</td>\n",
       "      <td>27.363192</td>\n",
       "      <td>10.268730</td>\n",
       "      <td>0.395765</td>\n",
       "      <td>0.117264</td>\n",
       "      <td>0.415309</td>\n",
       "      <td>0.630293</td>\n",
       "      <td>4557.546569</td>\n",
       "      <td>2184.938207</td>\n",
       "      <td>6792.834483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.459198</td>\n",
       "      <td>9.881187</td>\n",
       "      <td>2.628325</td>\n",
       "      <td>0.489413</td>\n",
       "      <td>0.321997</td>\n",
       "      <td>0.493177</td>\n",
       "      <td>0.483119</td>\n",
       "      <td>6477.964479</td>\n",
       "      <td>3295.679043</td>\n",
       "      <td>7470.730792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>238.283425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1042.330000</td>\n",
       "      <td>601.548400</td>\n",
       "      <td>4759.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7888.498250</td>\n",
       "      <td>3248.987500</td>\n",
       "      <td>10893.592500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35040.070000</td>\n",
       "      <td>25142.240000</td>\n",
       "      <td>60307.930000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            treat         age        educ       black      hispan     married  \\\n",
       "count  614.000000  614.000000  614.000000  614.000000  614.000000  614.000000   \n",
       "mean     0.301303   27.363192   10.268730    0.395765    0.117264    0.415309   \n",
       "std      0.459198    9.881187    2.628325    0.489413    0.321997    0.493177   \n",
       "min      0.000000   16.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000   20.000000    9.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000   25.000000   11.000000    0.000000    0.000000    0.000000   \n",
       "75%      1.000000   32.000000   12.000000    1.000000    0.000000    1.000000   \n",
       "max      1.000000   55.000000   18.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree          re74          re75          re78  \n",
       "count  614.000000    614.000000    614.000000    614.000000  \n",
       "mean     0.630293   4557.546569   2184.938207   6792.834483  \n",
       "std      0.483119   6477.964479   3295.679043   7470.730792  \n",
       "min      0.000000      0.000000      0.000000      0.000000  \n",
       "25%      0.000000      0.000000      0.000000    238.283425  \n",
       "50%      1.000000   1042.330000    601.548400   4759.018500  \n",
       "75%      1.000000   7888.498250   3248.987500  10893.592500  \n",
       "max      1.000000  35040.070000  25142.240000  60307.930000  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing really special to say here. These value do not mean much by themselves. They just show some basic information. Let's answer question 1 now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_treat = data_lalonde[\"treat\"] == 1\n",
    "index_no_treat = data_lalonde[\"treat\"] == 0\n",
    "data_lalonde_treat = data_lalonde[index_treat]\n",
    "data_lalonde_no_treat = data_lalonde[index_no_treat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just checking of there are no \"outliers\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde_treat.shape[0] + data_lalonde_no_treat.shape[0] == data_lalonde.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed Kulovic\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['logistic']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x243345216a0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB8AAAIMCAYAAAC9qgSnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsWjfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24334519b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (18, 9)\n",
    "plt.figure()\n",
    "\n",
    "data_lalonde_treat.re78.plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x243355bcc50>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCYAAAIMCAYAAAApTK1VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nESYAAACAMf8f4SetFIBIOYsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x243355cb630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lalonde_no_treat.re78.plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe not informative enough. We see that there are a few \"outliers\" in the treat case but generally speaking there are still more people on the left. Let's try to put everything on the same histogram so as to make the information clearer with respect to the scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBgAAAIMCAYAAABfdh/eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24335948e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 65000, 66)\n",
    "\n",
    "plt.hist(data_lalonde_treat.re78.tolist(), bins, alpha=0.5, label=\"treat\")\n",
    "plt.hist(data_lalonde_no_treat.re78.tolist(), bins, alpha=0.5, label=\"no treat\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't tell us much about the data to be honest. We clearly see that there are some outliers who make a lot of money on the far right but they are a minority so we cannot say something about that. Let's describe the two dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>185.0</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.816216</td>\n",
       "      <td>10.345946</td>\n",
       "      <td>0.843243</td>\n",
       "      <td>0.059459</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.708108</td>\n",
       "      <td>2095.573689</td>\n",
       "      <td>1532.055314</td>\n",
       "      <td>6349.143530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.155019</td>\n",
       "      <td>2.010650</td>\n",
       "      <td>0.364558</td>\n",
       "      <td>0.237124</td>\n",
       "      <td>0.392722</td>\n",
       "      <td>0.455867</td>\n",
       "      <td>4886.620353</td>\n",
       "      <td>3219.250870</td>\n",
       "      <td>7867.402218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>485.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4232.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1291.468000</td>\n",
       "      <td>1817.284000</td>\n",
       "      <td>9642.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35040.070000</td>\n",
       "      <td>25142.240000</td>\n",
       "      <td>60307.930000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       treat         age        educ       black      hispan     married  \\\n",
       "count  185.0  185.000000  185.000000  185.000000  185.000000  185.000000   \n",
       "mean     1.0   25.816216   10.345946    0.843243    0.059459    0.189189   \n",
       "std      0.0    7.155019    2.010650    0.364558    0.237124    0.392722   \n",
       "min      1.0   17.000000    4.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.0   20.000000    9.000000    1.000000    0.000000    0.000000   \n",
       "50%      1.0   25.000000   11.000000    1.000000    0.000000    0.000000   \n",
       "75%      1.0   29.000000   12.000000    1.000000    0.000000    0.000000   \n",
       "max      1.0   48.000000   16.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree          re74          re75          re78  \n",
       "count  185.000000    185.000000    185.000000    185.000000  \n",
       "mean     0.708108   2095.573689   1532.055314   6349.143530  \n",
       "std      0.455867   4886.620353   3219.250870   7867.402218  \n",
       "min      0.000000      0.000000      0.000000      0.000000  \n",
       "25%      0.000000      0.000000      0.000000    485.229800  \n",
       "50%      1.000000      0.000000      0.000000   4232.309000  \n",
       "75%      1.000000   1291.468000   1817.284000   9642.999000  \n",
       "max      1.000000  35040.070000  25142.240000  60307.930000  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde_treat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.0</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.030303</td>\n",
       "      <td>10.235431</td>\n",
       "      <td>0.202797</td>\n",
       "      <td>0.142191</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.596737</td>\n",
       "      <td>5619.236506</td>\n",
       "      <td>2466.484443</td>\n",
       "      <td>6984.169742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.786653</td>\n",
       "      <td>2.855238</td>\n",
       "      <td>0.402552</td>\n",
       "      <td>0.349654</td>\n",
       "      <td>0.500419</td>\n",
       "      <td>0.491126</td>\n",
       "      <td>6788.750796</td>\n",
       "      <td>3291.996183</td>\n",
       "      <td>7294.161791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2547.047000</td>\n",
       "      <td>1086.726000</td>\n",
       "      <td>4975.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9277.128000</td>\n",
       "      <td>3881.419000</td>\n",
       "      <td>11688.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25862.320000</td>\n",
       "      <td>18347.230000</td>\n",
       "      <td>25564.670000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       treat         age        educ       black      hispan     married  \\\n",
       "count  429.0  429.000000  429.000000  429.000000  429.000000  429.000000   \n",
       "mean     0.0   28.030303   10.235431    0.202797    0.142191    0.512821   \n",
       "std      0.0   10.786653    2.855238    0.402552    0.349654    0.500419   \n",
       "min      0.0   16.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.0   19.000000    9.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.0   25.000000   11.000000    0.000000    0.000000    1.000000   \n",
       "75%      0.0   35.000000   12.000000    0.000000    0.000000    1.000000   \n",
       "max      0.0   55.000000   18.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree          re74          re75          re78  \n",
       "count  429.000000    429.000000    429.000000    429.000000  \n",
       "mean     0.596737   5619.236506   2466.484443   6984.169742  \n",
       "std      0.491126   6788.750796   3291.996183   7294.161791  \n",
       "min      0.000000      0.000000      0.000000      0.000000  \n",
       "25%      0.000000      0.000000      0.000000    220.181300  \n",
       "50%      1.000000   2547.047000   1086.726000   4975.505000  \n",
       "75%      1.000000   9277.128000   3881.419000  11688.820000  \n",
       "max      1.000000  25862.320000  18347.230000  25564.670000  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde_no_treat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that the value has nothing special to do with treat or no treat. Somehow we see that the selection was far from uniform at random. In the treat group there are more people with no degree than with degree. The \"black\" variable is interesting also. We intuitively make associate the fact that there are a lot more black people to more difficult social conditions. We can also note that there is a three years difference in mean age between the two groups so, in a sense, we guess that people in the no treat group have more experience on average and this should imply the higher mean salary in the no treat group. We can also say that the age parameter has a lot of impact on the nodegree and the married parameters. Somehow, being 3 years older implies a higher probability of being married and a higher probability of having a degree.\n",
    "\n",
    "Now we want to measure the propensity scores using logistic regression in order to be able to improve our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# We won't include re78 since this is the salary after the subject is treated\n",
    "features = [\"age\", \"educ\", \"black\", \"hispan\", \"married\", \"nodegree\", \"re74\", \"re75\"]\n",
    "X = data_lalonde[features]\n",
    "y = data_lalonde.treat\n",
    "\n",
    "logistic.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80781758957654726"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for a start. We are really far from 0.5 which could mean that we are really far from a randomized experiment. Bad this score is not really informative, what we want here is the propensity score measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "propensity = [b for [a,b] in logistic.predict_proba(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our propensity scores for all \"nodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4433504229538332,\n",
       " 0.14465953271343432,\n",
       " 0.72235463272580003,\n",
       " 0.66415051683598725,\n",
       " 0.69828561078482354]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensity[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want now is to do matching : for example, if we take the first person in our dataset (the one with propensity score 0.443...), we would like to match them with another person whose propensity score is very close to 0.44 but which is in the opposite group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to separate the dataset into the treat and no treat groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_propensity = data_lalonde.copy()\n",
    "data_propensity[\"propensity\"] = propensity\n",
    "\n",
    "data_treat_propensity = data_propensity[data_propensity.treat == 1]\n",
    "data_no_treat_propensity = data_propensity[data_propensity.treat == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go and create a bipartite graph. So our first strategy (we will find another one if this one doesn't work) is that the weight of the edge between the two nodes is the absolute difference in propensity scores between them. And then we will use networkx to find a min-weight matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 20)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-621384be7ea9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow_j\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_no_treat_propensity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Since the method we use is based on max matching we use negative value to get min matching\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"propensity\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrow_j\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"propensity\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\networkx\\classes\\graph.py\u001b[0m in \u001b[0;36madd_edge\u001b[1;34m(self, u, v, **attr)\u001b[0m\n\u001b[0;32m    870\u001b[0m         \"\"\"\n\u001b[0;32m    871\u001b[0m         \u001b[1;31m# add nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mu\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjlist_inner_dict_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "G.add_nodes_from(range(data_treat_propensity.shape[0]), bipartite=0)\n",
    "G.add_nodes_from(range(data_no_treat_propensity.shape[0]), bipartite=1)\n",
    "\n",
    "for i, row_i in data_treat_propensity.iterrows():\n",
    "    for j, row_j in data_no_treat_propensity.iterrows():\n",
    "        # Since the method we use is based on max matching we use negative value to get min matching\n",
    "        G.add_edge(i, j, weight=-abs(row_i[\"propensity\"] - row_j[\"propensity\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compute the matching (it takes around a minute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matching_propensity_max_card = nx.max_weight_matching(G, maxcardinality=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the min weight matching without max cardinality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_propensity = nx.max_weight_matching(G)\n",
    "matching_propensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the problem, as we can see, is that we end up with too few datapoints in the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_matching(matching, verbose):\n",
    "    sum_differences = 0\n",
    "    for (a, b) in matching.items():\n",
    "        if a < data_treat_propensity.shape[0] / 2:\n",
    "            propensity_score_a = data_propensity.loc[a][\"propensity\"]\n",
    "            propensity_score_b = data_propensity.loc[b][\"propensity\"]\n",
    "            diff = abs(propensity_score_a - propensity_score_b)\n",
    "            if verbose:\n",
    "                print(\"Difference between {} and {} is {}\".format(a, b, diff))\n",
    "            sum_differences += diff\n",
    "    print(\"Mean difference in prop score is {}\".format(sum_differences / len(matching) / 2))\n",
    "    \n",
    "observe_matching(matching_propensity, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the problem with using min matching without max cardinality. The algorithm will compute the \"real\" min matching and it will only match the points with the same propensity scores for that. We will continue with the matchings we obtained from max cardinality matchings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_matching(matching_propensity_max_card, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matching takes into account all datapoints from the treat (the smallest) group. So it has 185 matchings. In general those matchings are pretty good, very similar points are matched with each other. But if we inspect the values we see that there are some \"outliers\" matchings since all points have to be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propensity_differences_matchings = list(filter(lambda x : x[0] < 185, \n",
    "                                               map(lambda matching: (matching[0], abs(data_propensity.loc[matching[0]][\"propensity\"] - data_propensity.loc[matching[1]][\"propensity\"])), \n",
    "                                                   matching_propensity_max_card.items())))\n",
    "propensity_differences_matchings = [prop for (index, prop) in propensity_differences_matchings]\n",
    "plt.hist(propensity_differences_matchings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this histogram we see that there are some bothering matchings where the differences in propensity scores are more than 0.5. A third alternative may be to apply a threshold after the matching to get rid of those values but let's already see how far we can go with our 185-matchings set\n",
    "\n",
    "Now let's look at the re78 values for both groups but we will take into account only the matched datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_treat = [a for (a, b) in matching_propensity_max_card.items() if a < data_treat_propensity.shape[0]]\n",
    "group_no_treat = [b for (a, b) in matching_propensity_max_card.items() if a < data_treat_propensity.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 65000, 66)\n",
    "plt.hist(data_lalonde.loc[group_treat].re78.tolist(), bins, alpha=0.5, label=\"treat\")\n",
    "plt.hist(data_lalonde.loc[group_no_treat].re78.tolist(), bins, alpha=0.5, label=\"no treat\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is our distribution. Maybe we can argue we see things in a better way on this histogram. The numer of samples is exactly the same for both groups (the number of samples in the treat group) and the two groups were obtained using the min matching method on the bipartite graph. Here we clearly see that the \"extreme\" outliers belong to the treat group. But however, there are not so many of them. We can also see that the first bin (the least salary) contains more people from the non treated than the treated group (however, if we look at the re78 column in out dataset we see that there are a lot of 0 values so either the dataset has errors or those people are simply unemployed). So the only thing we might try to guess from this is that the treat works well on a few people: the blue outliers. (but maybe those people anyway had more determination and this determination is what caused them to take the treat so the variable is something that we do not have in our dataset: the determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tweak things to see if we can get better insights. The first thing we want to try is to simply remove the matchings where the difference is too big. We will end up with much less datapoints but we might get something out of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_matching(matching, threshold=0.4):\n",
    "    def difference_matching(x, y):\n",
    "        return abs(data_propensity.loc[x][\"propensity\"] - data_propensity.loc[y][\"propensity\"])\n",
    "\n",
    "    matching_propensity_max_card_pairs = filter(lambda x : x[0] < 185, matching_propensity_max_card.items())\n",
    "    matching_propensity_max_card_pairs = map(lambda x : (x[0], x[1], difference_matching(x[0], x[1])), matching_propensity_max_card_pairs)\n",
    "    matching_propensity_max_card_pairs = filter(lambda x : x[2] <= threshold, matching_propensity_max_card_pairs)\n",
    "    return list(matching_propensity_max_card_pairs)\n",
    "\n",
    "def plot_matching_hist(matching, threshold=0.4):\n",
    "    bins = np.linspace(0, 65000, 66)\n",
    "    matchings = filter_matching(matching, threshold)\n",
    "    group_treat = [a for (a, b, prop) in matchings]\n",
    "    group_no_treat = [b for (a, b, prop) in matchings]\n",
    "    plt.hist(data_lalonde.loc[group_treat].re78.tolist(), bins, alpha=0.5, label=\"treat\")\n",
    "    plt.hist(data_lalonde.loc[group_no_treat].re78.tolist(), bins, alpha=0.5, label=\"no treat\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we get with the threshold at 0.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And at 0.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0.1 (very similar propensity scores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0.05 (very very similar propensity scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0 (Exactly the same propensity scores) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general thing we always see no matter the threshold is that the treat group always has those outliers who have really big salaries (except in the threshold 0 case where they do not appear)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we might try to get rid of some columns to see if we can get better insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Applied ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroup_train = fetch_20newsgroups(subset=\"train\")\n",
    "pprint(list(newsgroup_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroup_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "newsgroup_test = fetch_20newsgroups(subset=\"test\")\n",
    "vectors_test = vectorizer.transform(newsgroup_test.data)\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectors, newsgroup_train.target)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroup_test.target, pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroup = fetch_20newsgroups(subset=\"all\", remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroup.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectors\n",
    "y = newsgroup.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_fit, X_test, y_fit, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Here we put 0.11111111 because 0.1111111... * 0.9 = 0.1 which will give 80% / 10% / 10%\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_fit, y_fit, test_size=0.111111112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a little preview depending on the number of estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As if the score increases depending on the number of estimators. Let's perform a grid search on the number of trees to see what is happening with more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-8f1309842009>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 328\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=200)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_total = X_fit.shape[0]\n",
    "n_val = int(fit_total * 0.1111111)\n",
    "n_train = fit_total - n_val\n",
    "ps_array = [0] * n_train + [1] * n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n",
      "[CV] n_estimators=1, max_depth=1 .....................................\n",
      "[CV] ...................... n_estimators=1, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=1, max_depth=1 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... n_estimators=1, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=1 .....................................\n",
      "[CV] ...................... n_estimators=2, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=1 .....................................\n",
      "[CV] ...................... n_estimators=2, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=1 .....................................\n",
      "[CV] ...................... n_estimators=4, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=1 .....................................\n",
      "[CV] ...................... n_estimators=4, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=10, max_depth=1 ....................................\n",
      "[CV] ..................... n_estimators=10, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=10, max_depth=1 ....................................\n",
      "[CV] ..................... n_estimators=10, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=21, max_depth=1 ....................................\n",
      "[CV] ..................... n_estimators=21, max_depth=1, total=   0.1s\n",
      "[CV] n_estimators=21, max_depth=1 ....................................\n",
      "[CV] ..................... n_estimators=21, max_depth=1, total=   0.1s\n",
      "[CV] n_estimators=46, max_depth=1 ....................................\n",
      "[CV] ..................... n_estimators=46, max_depth=1, total=   0.3s\n",
      "[CV] n_estimators=46, max_depth=1 ....................................\n",
      "[CV] ..................... n_estimators=46, max_depth=1, total=   0.2s\n",
      "[CV] n_estimators=100, max_depth=1 ...................................\n",
      "[CV] .................... n_estimators=100, max_depth=1, total=   0.7s\n",
      "[CV] n_estimators=100, max_depth=1 ...................................\n",
      "[CV] .................... n_estimators=100, max_depth=1, total=   0.5s\n",
      "[CV] n_estimators=215, max_depth=1 ...................................\n",
      "[CV] .................... n_estimators=215, max_depth=1, total=   1.5s\n",
      "[CV] n_estimators=215, max_depth=1 ...................................\n",
      "[CV] .................... n_estimators=215, max_depth=1, total=   1.0s\n",
      "[CV] n_estimators=464, max_depth=1 ...................................\n",
      "[CV] .................... n_estimators=464, max_depth=1, total=   3.3s\n",
      "[CV] n_estimators=464, max_depth=1 ...................................\n",
      "[CV] .................... n_estimators=464, max_depth=1, total=   2.3s\n",
      "[CV] n_estimators=1000, max_depth=1 ..................................\n",
      "[CV] ................... n_estimators=1000, max_depth=1, total=   7.1s\n",
      "[CV] n_estimators=1000, max_depth=1 ..................................\n",
      "[CV] ................... n_estimators=1000, max_depth=1, total=   5.0s\n",
      "[CV] n_estimators=1, max_depth=2 .....................................\n",
      "[CV] ...................... n_estimators=1, max_depth=2, total=   0.0s\n",
      "[CV] n_estimators=1, max_depth=2 .....................................\n",
      "[CV] ...................... n_estimators=1, max_depth=2, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=2 .....................................\n",
      "[CV] ...................... n_estimators=2, max_depth=2, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=2 .....................................\n",
      "[CV] ...................... n_estimators=2, max_depth=2, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=2 .....................................\n",
      "[CV] ...................... n_estimators=4, max_depth=2, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=2 .....................................\n",
      "[CV] ...................... n_estimators=4, max_depth=2, total=   0.0s\n",
      "[CV] n_estimators=10, max_depth=2 ....................................\n",
      "[CV] ..................... n_estimators=10, max_depth=2, total=   0.1s\n",
      "[CV] n_estimators=10, max_depth=2 ....................................\n",
      "[CV] ..................... n_estimators=10, max_depth=2, total=   0.0s\n",
      "[CV] n_estimators=21, max_depth=2 ....................................\n",
      "[CV] ..................... n_estimators=21, max_depth=2, total=   0.1s\n",
      "[CV] n_estimators=21, max_depth=2 ....................................\n",
      "[CV] ..................... n_estimators=21, max_depth=2, total=   0.1s\n",
      "[CV] n_estimators=46, max_depth=2 ....................................\n",
      "[CV] ..................... n_estimators=46, max_depth=2, total=   0.3s\n",
      "[CV] n_estimators=46, max_depth=2 ....................................\n",
      "[CV] ..................... n_estimators=46, max_depth=2, total=   0.3s\n",
      "[CV] n_estimators=100, max_depth=2 ...................................\n",
      "[CV] .................... n_estimators=100, max_depth=2, total=   0.7s\n",
      "[CV] n_estimators=100, max_depth=2 ...................................\n",
      "[CV] .................... n_estimators=100, max_depth=2, total=   0.7s\n",
      "[CV] n_estimators=215, max_depth=2 ...................................\n",
      "[CV] .................... n_estimators=215, max_depth=2, total=   1.6s\n",
      "[CV] n_estimators=215, max_depth=2 ...................................\n",
      "[CV] .................... n_estimators=215, max_depth=2, total=   1.6s\n",
      "[CV] n_estimators=464, max_depth=2 ...................................\n",
      "[CV] .................... n_estimators=464, max_depth=2, total=   3.4s\n",
      "[CV] n_estimators=464, max_depth=2 ...................................\n",
      "[CV] .................... n_estimators=464, max_depth=2, total=   3.6s\n",
      "[CV] n_estimators=1000, max_depth=2 ..................................\n",
      "[CV] ................... n_estimators=1000, max_depth=2, total=   7.4s\n",
      "[CV] n_estimators=1000, max_depth=2 ..................................\n",
      "[CV] ................... n_estimators=1000, max_depth=2, total=   7.7s\n",
      "[CV] n_estimators=1, max_depth=4 .....................................\n",
      "[CV] ...................... n_estimators=1, max_depth=4, total=   0.0s\n",
      "[CV] n_estimators=1, max_depth=4 .....................................\n",
      "[CV] ...................... n_estimators=1, max_depth=4, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=4 .....................................\n",
      "[CV] ...................... n_estimators=2, max_depth=4, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=4 .....................................\n",
      "[CV] ...................... n_estimators=2, max_depth=4, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=4 .....................................\n",
      "[CV] ...................... n_estimators=4, max_depth=4, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=4 .....................................\n",
      "[CV] ...................... n_estimators=4, max_depth=4, total=   0.0s\n",
      "[CV] n_estimators=10, max_depth=4 ....................................\n",
      "[CV] ..................... n_estimators=10, max_depth=4, total=   0.1s\n",
      "[CV] n_estimators=10, max_depth=4 ....................................\n",
      "[CV] ..................... n_estimators=10, max_depth=4, total=   0.1s\n",
      "[CV] n_estimators=21, max_depth=4 ....................................\n",
      "[CV] ..................... n_estimators=21, max_depth=4, total=   0.1s\n",
      "[CV] n_estimators=21, max_depth=4 ....................................\n",
      "[CV] ..................... n_estimators=21, max_depth=4, total=   0.2s\n",
      "[CV] n_estimators=46, max_depth=4 ....................................\n",
      "[CV] ..................... n_estimators=46, max_depth=4, total=   0.4s\n",
      "[CV] n_estimators=46, max_depth=4 ....................................\n",
      "[CV] ..................... n_estimators=46, max_depth=4, total=   0.6s\n",
      "[CV] n_estimators=100, max_depth=4 ...................................\n",
      "[CV] .................... n_estimators=100, max_depth=4, total=   0.8s\n",
      "[CV] n_estimators=100, max_depth=4 ...................................\n",
      "[CV] .................... n_estimators=100, max_depth=4, total=   1.3s\n",
      "[CV] n_estimators=215, max_depth=4 ...................................\n",
      "[CV] .................... n_estimators=215, max_depth=4, total=   1.7s\n",
      "[CV] n_estimators=215, max_depth=4 ...................................\n",
      "[CV] .................... n_estimators=215, max_depth=4, total=   2.8s\n",
      "[CV] n_estimators=464, max_depth=4 ...................................\n",
      "[CV] .................... n_estimators=464, max_depth=4, total=   3.8s\n",
      "[CV] n_estimators=464, max_depth=4 ...................................\n",
      "[CV] .................... n_estimators=464, max_depth=4, total=   6.2s\n",
      "[CV] n_estimators=1000, max_depth=4 ..................................\n",
      "[CV] ................... n_estimators=1000, max_depth=4, total=   8.1s\n",
      "[CV] n_estimators=1000, max_depth=4 ..................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... n_estimators=1000, max_depth=4, total=  13.3s\n",
      "[CV] n_estimators=1, max_depth=10 ....................................\n",
      "[CV] ..................... n_estimators=1, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=1, max_depth=10 ....................................\n",
      "[CV] ..................... n_estimators=1, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=10 ....................................\n",
      "[CV] ..................... n_estimators=2, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=10 ....................................\n",
      "[CV] ..................... n_estimators=2, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=10 ....................................\n",
      "[CV] ..................... n_estimators=4, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=10 ....................................\n",
      "[CV] ..................... n_estimators=4, max_depth=10, total=   0.1s\n",
      "[CV] n_estimators=10, max_depth=10 ...................................\n",
      "[CV] .................... n_estimators=10, max_depth=10, total=   0.1s\n",
      "[CV] n_estimators=10, max_depth=10 ...................................\n",
      "[CV] .................... n_estimators=10, max_depth=10, total=   0.3s\n",
      "[CV] n_estimators=21, max_depth=10 ...................................\n",
      "[CV] .................... n_estimators=21, max_depth=10, total=   0.2s\n",
      "[CV] n_estimators=21, max_depth=10 ...................................\n",
      "[CV] .................... n_estimators=21, max_depth=10, total=   0.7s\n",
      "[CV] n_estimators=46, max_depth=10 ...................................\n",
      "[CV] .................... n_estimators=46, max_depth=10, total=   0.5s\n",
      "[CV] n_estimators=46, max_depth=10 ...................................\n",
      "[CV] .................... n_estimators=46, max_depth=10, total=   1.6s\n",
      "[CV] n_estimators=100, max_depth=10 ..................................\n",
      "[CV] ................... n_estimators=100, max_depth=10, total=   1.2s\n",
      "[CV] n_estimators=100, max_depth=10 ..................................\n",
      "[CV] ................... n_estimators=100, max_depth=10, total=   3.4s\n",
      "[CV] n_estimators=215, max_depth=10 ..................................\n",
      "[CV] ................... n_estimators=215, max_depth=10, total=   2.7s\n",
      "[CV] n_estimators=215, max_depth=10 ..................................\n",
      "[CV] ................... n_estimators=215, max_depth=10, total=   7.5s\n",
      "[CV] n_estimators=464, max_depth=10 ..................................\n",
      "[CV] ................... n_estimators=464, max_depth=10, total=   5.6s\n",
      "[CV] n_estimators=464, max_depth=10 ..................................\n",
      "[CV] ................... n_estimators=464, max_depth=10, total=  16.1s\n",
      "[CV] n_estimators=1000, max_depth=10 .................................\n",
      "[CV] .................. n_estimators=1000, max_depth=10, total=  11.8s\n",
      "[CV] n_estimators=1000, max_depth=10 .................................\n",
      "[CV] .................. n_estimators=1000, max_depth=10, total=  34.4s\n",
      "[CV] n_estimators=1, max_depth=21 ....................................\n",
      "[CV] ..................... n_estimators=1, max_depth=21, total=   0.0s\n",
      "[CV] n_estimators=1, max_depth=21 ....................................\n",
      "[CV] ..................... n_estimators=1, max_depth=21, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=21 ....................................\n",
      "[CV] ..................... n_estimators=2, max_depth=21, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=21 ....................................\n",
      "[CV] ..................... n_estimators=2, max_depth=21, total=   0.1s\n",
      "[CV] n_estimators=4, max_depth=21 ....................................\n",
      "[CV] ..................... n_estimators=4, max_depth=21, total=   0.1s\n",
      "[CV] n_estimators=4, max_depth=21 ....................................\n",
      "[CV] ..................... n_estimators=4, max_depth=21, total=   0.3s\n",
      "[CV] n_estimators=10, max_depth=21 ...................................\n",
      "[CV] .................... n_estimators=10, max_depth=21, total=   0.2s\n",
      "[CV] n_estimators=10, max_depth=21 ...................................\n",
      "[CV] .................... n_estimators=10, max_depth=21, total=   0.9s\n",
      "[CV] n_estimators=21, max_depth=21 ...................................\n",
      "[CV] .................... n_estimators=21, max_depth=21, total=   0.4s\n",
      "[CV] n_estimators=21, max_depth=21 ...................................\n",
      "[CV] .................... n_estimators=21, max_depth=21, total=   1.8s\n",
      "[CV] n_estimators=46, max_depth=21 ...................................\n",
      "[CV] .................... n_estimators=46, max_depth=21, total=   1.1s\n",
      "[CV] n_estimators=46, max_depth=21 ...................................\n",
      "[CV] .................... n_estimators=46, max_depth=21, total=   4.3s\n",
      "[CV] n_estimators=100, max_depth=21 ..................................\n",
      "[CV] ................... n_estimators=100, max_depth=21, total=   2.2s\n",
      "[CV] n_estimators=100, max_depth=21 ..................................\n",
      "[CV] ................... n_estimators=100, max_depth=21, total=   8.8s\n",
      "[CV] n_estimators=215, max_depth=21 ..................................\n",
      "[CV] ................... n_estimators=215, max_depth=21, total=   4.9s\n",
      "[CV] n_estimators=215, max_depth=21 ..................................\n",
      "[CV] ................... n_estimators=215, max_depth=21, total=  19.2s\n",
      "[CV] n_estimators=464, max_depth=21 ..................................\n",
      "[CV] ................... n_estimators=464, max_depth=21, total=  10.7s\n",
      "[CV] n_estimators=464, max_depth=21 ..................................\n",
      "[CV] ................... n_estimators=464, max_depth=21, total=  41.4s\n",
      "[CV] n_estimators=1000, max_depth=21 .................................\n",
      "[CV] .................. n_estimators=1000, max_depth=21, total=  22.7s\n",
      "[CV] n_estimators=1000, max_depth=21 .................................\n",
      "[CV] .................. n_estimators=1000, max_depth=21, total= 1.5min\n",
      "[CV] n_estimators=1, max_depth=46 ....................................\n",
      "[CV] ..................... n_estimators=1, max_depth=46, total=   0.0s\n",
      "[CV] n_estimators=1, max_depth=46 ....................................\n",
      "[CV] ..................... n_estimators=1, max_depth=46, total=   0.2s\n",
      "[CV] n_estimators=2, max_depth=46 ....................................\n",
      "[CV] ..................... n_estimators=2, max_depth=46, total=   0.1s\n",
      "[CV] n_estimators=2, max_depth=46 ....................................\n",
      "[CV] ..................... n_estimators=2, max_depth=46, total=   0.6s\n",
      "[CV] n_estimators=4, max_depth=46 ....................................\n",
      "[CV] ..................... n_estimators=4, max_depth=46, total=   0.2s\n",
      "[CV] n_estimators=4, max_depth=46 ....................................\n",
      "[CV] ..................... n_estimators=4, max_depth=46, total=   1.0s\n",
      "[CV] n_estimators=10, max_depth=46 ...................................\n",
      "[CV] .................... n_estimators=10, max_depth=46, total=   0.5s\n",
      "[CV] n_estimators=10, max_depth=46 ...................................\n",
      "[CV] .................... n_estimators=10, max_depth=46, total=   2.8s\n",
      "[CV] n_estimators=21, max_depth=46 ...................................\n",
      "[CV] .................... n_estimators=21, max_depth=46, total=   1.1s\n",
      "[CV] n_estimators=21, max_depth=46 ...................................\n",
      "[CV] .................... n_estimators=21, max_depth=46, total=   5.4s\n",
      "[CV] n_estimators=46, max_depth=46 ...................................\n",
      "[CV] .................... n_estimators=46, max_depth=46, total=   2.3s\n",
      "[CV] n_estimators=46, max_depth=46 ...................................\n",
      "[CV] .................... n_estimators=46, max_depth=46, total=  12.8s\n",
      "[CV] n_estimators=100, max_depth=46 ..................................\n",
      "[CV] ................... n_estimators=100, max_depth=46, total=   5.2s\n",
      "[CV] n_estimators=100, max_depth=46 ..................................\n",
      "[CV] ................... n_estimators=100, max_depth=46, total=  26.5s\n",
      "[CV] n_estimators=215, max_depth=46 ..................................\n",
      "[CV] ................... n_estimators=215, max_depth=46, total=  11.2s\n",
      "[CV] n_estimators=215, max_depth=46 ..................................\n",
      "[CV] ................... n_estimators=215, max_depth=46, total=61.0min\n",
      "[CV] n_estimators=464, max_depth=46 ..................................\n",
      "[CV] ................... n_estimators=464, max_depth=46, total=  26.8s\n",
      "[CV] n_estimators=464, max_depth=46 ..................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... n_estimators=464, max_depth=46, total= 2.3min\n",
      "[CV] n_estimators=1000, max_depth=46 .................................\n",
      "[CV] .................. n_estimators=1000, max_depth=46, total=  53.1s\n",
      "[CV] n_estimators=1000, max_depth=46 .................................\n",
      "[CV] .................. n_estimators=1000, max_depth=46, total= 4.5min\n",
      "[CV] n_estimators=1, max_depth=100 ...................................\n",
      "[CV] .................... n_estimators=1, max_depth=100, total=   0.1s\n",
      "[CV] n_estimators=1, max_depth=100 ...................................\n",
      "[CV] .................... n_estimators=1, max_depth=100, total=   0.6s\n",
      "[CV] n_estimators=2, max_depth=100 ...................................\n",
      "[CV] .................... n_estimators=2, max_depth=100, total=   0.2s\n",
      "[CV] n_estimators=2, max_depth=100 ...................................\n",
      "[CV] .................... n_estimators=2, max_depth=100, total=   1.5s\n",
      "[CV] n_estimators=4, max_depth=100 ...................................\n",
      "[CV] .................... n_estimators=4, max_depth=100, total=   0.5s\n",
      "[CV] n_estimators=4, max_depth=100 ...................................\n",
      "[CV] .................... n_estimators=4, max_depth=100, total=   3.5s\n",
      "[CV] n_estimators=10, max_depth=100 ..................................\n",
      "[CV] ................... n_estimators=10, max_depth=100, total=   1.3s\n",
      "[CV] n_estimators=10, max_depth=100 ..................................\n",
      "[CV] ................... n_estimators=10, max_depth=100, total=   6.8s\n",
      "[CV] n_estimators=21, max_depth=100 ..................................\n",
      "[CV] ................... n_estimators=21, max_depth=100, total=   2.4s\n",
      "[CV] n_estimators=21, max_depth=100 ..................................\n",
      "[CV] ................... n_estimators=21, max_depth=100, total=  15.1s\n",
      "[CV] n_estimators=46, max_depth=100 ..................................\n",
      "[CV] ................... n_estimators=46, max_depth=100, total=   5.3s\n",
      "[CV] n_estimators=46, max_depth=100 ..................................\n",
      "[CV] ................... n_estimators=46, max_depth=100, total=  37.4s\n",
      "[CV] n_estimators=100, max_depth=100 .................................\n",
      "[CV] .................. n_estimators=100, max_depth=100, total=  11.8s\n",
      "[CV] n_estimators=100, max_depth=100 .................................\n",
      "[CV] .................. n_estimators=100, max_depth=100, total= 1.3min\n",
      "[CV] n_estimators=215, max_depth=100 .................................\n",
      "[CV] .................. n_estimators=215, max_depth=100, total=  24.5s\n",
      "[CV] n_estimators=215, max_depth=100 .................................\n",
      "[CV] .................. n_estimators=215, max_depth=100, total= 2.6min\n",
      "[CV] n_estimators=464, max_depth=100 .................................\n",
      "[CV] .................. n_estimators=464, max_depth=100, total=  51.5s\n",
      "[CV] n_estimators=464, max_depth=100 .................................\n",
      "[CV] .................. n_estimators=464, max_depth=100, total= 5.6min\n",
      "[CV] n_estimators=1000, max_depth=100 ................................\n",
      "[CV] ................. n_estimators=1000, max_depth=100, total= 1.8min\n",
      "[CV] n_estimators=1000, max_depth=100 ................................\n",
      "[CV] ................. n_estimators=1000, max_depth=100, total=10.9min\n",
      "[CV] n_estimators=1, max_depth=215 ...................................\n",
      "[CV] .................... n_estimators=1, max_depth=215, total=   0.1s\n",
      "[CV] n_estimators=1, max_depth=215 ...................................\n",
      "[CV] .................... n_estimators=1, max_depth=215, total=   1.2s\n",
      "[CV] n_estimators=2, max_depth=215 ...................................\n",
      "[CV] .................... n_estimators=2, max_depth=215, total=   0.3s\n",
      "[CV] n_estimators=2, max_depth=215 ...................................\n",
      "[CV] .................... n_estimators=2, max_depth=215, total=   2.0s\n",
      "[CV] n_estimators=4, max_depth=215 ...................................\n",
      "[CV] .................... n_estimators=4, max_depth=215, total=   0.6s\n",
      "[CV] n_estimators=4, max_depth=215 ...................................\n",
      "[CV] .................... n_estimators=4, max_depth=215, total=   4.3s\n",
      "[CV] n_estimators=10, max_depth=215 ..................................\n",
      "[CV] ................... n_estimators=10, max_depth=215, total=   1.4s\n",
      "[CV] n_estimators=10, max_depth=215 ..................................\n",
      "[CV] ................... n_estimators=10, max_depth=215, total=  11.0s\n",
      "[CV] n_estimators=21, max_depth=215 ..................................\n",
      "[CV] ................... n_estimators=21, max_depth=215, total=   3.1s\n",
      "[CV] n_estimators=21, max_depth=215 ..................................\n",
      "[CV] ................... n_estimators=21, max_depth=215, total=  23.1s\n",
      "[CV] n_estimators=46, max_depth=215 ..................................\n",
      "[CV] ................... n_estimators=46, max_depth=215, total=   6.9s\n",
      "[CV] n_estimators=46, max_depth=215 ..................................\n",
      "[CV] ................... n_estimators=46, max_depth=215, total=  49.9s\n",
      "[CV] n_estimators=100, max_depth=215 .................................\n",
      "[CV] .................. n_estimators=100, max_depth=215, total=  28.4s\n",
      "[CV] n_estimators=100, max_depth=215 .................................\n",
      "[CV] .................. n_estimators=100, max_depth=215, total= 1.8min\n",
      "[CV] n_estimators=215, max_depth=215 .................................\n",
      "[CV] .................. n_estimators=215, max_depth=215, total=  32.2s\n",
      "[CV] n_estimators=215, max_depth=215 .................................\n",
      "[CV] .................. n_estimators=215, max_depth=215, total= 3.9min\n",
      "[CV] n_estimators=464, max_depth=215 .................................\n",
      "[CV] .................. n_estimators=464, max_depth=215, total= 1.2min\n",
      "[CV] n_estimators=464, max_depth=215 .................................\n",
      "[CV] .................. n_estimators=464, max_depth=215, total= 9.7min\n",
      "[CV] n_estimators=1000, max_depth=215 ................................\n",
      "[CV] ................. n_estimators=1000, max_depth=215, total= 2.8min\n",
      "[CV] n_estimators=1000, max_depth=215 ................................\n",
      "[CV] ................. n_estimators=1000, max_depth=215, total=20.9min\n",
      "[CV] n_estimators=1, max_depth=464 ...................................\n",
      "[CV] .................... n_estimators=1, max_depth=464, total=   0.2s\n",
      "[CV] n_estimators=1, max_depth=464 ...................................\n",
      "[CV] .................... n_estimators=1, max_depth=464, total=   1.4s\n",
      "[CV] n_estimators=2, max_depth=464 ...................................\n",
      "[CV] .................... n_estimators=2, max_depth=464, total=   0.3s\n",
      "[CV] n_estimators=2, max_depth=464 ...................................\n",
      "[CV] .................... n_estimators=2, max_depth=464, total=   2.7s\n",
      "[CV] n_estimators=4, max_depth=464 ...................................\n",
      "[CV] .................... n_estimators=4, max_depth=464, total=   0.7s\n",
      "[CV] n_estimators=4, max_depth=464 ...................................\n",
      "[CV] .................... n_estimators=4, max_depth=464, total=   6.0s\n",
      "[CV] n_estimators=10, max_depth=464 ..................................\n",
      "[CV] ................... n_estimators=10, max_depth=464, total=   1.8s\n",
      "[CV] n_estimators=10, max_depth=464 ..................................\n",
      "[CV] ................... n_estimators=10, max_depth=464, total=  15.0s\n",
      "[CV] n_estimators=21, max_depth=464 ..................................\n",
      "[CV] ................... n_estimators=21, max_depth=464, total=   4.0s\n",
      "[CV] n_estimators=21, max_depth=464 ..................................\n",
      "[CV] ................... n_estimators=21, max_depth=464, total=  31.5s\n",
      "[CV] n_estimators=46, max_depth=464 ..................................\n",
      "[CV] ................... n_estimators=46, max_depth=464, total=   8.1s\n",
      "[CV] n_estimators=46, max_depth=464 ..................................\n",
      "[CV] ................... n_estimators=46, max_depth=464, total= 1.2min\n",
      "[CV] n_estimators=100, max_depth=464 .................................\n",
      "[CV] .................. n_estimators=100, max_depth=464, total=  17.8s\n",
      "[CV] n_estimators=100, max_depth=464 .................................\n",
      "[CV] .................. n_estimators=100, max_depth=464, total= 2.5min\n",
      "[CV] n_estimators=215, max_depth=464 .................................\n",
      "[CV] .................. n_estimators=215, max_depth=464, total=  39.4s\n",
      "[CV] n_estimators=215, max_depth=464 .................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. n_estimators=215, max_depth=464, total= 5.4min\n",
      "[CV] n_estimators=464, max_depth=464 .................................\n",
      "[CV] .................. n_estimators=464, max_depth=464, total= 1.4min\n",
      "[CV] n_estimators=464, max_depth=464 .................................\n",
      "[CV] .................. n_estimators=464, max_depth=464, total=11.7min\n",
      "[CV] n_estimators=1000, max_depth=464 ................................\n",
      "[CV] ................. n_estimators=1000, max_depth=464, total= 3.1min\n",
      "[CV] n_estimators=1000, max_depth=464 ................................\n",
      "[CV] ................. n_estimators=1000, max_depth=464, total=25.1min\n",
      "[CV] n_estimators=1, max_depth=1000 ..................................\n",
      "[CV] ................... n_estimators=1, max_depth=1000, total=   0.1s\n",
      "[CV] n_estimators=1, max_depth=1000 ..................................\n",
      "[CV] ................... n_estimators=1, max_depth=1000, total=   1.3s\n",
      "[CV] n_estimators=2, max_depth=1000 ..................................\n",
      "[CV] ................... n_estimators=2, max_depth=1000, total=   0.3s\n",
      "[CV] n_estimators=2, max_depth=1000 ..................................\n",
      "[CV] ................... n_estimators=2, max_depth=1000, total=   2.8s\n",
      "[CV] n_estimators=4, max_depth=1000 ..................................\n",
      "[CV] ................... n_estimators=4, max_depth=1000, total=   0.6s\n",
      "[CV] n_estimators=4, max_depth=1000 ..................................\n",
      "[CV] ................... n_estimators=4, max_depth=1000, total=   5.1s\n",
      "[CV] n_estimators=10, max_depth=1000 .................................\n",
      "[CV] .................. n_estimators=10, max_depth=1000, total=   1.6s\n",
      "[CV] n_estimators=10, max_depth=1000 .................................\n",
      "[CV] .................. n_estimators=10, max_depth=1000, total=  13.1s\n",
      "[CV] n_estimators=21, max_depth=1000 .................................\n",
      "[CV] .................. n_estimators=21, max_depth=1000, total=   3.4s\n",
      "[CV] n_estimators=21, max_depth=1000 .................................\n",
      "[CV] .................. n_estimators=21, max_depth=1000, total=  27.2s\n",
      "[CV] n_estimators=46, max_depth=1000 .................................\n",
      "[CV] .................. n_estimators=46, max_depth=1000, total=   7.2s\n",
      "[CV] n_estimators=46, max_depth=1000 .................................\n",
      "[CV] .................. n_estimators=46, max_depth=1000, total= 1.0min\n",
      "[CV] n_estimators=100, max_depth=1000 ................................\n",
      "[CV] ................. n_estimators=100, max_depth=1000, total=  16.3s\n",
      "[CV] n_estimators=100, max_depth=1000 ................................\n",
      "[CV] ................. n_estimators=100, max_depth=1000, total= 2.2min\n",
      "[CV] n_estimators=215, max_depth=1000 ................................\n",
      "[CV] ................. n_estimators=215, max_depth=1000, total=  34.9s\n",
      "[CV] n_estimators=215, max_depth=1000 ................................\n",
      "[CV] ................. n_estimators=215, max_depth=1000, total= 4.7min\n",
      "[CV] n_estimators=464, max_depth=1000 ................................\n",
      "[CV] ................. n_estimators=464, max_depth=1000, total= 1.3min\n",
      "[CV] n_estimators=464, max_depth=1000 ................................\n",
      "[CV] ................. n_estimators=464, max_depth=1000, total=10.1min\n",
      "[CV] n_estimators=1000, max_depth=1000 ...............................\n",
      "[CV] ................ n_estimators=1000, max_depth=1000, total= 2.7min\n",
      "[CV] n_estimators=1000, max_depth=1000 ...............................\n",
      "[CV] ................ n_estimators=1000, max_depth=1000, total=53.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed: 278.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([0, 0, ..., 1, 1])),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': array([   1,    2,    4,   10,   21,   46,  100,  215,  464, 1000]), 'max_depth': array([   1,    2,    4,   10,   21,   46,  100,  215,  464, 1000])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "n_estimators_space = np.logspace(0, 3, num=10, dtype='int')\n",
    "max_depth_space = np.logspace(0, 3, num=10, dtype='int')\n",
    "rfc = RandomForestClassifier()\n",
    "clf_grid = GridSearchCV(rfc, param_grid={'n_estimators':n_estimators_space, 'max_depth':max_depth_space}, verbose=2, cv=PredefinedSplit(ps_array))\n",
    "clf_grid.fit(X_fit, y_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(clf_grid, open(\"models/clf_grid.sav\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test = pickle.load(open(\"models/clf_grid.sav\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67586206896551726"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_test.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
