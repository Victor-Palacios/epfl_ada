{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline\n",
    "\n",
    "Wednesday, November 22, 2017, 11:59PM\n",
    "\n",
    "## Important notes\n",
    "\n",
    "- When you push your Notebook to GitHub, all the cells must already have been evaluated.\n",
    "- Don't forget to add a textual description of your thought process and of any assumptions you've made.\n",
    "- Please write all your comments in English, and use meaningful variable names in your code.\n",
    "\n",
    "## Question 1: Propensity score matching\n",
    "\n",
    "In this exercise, you will apply [propensity score matching](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf), which we discussed in lecture 5 (\"Observational studies\"), in order to draw conclusions from an observational study.\n",
    "\n",
    "We will work with a by-now classic dataset from Robert LaLonde's study \"[Evaluating the Econometric Evaluations of Training Programs](http://people.hbs.edu/nashraf/LaLonde_1986.pdf)\" (1986).\n",
    "The study investigated the effect of a job training program (\"National Supported Work Demonstration\") on the real earnings of an individual, a couple of years after completion of the program.\n",
    "Your task is to determine the effectiveness of the \"treatment\" represented by the job training program.\n",
    "\n",
    "#### Dataset description\n",
    "\n",
    "- `treat`: 1 if the subject participated in the job training program, 0 otherwise\n",
    "- `age`: the subject's age\n",
    "- `educ`: years of education\n",
    "- `race`: categorical variable with three possible values: Black, Hispanic, or White\n",
    "- `married`: 1 if the subject was married at the time of the training program, 0 otherwise\n",
    "- `nodegree`: 1 if the subject has earned no school degree, 0 otherwise\n",
    "- `re74`: real earnings in 1974 (pre-treatment)\n",
    "- `re75`: real earnings in 1975 (pre-treatment)\n",
    "- `re78`: real earnings in 1978 (outcome)\n",
    "\n",
    "If you want to brush up your knowledge on propensity scores and observational studies, we highly recommend Rosenbaum's excellent book on the [\"Design of Observational Studies\"](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf). Even just reading the first chapter (18 pages) will help you a lot.\n",
    "\n",
    "#### 1. A naive analysis\n",
    "\n",
    "Compare the distribution of the outcome variable (`re78`) between the two groups, using plots and numbers.\n",
    "To summarize and compare the distributions, you may use the techniques we discussed in lectures 4 (\"Read the stats carefully\") and 6 (\"Data visualization\").\n",
    "\n",
    "What might a naive \"researcher\" conclude from this superficial analysis?\n",
    "\n",
    "#### 2. A closer look at the data\n",
    "\n",
    "You're not naive, of course (and even if you are, you've learned certain things in ADA), so you aren't content with a superficial analysis such as the above.\n",
    "You're aware of the dangers of observational studies, so you take a closer look at the data before jumping to conclusions.\n",
    "\n",
    "For each feature in the dataset, compare its distribution in the treated group with its distribution in the control group, using plots and numbers.\n",
    "As above, you may use the techniques we discussed in class for summarizing and comparing the distributions.\n",
    "\n",
    "What do you observe?\n",
    "Describe what your observations mean for the conclusions drawn by the naive \"researcher\" from his superficial analysis.\n",
    "\n",
    "#### 3. A propsensity score model\n",
    "\n",
    "Use logistic regression to estimate propensity scores for all points in the dataset.\n",
    "You may use `sklearn` to fit the logistic regression model and apply it to each data point to obtain propensity scores:\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "```\n",
    "\n",
    "Recall that the propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features (in this case, age, education, pre-treatment income, etc.).\n",
    "To brush up on propensity scores, you may read chapter 3.3 of the above-cited book by Rosenbaum or [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).\n",
    "\n",
    "Note: you do not need a train/test split here. Train and apply the model on the entire dataset. If you're wondering why this is the right thing to do in this situation, recall that the propensity score model is not used in order to make predictions about unseen data. Its sole purpose is to balance the dataset across treatment groups.\n",
    "(See p. 74 of Rosenbaum's book for an explanation why slight overfitting is even good for propensity scores.\n",
    "If you want even more information, read [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).)\n",
    "\n",
    "#### 4. Balancing the dataset via matching\n",
    "\n",
    "Use the propensity scores to match each data point from the treated group with exactly one data point from the control group, while ensuring that each data point from the control group is matched with at most one data point from the treated group.\n",
    "(Hint: you may explore the `networkx` package in Python for predefined matching functions.)\n",
    "\n",
    "Your matching should maximize the similarity between matched subjects, as captured by their propensity scores.\n",
    "In other words, the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects should be minimized.\n",
    "\n",
    "After matching, you have as many treated as you have control subjects.\n",
    "Compare the outcomes (`re78`) between the two groups (treated and control).\n",
    "\n",
    "Also, compare again the feature-value distributions between the two groups, as you've done in part 2 above, but now only for the matched subjects.\n",
    "What do you observe?\n",
    "Are you closer to being able to draw valid conclusions now than you were before?\n",
    "\n",
    "\n",
    "#### 5. Balancing the groups further\n",
    "\n",
    "Based on your comparison of feature-value distributions from part 4, are you fully satisfied with your matching?\n",
    "Would you say your dataset is sufficiently balanced?\n",
    "If not, in what ways could the \"balanced\" dataset you have obtained still not allow you to draw valid conclusions?\n",
    "\n",
    "Improve your matching by explicitly making sure that you match only subjects that have the same value for the problematic feature.\n",
    "Argue with numbers and plots that the two groups (treated and control) are now better balanced than after part 4.\n",
    "\n",
    "\n",
    "#### 6. A less naive analysis\n",
    "\n",
    "Compare the outcomes (`re78`) between treated and control subjects, as you've done in part 1, but now only for the matched dataset you've obtained from part 5.\n",
    "What do you conclude about the effectiveness of the job training program?\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## Question 2: Applied ML\n",
    "\n",
    "We are going to build a classifier of news to directly assign them to 20 news categories. Note that the pipeline that you will build in this exercise could be of great help during your project if you plan to work with text!\n",
    "\n",
    "1. Load the 20newsgroup dataset. It is, again, a classic dataset that can directly be loaded using sklearn ([link](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)).  \n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequencyâ€“inverse document frequency, is of great help when if comes to compute textual features. Indeed, it gives more importance to terms that are more specific to the considered articles (TF) but reduces the importance of terms that are very frequent in the entire corpus (IDF). Compute TF-IDF features for every article using [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Then, split your dataset into a training, a testing and a validation set (10% for validation and 10% for testing). Each observation should be paired with its corresponding label (the article category).\n",
    "\n",
    "\n",
    "2. Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NSW1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9930.0460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSW2</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3595.8940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NSW3</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24909.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NSW4</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7506.1460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NSW5</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.7899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  treat  age  educ  black  hispan  married  nodegree  re74  re75  \\\n",
       "0  NSW1      1   37    11      1       0        1         1   0.0   0.0   \n",
       "1  NSW2      1   22     9      0       1        0         1   0.0   0.0   \n",
       "2  NSW3      1   30    12      1       0        0         0   0.0   0.0   \n",
       "3  NSW4      1   27    11      1       0        0         1   0.0   0.0   \n",
       "4  NSW5      1   33     8      1       0        0         1   0.0   0.0   \n",
       "\n",
       "         re78  \n",
       "0   9930.0460  \n",
       "1   3595.8940  \n",
       "2  24909.4500  \n",
       "3   7506.1460  \n",
       "4    289.7899  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde = pd.read_csv(r'lalonde.csv')\n",
    "data_lalonde.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's do a little describe to see what we have here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>614.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.301303</td>\n",
       "      <td>27.363192</td>\n",
       "      <td>10.268730</td>\n",
       "      <td>0.395765</td>\n",
       "      <td>0.117264</td>\n",
       "      <td>0.415309</td>\n",
       "      <td>0.630293</td>\n",
       "      <td>4557.546569</td>\n",
       "      <td>2184.938207</td>\n",
       "      <td>6792.834483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.459198</td>\n",
       "      <td>9.881187</td>\n",
       "      <td>2.628325</td>\n",
       "      <td>0.489413</td>\n",
       "      <td>0.321997</td>\n",
       "      <td>0.493177</td>\n",
       "      <td>0.483119</td>\n",
       "      <td>6477.964479</td>\n",
       "      <td>3295.679043</td>\n",
       "      <td>7470.730792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>238.283425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1042.330000</td>\n",
       "      <td>601.548400</td>\n",
       "      <td>4759.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7888.498250</td>\n",
       "      <td>3248.987500</td>\n",
       "      <td>10893.592500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35040.070000</td>\n",
       "      <td>25142.240000</td>\n",
       "      <td>60307.930000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            treat         age        educ       black      hispan     married  \\\n",
       "count  614.000000  614.000000  614.000000  614.000000  614.000000  614.000000   \n",
       "mean     0.301303   27.363192   10.268730    0.395765    0.117264    0.415309   \n",
       "std      0.459198    9.881187    2.628325    0.489413    0.321997    0.493177   \n",
       "min      0.000000   16.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000   20.000000    9.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000   25.000000   11.000000    0.000000    0.000000    0.000000   \n",
       "75%      1.000000   32.000000   12.000000    1.000000    0.000000    1.000000   \n",
       "max      1.000000   55.000000   18.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree          re74          re75          re78  \n",
       "count  614.000000    614.000000    614.000000    614.000000  \n",
       "mean     0.630293   4557.546569   2184.938207   6792.834483  \n",
       "std      0.483119   6477.964479   3295.679043   7470.730792  \n",
       "min      0.000000      0.000000      0.000000      0.000000  \n",
       "25%      0.000000      0.000000      0.000000    238.283425  \n",
       "50%      1.000000   1042.330000    601.548400   4759.018500  \n",
       "75%      1.000000   7888.498250   3248.987500  10893.592500  \n",
       "max      1.000000  35040.070000  25142.240000  60307.930000  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing really special to say here. These value do not mean much by themselves. They just show some basic information. Let's answer question 1 now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_treat = data_lalonde[\"treat\"] == 1\n",
    "index_no_treat = data_lalonde[\"treat\"] == 0\n",
    "data_lalonde_treat = data_lalonde[index_treat]\n",
    "data_lalonde_no_treat = data_lalonde[index_no_treat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just checking of there are no \"outliers\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde_treat.shape[0] + data_lalonde_no_treat.shape[0] == data_lalonde.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed Kulovic\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['logistic']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x243345216a0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABB8AAAIMCAYAAAC9qgSnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsWjfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24334519b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (18, 9)\n",
    "plt.figure()\n",
    "\n",
    "data_lalonde_treat.re78.plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x243355bcc50>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCYAAAIMCAYAAAApTK1VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nESYAAACAMf8f4SetFIBIOYsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x243355cb630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lalonde_no_treat.re78.plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe not informative enough. We see that there are a few \"outliers\" in the treat case but generally speaking there are still more people on the left. Let's try to put everything on the same histogram so as to make the information clearer with respect to the scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBgAAAIMCAYAAABfdh/eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24335948e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 65000, 66)\n",
    "\n",
    "plt.hist(data_lalonde_treat.re78.tolist(), bins, alpha=0.5, label=\"treat\")\n",
    "plt.hist(data_lalonde_no_treat.re78.tolist(), bins, alpha=0.5, label=\"no treat\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't tell us much about the data to be honest. We clearly see that there are some outliers who make a lot of money on the far right but they are a minority so we cannot say something about that. Let's describe the two dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>185.0</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.816216</td>\n",
       "      <td>10.345946</td>\n",
       "      <td>0.843243</td>\n",
       "      <td>0.059459</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.708108</td>\n",
       "      <td>2095.573689</td>\n",
       "      <td>1532.055314</td>\n",
       "      <td>6349.143530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.155019</td>\n",
       "      <td>2.010650</td>\n",
       "      <td>0.364558</td>\n",
       "      <td>0.237124</td>\n",
       "      <td>0.392722</td>\n",
       "      <td>0.455867</td>\n",
       "      <td>4886.620353</td>\n",
       "      <td>3219.250870</td>\n",
       "      <td>7867.402218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>485.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4232.309000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1291.468000</td>\n",
       "      <td>1817.284000</td>\n",
       "      <td>9642.999000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35040.070000</td>\n",
       "      <td>25142.240000</td>\n",
       "      <td>60307.930000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       treat         age        educ       black      hispan     married  \\\n",
       "count  185.0  185.000000  185.000000  185.000000  185.000000  185.000000   \n",
       "mean     1.0   25.816216   10.345946    0.843243    0.059459    0.189189   \n",
       "std      0.0    7.155019    2.010650    0.364558    0.237124    0.392722   \n",
       "min      1.0   17.000000    4.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.0   20.000000    9.000000    1.000000    0.000000    0.000000   \n",
       "50%      1.0   25.000000   11.000000    1.000000    0.000000    0.000000   \n",
       "75%      1.0   29.000000   12.000000    1.000000    0.000000    0.000000   \n",
       "max      1.0   48.000000   16.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree          re74          re75          re78  \n",
       "count  185.000000    185.000000    185.000000    185.000000  \n",
       "mean     0.708108   2095.573689   1532.055314   6349.143530  \n",
       "std      0.455867   4886.620353   3219.250870   7867.402218  \n",
       "min      0.000000      0.000000      0.000000      0.000000  \n",
       "25%      0.000000      0.000000      0.000000    485.229800  \n",
       "50%      1.000000      0.000000      0.000000   4232.309000  \n",
       "75%      1.000000   1291.468000   1817.284000   9642.999000  \n",
       "max      1.000000  35040.070000  25142.240000  60307.930000  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde_treat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treat</th>\n",
       "      <th>age</th>\n",
       "      <th>educ</th>\n",
       "      <th>black</th>\n",
       "      <th>hispan</th>\n",
       "      <th>married</th>\n",
       "      <th>nodegree</th>\n",
       "      <th>re74</th>\n",
       "      <th>re75</th>\n",
       "      <th>re78</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.0</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28.030303</td>\n",
       "      <td>10.235431</td>\n",
       "      <td>0.202797</td>\n",
       "      <td>0.142191</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.596737</td>\n",
       "      <td>5619.236506</td>\n",
       "      <td>2466.484443</td>\n",
       "      <td>6984.169742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.786653</td>\n",
       "      <td>2.855238</td>\n",
       "      <td>0.402552</td>\n",
       "      <td>0.349654</td>\n",
       "      <td>0.500419</td>\n",
       "      <td>0.491126</td>\n",
       "      <td>6788.750796</td>\n",
       "      <td>3291.996183</td>\n",
       "      <td>7294.161791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>220.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2547.047000</td>\n",
       "      <td>1086.726000</td>\n",
       "      <td>4975.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9277.128000</td>\n",
       "      <td>3881.419000</td>\n",
       "      <td>11688.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25862.320000</td>\n",
       "      <td>18347.230000</td>\n",
       "      <td>25564.670000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       treat         age        educ       black      hispan     married  \\\n",
       "count  429.0  429.000000  429.000000  429.000000  429.000000  429.000000   \n",
       "mean     0.0   28.030303   10.235431    0.202797    0.142191    0.512821   \n",
       "std      0.0   10.786653    2.855238    0.402552    0.349654    0.500419   \n",
       "min      0.0   16.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.0   19.000000    9.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.0   25.000000   11.000000    0.000000    0.000000    1.000000   \n",
       "75%      0.0   35.000000   12.000000    0.000000    0.000000    1.000000   \n",
       "max      0.0   55.000000   18.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         nodegree          re74          re75          re78  \n",
       "count  429.000000    429.000000    429.000000    429.000000  \n",
       "mean     0.596737   5619.236506   2466.484443   6984.169742  \n",
       "std      0.491126   6788.750796   3291.996183   7294.161791  \n",
       "min      0.000000      0.000000      0.000000      0.000000  \n",
       "25%      0.000000      0.000000      0.000000    220.181300  \n",
       "50%      1.000000   2547.047000   1086.726000   4975.505000  \n",
       "75%      1.000000   9277.128000   3881.419000  11688.820000  \n",
       "max      1.000000  25862.320000  18347.230000  25564.670000  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lalonde_no_treat.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that the value has nothing special to do with treat or no treat. Somehow we see that the selection was far from uniform at random. In the treat group there are more people with no degree than with degree. The \"black\" variable is interesting also. We intuitively make associate the fact that there are a lot more black people to more difficult social conditions. We can also note that there is a three years difference in mean age between the two groups so, in a sense, we guess that people in the no treat group have more experience on average and this should imply the higher mean salary in the no treat group. We can also say that the age parameter has a lot of impact on the nodegree and the married parameters. Somehow, being 3 years older implies a higher probability of being married and a higher probability of having a degree.\n",
    "\n",
    "Now we want to measure the propensity scores using logistic regression in order to be able to improve our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# We won't include re78 since this is the salary after the subject is treated\n",
    "features = [\"age\", \"educ\", \"black\", \"hispan\", \"married\", \"nodegree\", \"re74\", \"re75\"]\n",
    "X = data_lalonde[features]\n",
    "y = data_lalonde.treat\n",
    "\n",
    "logistic.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80781758957654726"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for a start. We are really far from 0.5 which could mean that we are really far from a randomized experiment. Bad this score is not really informative, what we want here is the propensity score measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "propensity = [b for [a,b] in logistic.predict_proba(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our propensity scores for all \"nodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4433504229538332,\n",
       " 0.14465953271343432,\n",
       " 0.72235463272580003,\n",
       " 0.66415051683598725,\n",
       " 0.69828561078482354]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensity[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want now is to do matching : for example, if we take the first person in our dataset (the one with propensity score 0.443...), we would like to match them with another person whose propensity score is very close to 0.44 but which is in the opposite group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to separate the dataset into the treat and no treat groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_propensity = data_lalonde.copy()\n",
    "data_propensity[\"propensity\"] = propensity\n",
    "\n",
    "data_treat_propensity = data_propensity[data_propensity.treat == 1]\n",
    "data_no_treat_propensity = data_propensity[data_propensity.treat == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go and create a bipartite graph. So our first strategy (we will find another one if this one doesn't work) is that the weight of the edge between the two nodes is the absolute difference in propensity scores between them. And then we will use networkx to find a min-weight matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 20)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-621384be7ea9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow_j\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_no_treat_propensity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Since the method we use is based on max matching we use negative value to get min matching\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow_i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"propensity\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrow_j\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"propensity\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\networkx\\classes\\graph.py\u001b[0m in \u001b[0;36madd_edge\u001b[1;34m(self, u, v, **attr)\u001b[0m\n\u001b[0;32m    870\u001b[0m         \"\"\"\n\u001b[0;32m    871\u001b[0m         \u001b[1;31m# add nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mu\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madjlist_inner_dict_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "G.add_nodes_from(range(data_treat_propensity.shape[0]), bipartite=0)\n",
    "G.add_nodes_from(range(data_no_treat_propensity.shape[0]), bipartite=1)\n",
    "\n",
    "for i, row_i in data_treat_propensity.iterrows():\n",
    "    for j, row_j in data_no_treat_propensity.iterrows():\n",
    "        # Since the method we use is based on max matching we use negative value to get min matching\n",
    "        G.add_edge(i, j, weight=-abs(row_i[\"propensity\"] - row_j[\"propensity\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compute the matching (it takes around a minute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matching_propensity_max_card = nx.max_weight_matching(G, maxcardinality=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the min weight matching without max cardinality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_propensity = nx.max_weight_matching(G)\n",
    "matching_propensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the problem, as we can see, is that we end up with too few datapoints in the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def observe_matching(matching, verbose):\n",
    "    sum_differences = 0\n",
    "    for (a, b) in matching.items():\n",
    "        if a < data_treat_propensity.shape[0] / 2:\n",
    "            propensity_score_a = data_propensity.loc[a][\"propensity\"]\n",
    "            propensity_score_b = data_propensity.loc[b][\"propensity\"]\n",
    "            diff = abs(propensity_score_a - propensity_score_b)\n",
    "            if verbose:\n",
    "                print(\"Difference between {} and {} is {}\".format(a, b, diff))\n",
    "            sum_differences += diff\n",
    "    print(\"Mean difference in prop score is {}\".format(sum_differences / len(matching) / 2))\n",
    "    \n",
    "observe_matching(matching_propensity, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the problem with using min matching without max cardinality. The algorithm will compute the \"real\" min matching and it will only match the points with the same propensity scores for that. We will continue with the matchings we obtained from max cardinality matchings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observe_matching(matching_propensity_max_card, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matching takes into account all datapoints from the treat (the smallest) group. So it has 185 matchings. In general those matchings are pretty good, very similar points are matched with each other. But if we inspect the values we see that there are some \"outliers\" matchings since all points have to be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propensity_differences_matchings = list(filter(lambda x : x[0] < 185, \n",
    "                                               map(lambda matching: (matching[0], abs(data_propensity.loc[matching[0]][\"propensity\"] - data_propensity.loc[matching[1]][\"propensity\"])), \n",
    "                                                   matching_propensity_max_card.items())))\n",
    "propensity_differences_matchings = [prop for (index, prop) in propensity_differences_matchings]\n",
    "plt.hist(propensity_differences_matchings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this histogram we see that there are some bothering matchings where the differences in propensity scores are more than 0.5. A third alternative may be to apply a threshold after the matching to get rid of those values but let's already see how far we can go with our 185-matchings set\n",
    "\n",
    "Now let's look at the re78 values for both groups but we will take into account only the matched datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_treat = [a for (a, b) in matching_propensity_max_card.items() if a < data_treat_propensity.shape[0]]\n",
    "group_no_treat = [b for (a, b) in matching_propensity_max_card.items() if a < data_treat_propensity.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 65000, 66)\n",
    "plt.hist(data_lalonde.loc[group_treat].re78.tolist(), bins, alpha=0.5, label=\"treat\")\n",
    "plt.hist(data_lalonde.loc[group_no_treat].re78.tolist(), bins, alpha=0.5, label=\"no treat\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is our distribution. Maybe we can argue we see things in a better way on this histogram. The numer of samples is exactly the same for both groups (the number of samples in the treat group) and the two groups were obtained using the min matching method on the bipartite graph. Here we clearly see that the \"extreme\" outliers belong to the treat group. But however, there are not so many of them. We can also see that the first bin (the least salary) contains more people from the non treated than the treated group (however, if we look at the re78 column in out dataset we see that there are a lot of 0 values so either the dataset has errors or those people are simply unemployed). So the only thing we might try to guess from this is that the treat works well on a few people: the blue outliers. (but maybe those people anyway had more determination and this determination is what caused them to take the treat so the variable is something that we do not have in our dataset: the determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tweak things to see if we can get better insights. The first thing we want to try is to simply remove the matchings where the difference is too big. We will end up with much less datapoints but we might get something out of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_matching(matching, threshold=0.4):\n",
    "    def difference_matching(x, y):\n",
    "        return abs(data_propensity.loc[x][\"propensity\"] - data_propensity.loc[y][\"propensity\"])\n",
    "\n",
    "    matching_propensity_max_card_pairs = filter(lambda x : x[0] < 185, matching_propensity_max_card.items())\n",
    "    matching_propensity_max_card_pairs = map(lambda x : (x[0], x[1], difference_matching(x[0], x[1])), matching_propensity_max_card_pairs)\n",
    "    matching_propensity_max_card_pairs = filter(lambda x : x[2] <= threshold, matching_propensity_max_card_pairs)\n",
    "    return list(matching_propensity_max_card_pairs)\n",
    "\n",
    "def plot_matching_hist(matching, threshold=0.4):\n",
    "    bins = np.linspace(0, 65000, 66)\n",
    "    matchings = filter_matching(matching, threshold)\n",
    "    group_treat = [a for (a, b, prop) in matchings]\n",
    "    group_no_treat = [b for (a, b, prop) in matchings]\n",
    "    plt.hist(data_lalonde.loc[group_treat].re78.tolist(), bins, alpha=0.5, label=\"treat\")\n",
    "    plt.hist(data_lalonde.loc[group_no_treat].re78.tolist(), bins, alpha=0.5, label=\"no treat\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we get with the threshold at 0.4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And at 0.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0.1 (very similar propensity scores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0.05 (very very similar propensity scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 0 (Exactly the same propensity scores) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matching_hist(matching_propensity_max_card, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general thing we always see no matter the threshold is that the treat group always has those outliers who have really big salaries (except in the threshold 0 case where they do not appear)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we might try to get rid of some columns to see if we can get better insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Applied ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroup_train = fetch_20newsgroups(subset=\"train\")\n",
    "pprint(list(newsgroup_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroup_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "newsgroup_test = fetch_20newsgroups(subset=\"test\")\n",
    "vectors_test = vectorizer.transform(newsgroup_test.data)\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectors, newsgroup_train.target)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroup_test.target, pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup = fetch_20newsgroups(subset=\"all\", remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 134410)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroup.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroup.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectors\n",
    "y = newsgroup.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_fit, X_test, y_fit, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Here we put 0.11111111 because 0.1111111... * 0.9 = 0.1 which will give 80% / 10% / 10%\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_fit, y_fit, test_size=0.111111112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a little preview depending on the number of estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As if the score increases depending on the number of estimators. Let's perform a grid search on the number of trees to see what is happening with more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-8f1309842009>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 328\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ada\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=200)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_total = X_fit.shape[0]\n",
    "n_val = int(fit_total * 0.1111111)\n",
    "n_train = fit_total - n_val\n",
    "ps_array = [0] * n_train + [1] * n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 100 candidates, totalling 200 fits\n",
      "[CV] n_estimators=1, max_depth=1 .....................................\n",
      "[CV] ...................... n_estimators=1, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=1, max_depth=1 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... n_estimators=1, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=1 .....................................\n",
      "[CV] ...................... n_estimators=2, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=1 .....................................\n",
      "[CV] ...................... n_estimators=2, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=1 .....................................\n",
      "[CV] ...................... n_estimators=4, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=1 .....................................\n",
      "[CV] ...................... n_estimators=4, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=10, max_depth=1 ....................................\n",
      "[CV] ..................... n_estimators=10, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=10, max_depth=1 ....................................\n",
      "[CV] ..................... n_estimators=10, max_depth=1, total=   0.0s\n",
      "[CV] n_estimators=21, max_depth=1 ....................................\n",
      "[CV] ..................... n_estimators=21, max_depth=1, total=   0.1s\n",
      "[CV] n_estimators=21, max_depth=1 ....................................\n",
      "[CV] ..................... n_estimators=21, max_depth=1, total=   0.1s\n",
      "[CV] n_estimators=46, max_depth=1 ....................................\n",
      "[CV] ..................... n_estimators=46, max_depth=1, total=   0.3s\n",
      "[CV] n_estimators=46, max_depth=1 ....................................\n",
      "[CV] ..................... n_estimators=46, max_depth=1, total=   0.2s\n",
      "[CV] n_estimators=100, max_depth=1 ...................................\n",
      "[CV] .................... n_estimators=100, max_depth=1, total=   0.7s\n",
      "[CV] n_estimators=100, max_depth=1 ...................................\n",
      "[CV] .................... n_estimators=100, max_depth=1, total=   0.5s\n",
      "[CV] n_estimators=215, max_depth=1 ...................................\n",
      "[CV] .................... n_estimators=215, max_depth=1, total=   1.5s\n",
      "[CV] n_estimators=215, max_depth=1 ...................................\n",
      "[CV] .................... n_estimators=215, max_depth=1, total=   1.0s\n",
      "[CV] n_estimators=464, max_depth=1 ...................................\n",
      "[CV] .................... n_estimators=464, max_depth=1, total=   3.3s\n",
      "[CV] n_estimators=464, max_depth=1 ...................................\n",
      "[CV] .................... n_estimators=464, max_depth=1, total=   2.3s\n",
      "[CV] n_estimators=1000, max_depth=1 ..................................\n",
      "[CV] ................... n_estimators=1000, max_depth=1, total=   7.1s\n",
      "[CV] n_estimators=1000, max_depth=1 ..................................\n",
      "[CV] ................... n_estimators=1000, max_depth=1, total=   5.0s\n",
      "[CV] n_estimators=1, max_depth=2 .....................................\n",
      "[CV] ...................... n_estimators=1, max_depth=2, total=   0.0s\n",
      "[CV] n_estimators=1, max_depth=2 .....................................\n",
      "[CV] ...................... n_estimators=1, max_depth=2, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=2 .....................................\n",
      "[CV] ...................... n_estimators=2, max_depth=2, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=2 .....................................\n",
      "[CV] ...................... n_estimators=2, max_depth=2, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=2 .....................................\n",
      "[CV] ...................... n_estimators=4, max_depth=2, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=2 .....................................\n",
      "[CV] ...................... n_estimators=4, max_depth=2, total=   0.0s\n",
      "[CV] n_estimators=10, max_depth=2 ....................................\n",
      "[CV] ..................... n_estimators=10, max_depth=2, total=   0.1s\n",
      "[CV] n_estimators=10, max_depth=2 ....................................\n",
      "[CV] ..................... n_estimators=10, max_depth=2, total=   0.0s\n",
      "[CV] n_estimators=21, max_depth=2 ....................................\n",
      "[CV] ..................... n_estimators=21, max_depth=2, total=   0.1s\n",
      "[CV] n_estimators=21, max_depth=2 ....................................\n",
      "[CV] ..................... n_estimators=21, max_depth=2, total=   0.1s\n",
      "[CV] n_estimators=46, max_depth=2 ....................................\n",
      "[CV] ..................... n_estimators=46, max_depth=2, total=   0.3s\n",
      "[CV] n_estimators=46, max_depth=2 ....................................\n",
      "[CV] ..................... n_estimators=46, max_depth=2, total=   0.3s\n",
      "[CV] n_estimators=100, max_depth=2 ...................................\n",
      "[CV] .................... n_estimators=100, max_depth=2, total=   0.7s\n",
      "[CV] n_estimators=100, max_depth=2 ...................................\n",
      "[CV] .................... n_estimators=100, max_depth=2, total=   0.7s\n",
      "[CV] n_estimators=215, max_depth=2 ...................................\n",
      "[CV] .................... n_estimators=215, max_depth=2, total=   1.6s\n",
      "[CV] n_estimators=215, max_depth=2 ...................................\n",
      "[CV] .................... n_estimators=215, max_depth=2, total=   1.6s\n",
      "[CV] n_estimators=464, max_depth=2 ...................................\n",
      "[CV] .................... n_estimators=464, max_depth=2, total=   3.4s\n",
      "[CV] n_estimators=464, max_depth=2 ...................................\n",
      "[CV] .................... n_estimators=464, max_depth=2, total=   3.6s\n",
      "[CV] n_estimators=1000, max_depth=2 ..................................\n",
      "[CV] ................... n_estimators=1000, max_depth=2, total=   7.4s\n",
      "[CV] n_estimators=1000, max_depth=2 ..................................\n",
      "[CV] ................... n_estimators=1000, max_depth=2, total=   7.7s\n",
      "[CV] n_estimators=1, max_depth=4 .....................................\n",
      "[CV] ...................... n_estimators=1, max_depth=4, total=   0.0s\n",
      "[CV] n_estimators=1, max_depth=4 .....................................\n",
      "[CV] ...................... n_estimators=1, max_depth=4, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=4 .....................................\n",
      "[CV] ...................... n_estimators=2, max_depth=4, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=4 .....................................\n",
      "[CV] ...................... n_estimators=2, max_depth=4, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=4 .....................................\n",
      "[CV] ...................... n_estimators=4, max_depth=4, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=4 .....................................\n",
      "[CV] ...................... n_estimators=4, max_depth=4, total=   0.0s\n",
      "[CV] n_estimators=10, max_depth=4 ....................................\n",
      "[CV] ..................... n_estimators=10, max_depth=4, total=   0.1s\n",
      "[CV] n_estimators=10, max_depth=4 ....................................\n",
      "[CV] ..................... n_estimators=10, max_depth=4, total=   0.1s\n",
      "[CV] n_estimators=21, max_depth=4 ....................................\n",
      "[CV] ..................... n_estimators=21, max_depth=4, total=   0.1s\n",
      "[CV] n_estimators=21, max_depth=4 ....................................\n",
      "[CV] ..................... n_estimators=21, max_depth=4, total=   0.2s\n",
      "[CV] n_estimators=46, max_depth=4 ....................................\n",
      "[CV] ..................... n_estimators=46, max_depth=4, total=   0.4s\n",
      "[CV] n_estimators=46, max_depth=4 ....................................\n",
      "[CV] ..................... n_estimators=46, max_depth=4, total=   0.6s\n",
      "[CV] n_estimators=100, max_depth=4 ...................................\n",
      "[CV] .................... n_estimators=100, max_depth=4, total=   0.8s\n",
      "[CV] n_estimators=100, max_depth=4 ...................................\n",
      "[CV] .................... n_estimators=100, max_depth=4, total=   1.3s\n",
      "[CV] n_estimators=215, max_depth=4 ...................................\n",
      "[CV] .................... n_estimators=215, max_depth=4, total=   1.7s\n",
      "[CV] n_estimators=215, max_depth=4 ...................................\n",
      "[CV] .................... n_estimators=215, max_depth=4, total=   2.8s\n",
      "[CV] n_estimators=464, max_depth=4 ...................................\n",
      "[CV] .................... n_estimators=464, max_depth=4, total=   3.8s\n",
      "[CV] n_estimators=464, max_depth=4 ...................................\n",
      "[CV] .................... n_estimators=464, max_depth=4, total=   6.2s\n",
      "[CV] n_estimators=1000, max_depth=4 ..................................\n",
      "[CV] ................... n_estimators=1000, max_depth=4, total=   8.1s\n",
      "[CV] n_estimators=1000, max_depth=4 ..................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... n_estimators=1000, max_depth=4, total=  13.3s\n",
      "[CV] n_estimators=1, max_depth=10 ....................................\n",
      "[CV] ..................... n_estimators=1, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=1, max_depth=10 ....................................\n",
      "[CV] ..................... n_estimators=1, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=10 ....................................\n",
      "[CV] ..................... n_estimators=2, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=10 ....................................\n",
      "[CV] ..................... n_estimators=2, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=10 ....................................\n",
      "[CV] ..................... n_estimators=4, max_depth=10, total=   0.0s\n",
      "[CV] n_estimators=4, max_depth=10 ....................................\n",
      "[CV] ..................... n_estimators=4, max_depth=10, total=   0.1s\n",
      "[CV] n_estimators=10, max_depth=10 ...................................\n",
      "[CV] .................... n_estimators=10, max_depth=10, total=   0.1s\n",
      "[CV] n_estimators=10, max_depth=10 ...................................\n",
      "[CV] .................... n_estimators=10, max_depth=10, total=   0.3s\n",
      "[CV] n_estimators=21, max_depth=10 ...................................\n",
      "[CV] .................... n_estimators=21, max_depth=10, total=   0.2s\n",
      "[CV] n_estimators=21, max_depth=10 ...................................\n",
      "[CV] .................... n_estimators=21, max_depth=10, total=   0.7s\n",
      "[CV] n_estimators=46, max_depth=10 ...................................\n",
      "[CV] .................... n_estimators=46, max_depth=10, total=   0.5s\n",
      "[CV] n_estimators=46, max_depth=10 ...................................\n",
      "[CV] .................... n_estimators=46, max_depth=10, total=   1.6s\n",
      "[CV] n_estimators=100, max_depth=10 ..................................\n",
      "[CV] ................... n_estimators=100, max_depth=10, total=   1.2s\n",
      "[CV] n_estimators=100, max_depth=10 ..................................\n",
      "[CV] ................... n_estimators=100, max_depth=10, total=   3.4s\n",
      "[CV] n_estimators=215, max_depth=10 ..................................\n",
      "[CV] ................... n_estimators=215, max_depth=10, total=   2.7s\n",
      "[CV] n_estimators=215, max_depth=10 ..................................\n",
      "[CV] ................... n_estimators=215, max_depth=10, total=   7.5s\n",
      "[CV] n_estimators=464, max_depth=10 ..................................\n",
      "[CV] ................... n_estimators=464, max_depth=10, total=   5.6s\n",
      "[CV] n_estimators=464, max_depth=10 ..................................\n",
      "[CV] ................... n_estimators=464, max_depth=10, total=  16.1s\n",
      "[CV] n_estimators=1000, max_depth=10 .................................\n",
      "[CV] .................. n_estimators=1000, max_depth=10, total=  11.8s\n",
      "[CV] n_estimators=1000, max_depth=10 .................................\n",
      "[CV] .................. n_estimators=1000, max_depth=10, total=  34.4s\n",
      "[CV] n_estimators=1, max_depth=21 ....................................\n",
      "[CV] ..................... n_estimators=1, max_depth=21, total=   0.0s\n",
      "[CV] n_estimators=1, max_depth=21 ....................................\n",
      "[CV] ..................... n_estimators=1, max_depth=21, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=21 ....................................\n",
      "[CV] ..................... n_estimators=2, max_depth=21, total=   0.0s\n",
      "[CV] n_estimators=2, max_depth=21 ....................................\n",
      "[CV] ..................... n_estimators=2, max_depth=21, total=   0.1s\n",
      "[CV] n_estimators=4, max_depth=21 ....................................\n",
      "[CV] ..................... n_estimators=4, max_depth=21, total=   0.1s\n",
      "[CV] n_estimators=4, max_depth=21 ....................................\n",
      "[CV] ..................... n_estimators=4, max_depth=21, total=   0.3s\n",
      "[CV] n_estimators=10, max_depth=21 ...................................\n",
      "[CV] .................... n_estimators=10, max_depth=21, total=   0.2s\n",
      "[CV] n_estimators=10, max_depth=21 ...................................\n",
      "[CV] .................... n_estimators=10, max_depth=21, total=   0.9s\n",
      "[CV] n_estimators=21, max_depth=21 ...................................\n",
      "[CV] .................... n_estimators=21, max_depth=21, total=   0.4s\n",
      "[CV] n_estimators=21, max_depth=21 ...................................\n",
      "[CV] .................... n_estimators=21, max_depth=21, total=   1.8s\n",
      "[CV] n_estimators=46, max_depth=21 ...................................\n",
      "[CV] .................... n_estimators=46, max_depth=21, total=   1.1s\n",
      "[CV] n_estimators=46, max_depth=21 ...................................\n",
      "[CV] .................... n_estimators=46, max_depth=21, total=   4.3s\n",
      "[CV] n_estimators=100, max_depth=21 ..................................\n",
      "[CV] ................... n_estimators=100, max_depth=21, total=   2.2s\n",
      "[CV] n_estimators=100, max_depth=21 ..................................\n",
      "[CV] ................... n_estimators=100, max_depth=21, total=   8.8s\n",
      "[CV] n_estimators=215, max_depth=21 ..................................\n",
      "[CV] ................... n_estimators=215, max_depth=21, total=   4.9s\n",
      "[CV] n_estimators=215, max_depth=21 ..................................\n",
      "[CV] ................... n_estimators=215, max_depth=21, total=  19.2s\n",
      "[CV] n_estimators=464, max_depth=21 ..................................\n",
      "[CV] ................... n_estimators=464, max_depth=21, total=  10.7s\n",
      "[CV] n_estimators=464, max_depth=21 ..................................\n",
      "[CV] ................... n_estimators=464, max_depth=21, total=  41.4s\n",
      "[CV] n_estimators=1000, max_depth=21 .................................\n",
      "[CV] .................. n_estimators=1000, max_depth=21, total=  22.7s\n",
      "[CV] n_estimators=1000, max_depth=21 .................................\n",
      "[CV] .................. n_estimators=1000, max_depth=21, total= 1.5min\n",
      "[CV] n_estimators=1, max_depth=46 ....................................\n",
      "[CV] ..................... n_estimators=1, max_depth=46, total=   0.0s\n",
      "[CV] n_estimators=1, max_depth=46 ....................................\n",
      "[CV] ..................... n_estimators=1, max_depth=46, total=   0.2s\n",
      "[CV] n_estimators=2, max_depth=46 ....................................\n",
      "[CV] ..................... n_estimators=2, max_depth=46, total=   0.1s\n",
      "[CV] n_estimators=2, max_depth=46 ....................................\n",
      "[CV] ..................... n_estimators=2, max_depth=46, total=   0.6s\n",
      "[CV] n_estimators=4, max_depth=46 ....................................\n",
      "[CV] ..................... n_estimators=4, max_depth=46, total=   0.2s\n",
      "[CV] n_estimators=4, max_depth=46 ....................................\n",
      "[CV] ..................... n_estimators=4, max_depth=46, total=   1.0s\n",
      "[CV] n_estimators=10, max_depth=46 ...................................\n",
      "[CV] .................... n_estimators=10, max_depth=46, total=   0.5s\n",
      "[CV] n_estimators=10, max_depth=46 ...................................\n",
      "[CV] .................... n_estimators=10, max_depth=46, total=   2.8s\n",
      "[CV] n_estimators=21, max_depth=46 ...................................\n",
      "[CV] .................... n_estimators=21, max_depth=46, total=   1.1s\n",
      "[CV] n_estimators=21, max_depth=46 ...................................\n",
      "[CV] .................... n_estimators=21, max_depth=46, total=   5.4s\n",
      "[CV] n_estimators=46, max_depth=46 ...................................\n",
      "[CV] .................... n_estimators=46, max_depth=46, total=   2.3s\n",
      "[CV] n_estimators=46, max_depth=46 ...................................\n",
      "[CV] .................... n_estimators=46, max_depth=46, total=  12.8s\n",
      "[CV] n_estimators=100, max_depth=46 ..................................\n",
      "[CV] ................... n_estimators=100, max_depth=46, total=   5.2s\n",
      "[CV] n_estimators=100, max_depth=46 ..................................\n",
      "[CV] ................... n_estimators=100, max_depth=46, total=  26.5s\n",
      "[CV] n_estimators=215, max_depth=46 ..................................\n",
      "[CV] ................... n_estimators=215, max_depth=46, total=  11.2s\n",
      "[CV] n_estimators=215, max_depth=46 ..................................\n",
      "[CV] ................... n_estimators=215, max_depth=46, total=61.0min\n",
      "[CV] n_estimators=464, max_depth=46 ..................................\n",
      "[CV] ................... n_estimators=464, max_depth=46, total=  26.8s\n",
      "[CV] n_estimators=464, max_depth=46 ..................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... n_estimators=464, max_depth=46, total= 2.3min\n",
      "[CV] n_estimators=1000, max_depth=46 .................................\n",
      "[CV] .................. n_estimators=1000, max_depth=46, total=  53.1s\n",
      "[CV] n_estimators=1000, max_depth=46 .................................\n",
      "[CV] .................. n_estimators=1000, max_depth=46, total= 4.5min\n",
      "[CV] n_estimators=1, max_depth=100 ...................................\n",
      "[CV] .................... n_estimators=1, max_depth=100, total=   0.1s\n",
      "[CV] n_estimators=1, max_depth=100 ...................................\n",
      "[CV] .................... n_estimators=1, max_depth=100, total=   0.6s\n",
      "[CV] n_estimators=2, max_depth=100 ...................................\n",
      "[CV] .................... n_estimators=2, max_depth=100, total=   0.2s\n",
      "[CV] n_estimators=2, max_depth=100 ...................................\n",
      "[CV] .................... n_estimators=2, max_depth=100, total=   1.5s\n",
      "[CV] n_estimators=4, max_depth=100 ...................................\n",
      "[CV] .................... n_estimators=4, max_depth=100, total=   0.5s\n",
      "[CV] n_estimators=4, max_depth=100 ...................................\n",
      "[CV] .................... n_estimators=4, max_depth=100, total=   3.5s\n",
      "[CV] n_estimators=10, max_depth=100 ..................................\n",
      "[CV] ................... n_estimators=10, max_depth=100, total=   1.3s\n",
      "[CV] n_estimators=10, max_depth=100 ..................................\n",
      "[CV] ................... n_estimators=10, max_depth=100, total=   6.8s\n",
      "[CV] n_estimators=21, max_depth=100 ..................................\n",
      "[CV] ................... n_estimators=21, max_depth=100, total=   2.4s\n",
      "[CV] n_estimators=21, max_depth=100 ..................................\n",
      "[CV] ................... n_estimators=21, max_depth=100, total=  15.1s\n",
      "[CV] n_estimators=46, max_depth=100 ..................................\n",
      "[CV] ................... n_estimators=46, max_depth=100, total=   5.3s\n",
      "[CV] n_estimators=46, max_depth=100 ..................................\n",
      "[CV] ................... n_estimators=46, max_depth=100, total=  37.4s\n",
      "[CV] n_estimators=100, max_depth=100 .................................\n",
      "[CV] .................. n_estimators=100, max_depth=100, total=  11.8s\n",
      "[CV] n_estimators=100, max_depth=100 .................................\n",
      "[CV] .................. n_estimators=100, max_depth=100, total= 1.3min\n",
      "[CV] n_estimators=215, max_depth=100 .................................\n",
      "[CV] .................. n_estimators=215, max_depth=100, total=  24.5s\n",
      "[CV] n_estimators=215, max_depth=100 .................................\n",
      "[CV] .................. n_estimators=215, max_depth=100, total= 2.6min\n",
      "[CV] n_estimators=464, max_depth=100 .................................\n",
      "[CV] .................. n_estimators=464, max_depth=100, total=  51.5s\n",
      "[CV] n_estimators=464, max_depth=100 .................................\n",
      "[CV] .................. n_estimators=464, max_depth=100, total= 5.6min\n",
      "[CV] n_estimators=1000, max_depth=100 ................................\n",
      "[CV] ................. n_estimators=1000, max_depth=100, total= 1.8min\n",
      "[CV] n_estimators=1000, max_depth=100 ................................\n",
      "[CV] ................. n_estimators=1000, max_depth=100, total=10.9min\n",
      "[CV] n_estimators=1, max_depth=215 ...................................\n",
      "[CV] .................... n_estimators=1, max_depth=215, total=   0.1s\n",
      "[CV] n_estimators=1, max_depth=215 ...................................\n",
      "[CV] .................... n_estimators=1, max_depth=215, total=   1.2s\n",
      "[CV] n_estimators=2, max_depth=215 ...................................\n",
      "[CV] .................... n_estimators=2, max_depth=215, total=   0.3s\n",
      "[CV] n_estimators=2, max_depth=215 ...................................\n",
      "[CV] .................... n_estimators=2, max_depth=215, total=   2.0s\n",
      "[CV] n_estimators=4, max_depth=215 ...................................\n",
      "[CV] .................... n_estimators=4, max_depth=215, total=   0.6s\n",
      "[CV] n_estimators=4, max_depth=215 ...................................\n",
      "[CV] .................... n_estimators=4, max_depth=215, total=   4.3s\n",
      "[CV] n_estimators=10, max_depth=215 ..................................\n",
      "[CV] ................... n_estimators=10, max_depth=215, total=   1.4s\n",
      "[CV] n_estimators=10, max_depth=215 ..................................\n",
      "[CV] ................... n_estimators=10, max_depth=215, total=  11.0s\n",
      "[CV] n_estimators=21, max_depth=215 ..................................\n",
      "[CV] ................... n_estimators=21, max_depth=215, total=   3.1s\n",
      "[CV] n_estimators=21, max_depth=215 ..................................\n",
      "[CV] ................... n_estimators=21, max_depth=215, total=  23.1s\n",
      "[CV] n_estimators=46, max_depth=215 ..................................\n",
      "[CV] ................... n_estimators=46, max_depth=215, total=   6.9s\n",
      "[CV] n_estimators=46, max_depth=215 ..................................\n",
      "[CV] ................... n_estimators=46, max_depth=215, total=  49.9s\n",
      "[CV] n_estimators=100, max_depth=215 .................................\n",
      "[CV] .................. n_estimators=100, max_depth=215, total=  28.4s\n",
      "[CV] n_estimators=100, max_depth=215 .................................\n",
      "[CV] .................. n_estimators=100, max_depth=215, total= 1.8min\n",
      "[CV] n_estimators=215, max_depth=215 .................................\n",
      "[CV] .................. n_estimators=215, max_depth=215, total=  32.2s\n",
      "[CV] n_estimators=215, max_depth=215 .................................\n",
      "[CV] .................. n_estimators=215, max_depth=215, total= 3.9min\n",
      "[CV] n_estimators=464, max_depth=215 .................................\n",
      "[CV] .................. n_estimators=464, max_depth=215, total= 1.2min\n",
      "[CV] n_estimators=464, max_depth=215 .................................\n",
      "[CV] .................. n_estimators=464, max_depth=215, total= 9.7min\n",
      "[CV] n_estimators=1000, max_depth=215 ................................\n",
      "[CV] ................. n_estimators=1000, max_depth=215, total= 2.8min\n",
      "[CV] n_estimators=1000, max_depth=215 ................................\n",
      "[CV] ................. n_estimators=1000, max_depth=215, total=20.9min\n",
      "[CV] n_estimators=1, max_depth=464 ...................................\n",
      "[CV] .................... n_estimators=1, max_depth=464, total=   0.2s\n",
      "[CV] n_estimators=1, max_depth=464 ...................................\n",
      "[CV] .................... n_estimators=1, max_depth=464, total=   1.4s\n",
      "[CV] n_estimators=2, max_depth=464 ...................................\n",
      "[CV] .................... n_estimators=2, max_depth=464, total=   0.3s\n",
      "[CV] n_estimators=2, max_depth=464 ...................................\n",
      "[CV] .................... n_estimators=2, max_depth=464, total=   2.7s\n",
      "[CV] n_estimators=4, max_depth=464 ...................................\n",
      "[CV] .................... n_estimators=4, max_depth=464, total=   0.7s\n",
      "[CV] n_estimators=4, max_depth=464 ...................................\n",
      "[CV] .................... n_estimators=4, max_depth=464, total=   6.0s\n",
      "[CV] n_estimators=10, max_depth=464 ..................................\n",
      "[CV] ................... n_estimators=10, max_depth=464, total=   1.8s\n",
      "[CV] n_estimators=10, max_depth=464 ..................................\n",
      "[CV] ................... n_estimators=10, max_depth=464, total=  15.0s\n",
      "[CV] n_estimators=21, max_depth=464 ..................................\n",
      "[CV] ................... n_estimators=21, max_depth=464, total=   4.0s\n",
      "[CV] n_estimators=21, max_depth=464 ..................................\n",
      "[CV] ................... n_estimators=21, max_depth=464, total=  31.5s\n",
      "[CV] n_estimators=46, max_depth=464 ..................................\n",
      "[CV] ................... n_estimators=46, max_depth=464, total=   8.1s\n",
      "[CV] n_estimators=46, max_depth=464 ..................................\n",
      "[CV] ................... n_estimators=46, max_depth=464, total= 1.2min\n",
      "[CV] n_estimators=100, max_depth=464 .................................\n",
      "[CV] .................. n_estimators=100, max_depth=464, total=  17.8s\n",
      "[CV] n_estimators=100, max_depth=464 .................................\n",
      "[CV] .................. n_estimators=100, max_depth=464, total= 2.5min\n",
      "[CV] n_estimators=215, max_depth=464 .................................\n",
      "[CV] .................. n_estimators=215, max_depth=464, total=  39.4s\n",
      "[CV] n_estimators=215, max_depth=464 .................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................. n_estimators=215, max_depth=464, total= 5.4min\n",
      "[CV] n_estimators=464, max_depth=464 .................................\n",
      "[CV] .................. n_estimators=464, max_depth=464, total= 1.4min\n",
      "[CV] n_estimators=464, max_depth=464 .................................\n",
      "[CV] .................. n_estimators=464, max_depth=464, total=11.7min\n",
      "[CV] n_estimators=1000, max_depth=464 ................................\n",
      "[CV] ................. n_estimators=1000, max_depth=464, total= 3.1min\n",
      "[CV] n_estimators=1000, max_depth=464 ................................\n",
      "[CV] ................. n_estimators=1000, max_depth=464, total=25.1min\n",
      "[CV] n_estimators=1, max_depth=1000 ..................................\n",
      "[CV] ................... n_estimators=1, max_depth=1000, total=   0.1s\n",
      "[CV] n_estimators=1, max_depth=1000 ..................................\n",
      "[CV] ................... n_estimators=1, max_depth=1000, total=   1.3s\n",
      "[CV] n_estimators=2, max_depth=1000 ..................................\n",
      "[CV] ................... n_estimators=2, max_depth=1000, total=   0.3s\n",
      "[CV] n_estimators=2, max_depth=1000 ..................................\n",
      "[CV] ................... n_estimators=2, max_depth=1000, total=   2.8s\n",
      "[CV] n_estimators=4, max_depth=1000 ..................................\n",
      "[CV] ................... n_estimators=4, max_depth=1000, total=   0.6s\n",
      "[CV] n_estimators=4, max_depth=1000 ..................................\n",
      "[CV] ................... n_estimators=4, max_depth=1000, total=   5.1s\n",
      "[CV] n_estimators=10, max_depth=1000 .................................\n",
      "[CV] .................. n_estimators=10, max_depth=1000, total=   1.6s\n",
      "[CV] n_estimators=10, max_depth=1000 .................................\n",
      "[CV] .................. n_estimators=10, max_depth=1000, total=  13.1s\n",
      "[CV] n_estimators=21, max_depth=1000 .................................\n",
      "[CV] .................. n_estimators=21, max_depth=1000, total=   3.4s\n",
      "[CV] n_estimators=21, max_depth=1000 .................................\n",
      "[CV] .................. n_estimators=21, max_depth=1000, total=  27.2s\n",
      "[CV] n_estimators=46, max_depth=1000 .................................\n",
      "[CV] .................. n_estimators=46, max_depth=1000, total=   7.2s\n",
      "[CV] n_estimators=46, max_depth=1000 .................................\n",
      "[CV] .................. n_estimators=46, max_depth=1000, total= 1.0min\n",
      "[CV] n_estimators=100, max_depth=1000 ................................\n",
      "[CV] ................. n_estimators=100, max_depth=1000, total=  16.3s\n",
      "[CV] n_estimators=100, max_depth=1000 ................................\n",
      "[CV] ................. n_estimators=100, max_depth=1000, total= 2.2min\n",
      "[CV] n_estimators=215, max_depth=1000 ................................\n",
      "[CV] ................. n_estimators=215, max_depth=1000, total=  34.9s\n",
      "[CV] n_estimators=215, max_depth=1000 ................................\n",
      "[CV] ................. n_estimators=215, max_depth=1000, total= 4.7min\n",
      "[CV] n_estimators=464, max_depth=1000 ................................\n",
      "[CV] ................. n_estimators=464, max_depth=1000, total= 1.3min\n",
      "[CV] n_estimators=464, max_depth=1000 ................................\n",
      "[CV] ................. n_estimators=464, max_depth=1000, total=10.1min\n",
      "[CV] n_estimators=1000, max_depth=1000 ...............................\n",
      "[CV] ................ n_estimators=1000, max_depth=1000, total= 2.7min\n",
      "[CV] n_estimators=1000, max_depth=1000 ...............................\n",
      "[CV] ................ n_estimators=1000, max_depth=1000, total=53.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed: 278.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([0, 0, ..., 1, 1])),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': array([   1,    2,    4,   10,   21,   46,  100,  215,  464, 1000]), 'max_depth': array([   1,    2,    4,   10,   21,   46,  100,  215,  464, 1000])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "n_estimators_space = np.logspace(0, 3, num=10, dtype='int')\n",
    "max_depth_space = np.logspace(0, 3, num=10, dtype='int')\n",
    "rfc = RandomForestClassifier()\n",
    "clf_grid = GridSearchCV(rfc, param_grid={'n_estimators':n_estimators_space, 'max_depth':max_depth_space}, verbose=2, cv=PredefinedSplit(ps_array))\n",
    "clf_grid.fit(X_fit, y_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(clf_grid, open(\"models/clf_grid.sav\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_test = pickle.load(open(\"models/clf_grid.sav\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.19.1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "cm = metrics.confusion_matrix(clf_test.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  3, 17, ...,  3,  1,  7])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroup.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/python_libs/anaconda3/envs/ada/lib/python3.5/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/ahmed/python_libs/anaconda3/envs/ada/lib/python3.5/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/ahmed/python_libs/anaconda3/envs/ada/lib/python3.5/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/ahmed/python_libs/anaconda3/envs/ada/lib/python3.5/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034077</td>\n",
       "      <td>0.036555</td>\n",
       "      <td>0.054360</td>\n",
       "      <td>0.066836</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_estimators': 1, 'max_depth': 1}</td>\n",
       "      <td>100</td>\n",
       "      <td>0.053592</td>\n",
       "      <td>0.072718</td>\n",
       "      <td>0.060510</td>\n",
       "      <td>0.060954</td>\n",
       "      <td>0.012018</td>\n",
       "      <td>0.028534</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.005882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.039043</td>\n",
       "      <td>0.039605</td>\n",
       "      <td>0.059195</td>\n",
       "      <td>0.067798</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_estimators': 2, 'max_depth': 1}</td>\n",
       "      <td>99</td>\n",
       "      <td>0.057041</td>\n",
       "      <td>0.075372</td>\n",
       "      <td>0.076433</td>\n",
       "      <td>0.060224</td>\n",
       "      <td>0.016984</td>\n",
       "      <td>0.030581</td>\n",
       "      <td>0.006094</td>\n",
       "      <td>0.007574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.036904</td>\n",
       "      <td>0.052898</td>\n",
       "      <td>0.077472</td>\n",
       "      <td>0.095692</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>{'n_estimators': 4, 'max_depth': 1}</td>\n",
       "      <td>94</td>\n",
       "      <td>0.076076</td>\n",
       "      <td>0.098195</td>\n",
       "      <td>0.088641</td>\n",
       "      <td>0.093188</td>\n",
       "      <td>0.021250</td>\n",
       "      <td>0.041868</td>\n",
       "      <td>0.003948</td>\n",
       "      <td>0.002504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.054689</td>\n",
       "      <td>0.067648</td>\n",
       "      <td>0.077649</td>\n",
       "      <td>0.100103</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 1}</td>\n",
       "      <td>93</td>\n",
       "      <td>0.076010</td>\n",
       "      <td>0.102442</td>\n",
       "      <td>0.090764</td>\n",
       "      <td>0.097765</td>\n",
       "      <td>0.023606</td>\n",
       "      <td>0.044720</td>\n",
       "      <td>0.004636</td>\n",
       "      <td>0.002338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.087716</td>\n",
       "      <td>0.104019</td>\n",
       "      <td>0.099758</td>\n",
       "      <td>0.133702</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>{'n_estimators': 21, 'max_depth': 1}</td>\n",
       "      <td>90</td>\n",
       "      <td>0.093653</td>\n",
       "      <td>0.129512</td>\n",
       "      <td>0.148620</td>\n",
       "      <td>0.137892</td>\n",
       "      <td>0.042595</td>\n",
       "      <td>0.077949</td>\n",
       "      <td>0.017272</td>\n",
       "      <td>0.004190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.155730</td>\n",
       "      <td>0.188178</td>\n",
       "      <td>0.115441</td>\n",
       "      <td>0.169222</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>{'n_estimators': 46, 'max_depth': 1}</td>\n",
       "      <td>86</td>\n",
       "      <td>0.105857</td>\n",
       "      <td>0.143843</td>\n",
       "      <td>0.192144</td>\n",
       "      <td>0.194601</td>\n",
       "      <td>0.078554</td>\n",
       "      <td>0.140029</td>\n",
       "      <td>0.027114</td>\n",
       "      <td>0.025379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.315362</td>\n",
       "      <td>0.393215</td>\n",
       "      <td>0.116974</td>\n",
       "      <td>0.204273</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 1}</td>\n",
       "      <td>83</td>\n",
       "      <td>0.098428</td>\n",
       "      <td>0.130042</td>\n",
       "      <td>0.265393</td>\n",
       "      <td>0.278504</td>\n",
       "      <td>0.165778</td>\n",
       "      <td>0.283310</td>\n",
       "      <td>0.052465</td>\n",
       "      <td>0.074231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.610134</td>\n",
       "      <td>0.748993</td>\n",
       "      <td>0.116149</td>\n",
       "      <td>0.231266</td>\n",
       "      <td>1</td>\n",
       "      <td>215</td>\n",
       "      <td>{'n_estimators': 215, 'max_depth': 1}</td>\n",
       "      <td>84</td>\n",
       "      <td>0.087484</td>\n",
       "      <td>0.116242</td>\n",
       "      <td>0.345541</td>\n",
       "      <td>0.346289</td>\n",
       "      <td>0.325535</td>\n",
       "      <td>0.555561</td>\n",
       "      <td>0.081089</td>\n",
       "      <td>0.115024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.282290</td>\n",
       "      <td>1.589282</td>\n",
       "      <td>0.124698</td>\n",
       "      <td>0.270896</td>\n",
       "      <td>1</td>\n",
       "      <td>464</td>\n",
       "      <td>{'n_estimators': 464, 'max_depth': 1}</td>\n",
       "      <td>80</td>\n",
       "      <td>0.088346</td>\n",
       "      <td>0.119958</td>\n",
       "      <td>0.415605</td>\n",
       "      <td>0.421835</td>\n",
       "      <td>0.692105</td>\n",
       "      <td>1.183326</td>\n",
       "      <td>0.102834</td>\n",
       "      <td>0.150939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.757416</td>\n",
       "      <td>3.403080</td>\n",
       "      <td>0.122693</td>\n",
       "      <td>0.279087</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'n_estimators': 1000, 'max_depth': 1}</td>\n",
       "      <td>81</td>\n",
       "      <td>0.084964</td>\n",
       "      <td>0.118365</td>\n",
       "      <td>0.424628</td>\n",
       "      <td>0.439809</td>\n",
       "      <td>1.465024</td>\n",
       "      <td>2.522992</td>\n",
       "      <td>0.106733</td>\n",
       "      <td>0.160722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.035120</td>\n",
       "      <td>0.035186</td>\n",
       "      <td>0.065916</td>\n",
       "      <td>0.073239</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_estimators': 1, 'max_depth': 2}</td>\n",
       "      <td>97</td>\n",
       "      <td>0.067387</td>\n",
       "      <td>0.088110</td>\n",
       "      <td>0.054140</td>\n",
       "      <td>0.058367</td>\n",
       "      <td>0.014041</td>\n",
       "      <td>0.028170</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>0.014872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.035167</td>\n",
       "      <td>0.042889</td>\n",
       "      <td>0.075113</td>\n",
       "      <td>0.084117</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_estimators': 2, 'max_depth': 2}</td>\n",
       "      <td>96</td>\n",
       "      <td>0.075347</td>\n",
       "      <td>0.094480</td>\n",
       "      <td>0.073248</td>\n",
       "      <td>0.073755</td>\n",
       "      <td>0.013082</td>\n",
       "      <td>0.027242</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.010363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.043646</td>\n",
       "      <td>0.052117</td>\n",
       "      <td>0.076116</td>\n",
       "      <td>0.093670</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>{'n_estimators': 4, 'max_depth': 2}</td>\n",
       "      <td>95</td>\n",
       "      <td>0.076938</td>\n",
       "      <td>0.102972</td>\n",
       "      <td>0.069533</td>\n",
       "      <td>0.084367</td>\n",
       "      <td>0.031490</td>\n",
       "      <td>0.042071</td>\n",
       "      <td>0.002327</td>\n",
       "      <td>0.009303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.071754</td>\n",
       "      <td>0.067163</td>\n",
       "      <td>0.105359</td>\n",
       "      <td>0.132941</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 2}</td>\n",
       "      <td>88</td>\n",
       "      <td>0.104132</td>\n",
       "      <td>0.139066</td>\n",
       "      <td>0.115180</td>\n",
       "      <td>0.126816</td>\n",
       "      <td>0.039575</td>\n",
       "      <td>0.051119</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.006125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.116116</td>\n",
       "      <td>0.103272</td>\n",
       "      <td>0.173457</td>\n",
       "      <td>0.208532</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>{'n_estimators': 21, 'max_depth': 2}</td>\n",
       "      <td>66</td>\n",
       "      <td>0.170326</td>\n",
       "      <td>0.214437</td>\n",
       "      <td>0.198514</td>\n",
       "      <td>0.202627</td>\n",
       "      <td>0.064947</td>\n",
       "      <td>0.079133</td>\n",
       "      <td>0.008858</td>\n",
       "      <td>0.005905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.223630</td>\n",
       "      <td>0.184531</td>\n",
       "      <td>0.183244</td>\n",
       "      <td>0.271080</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>{'n_estimators': 46, 'max_depth': 2}</td>\n",
       "      <td>63</td>\n",
       "      <td>0.169463</td>\n",
       "      <td>0.230361</td>\n",
       "      <td>0.293524</td>\n",
       "      <td>0.311799</td>\n",
       "      <td>0.133515</td>\n",
       "      <td>0.137332</td>\n",
       "      <td>0.038984</td>\n",
       "      <td>0.040719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.444032</td>\n",
       "      <td>0.368696</td>\n",
       "      <td>0.184659</td>\n",
       "      <td>0.300660</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 2}</td>\n",
       "      <td>62</td>\n",
       "      <td>0.160045</td>\n",
       "      <td>0.221868</td>\n",
       "      <td>0.381635</td>\n",
       "      <td>0.379452</td>\n",
       "      <td>0.274060</td>\n",
       "      <td>0.273474</td>\n",
       "      <td>0.069630</td>\n",
       "      <td>0.078792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.920019</td>\n",
       "      <td>0.755425</td>\n",
       "      <td>0.170214</td>\n",
       "      <td>0.324564</td>\n",
       "      <td>2</td>\n",
       "      <td>215</td>\n",
       "      <td>{'n_estimators': 215, 'max_depth': 2}</td>\n",
       "      <td>70</td>\n",
       "      <td>0.133515</td>\n",
       "      <td>0.179936</td>\n",
       "      <td>0.463907</td>\n",
       "      <td>0.469191</td>\n",
       "      <td>0.576384</td>\n",
       "      <td>0.558397</td>\n",
       "      <td>0.103819</td>\n",
       "      <td>0.144628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.977142</td>\n",
       "      <td>1.594763</td>\n",
       "      <td>0.173457</td>\n",
       "      <td>0.341942</td>\n",
       "      <td>2</td>\n",
       "      <td>464</td>\n",
       "      <td>{'n_estimators': 464, 'max_depth': 2}</td>\n",
       "      <td>66</td>\n",
       "      <td>0.133050</td>\n",
       "      <td>0.181529</td>\n",
       "      <td>0.496815</td>\n",
       "      <td>0.502355</td>\n",
       "      <td>1.260473</td>\n",
       "      <td>1.181155</td>\n",
       "      <td>0.114306</td>\n",
       "      <td>0.160413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.201221</td>\n",
       "      <td>3.435231</td>\n",
       "      <td>0.176641</td>\n",
       "      <td>0.362173</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'n_estimators': 1000, 'max_depth': 2}</td>\n",
       "      <td>65</td>\n",
       "      <td>0.132652</td>\n",
       "      <td>0.191614</td>\n",
       "      <td>0.528662</td>\n",
       "      <td>0.532732</td>\n",
       "      <td>2.683573</td>\n",
       "      <td>2.534546</td>\n",
       "      <td>0.124438</td>\n",
       "      <td>0.170559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.041601</td>\n",
       "      <td>0.035492</td>\n",
       "      <td>0.063793</td>\n",
       "      <td>0.085310</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_estimators': 1, 'max_depth': 4}</td>\n",
       "      <td>98</td>\n",
       "      <td>0.060224</td>\n",
       "      <td>0.089703</td>\n",
       "      <td>0.092357</td>\n",
       "      <td>0.080918</td>\n",
       "      <td>0.019541</td>\n",
       "      <td>0.027470</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.004392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.054754</td>\n",
       "      <td>0.039149</td>\n",
       "      <td>0.078828</td>\n",
       "      <td>0.103420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_estimators': 2, 'max_depth': 4}</td>\n",
       "      <td>92</td>\n",
       "      <td>0.076010</td>\n",
       "      <td>0.105096</td>\n",
       "      <td>0.101380</td>\n",
       "      <td>0.101744</td>\n",
       "      <td>0.024432</td>\n",
       "      <td>0.030123</td>\n",
       "      <td>0.007972</td>\n",
       "      <td>0.001676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.065592</td>\n",
       "      <td>0.046164</td>\n",
       "      <td>0.107423</td>\n",
       "      <td>0.130853</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>{'n_estimators': 4, 'max_depth': 4}</td>\n",
       "      <td>87</td>\n",
       "      <td>0.107714</td>\n",
       "      <td>0.146497</td>\n",
       "      <td>0.105096</td>\n",
       "      <td>0.115209</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.036110</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.015644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.093399</td>\n",
       "      <td>0.078450</td>\n",
       "      <td>0.150109</td>\n",
       "      <td>0.211549</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 4}</td>\n",
       "      <td>78</td>\n",
       "      <td>0.142800</td>\n",
       "      <td>0.208068</td>\n",
       "      <td>0.208599</td>\n",
       "      <td>0.215030</td>\n",
       "      <td>0.062125</td>\n",
       "      <td>0.051479</td>\n",
       "      <td>0.020676</td>\n",
       "      <td>0.003481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.188624</td>\n",
       "      <td>0.104712</td>\n",
       "      <td>0.218501</td>\n",
       "      <td>0.293874</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>{'n_estimators': 21, 'max_depth': 4}</td>\n",
       "      <td>56</td>\n",
       "      <td>0.212177</td>\n",
       "      <td>0.294586</td>\n",
       "      <td>0.269108</td>\n",
       "      <td>0.293162</td>\n",
       "      <td>0.123450</td>\n",
       "      <td>0.079645</td>\n",
       "      <td>0.017889</td>\n",
       "      <td>0.000712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.374242</td>\n",
       "      <td>0.194903</td>\n",
       "      <td>0.238076</td>\n",
       "      <td>0.358681</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>{'n_estimators': 46, 'max_depth': 4}</td>\n",
       "      <td>53</td>\n",
       "      <td>0.223387</td>\n",
       "      <td>0.331741</td>\n",
       "      <td>0.355626</td>\n",
       "      <td>0.385620</td>\n",
       "      <td>0.262484</td>\n",
       "      <td>0.144917</td>\n",
       "      <td>0.041554</td>\n",
       "      <td>0.026940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.780618</td>\n",
       "      <td>0.381128</td>\n",
       "      <td>0.263900</td>\n",
       "      <td>0.422127</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 4}</td>\n",
       "      <td>52</td>\n",
       "      <td>0.242422</td>\n",
       "      <td>0.366242</td>\n",
       "      <td>0.435775</td>\n",
       "      <td>0.478013</td>\n",
       "      <td>0.502881</td>\n",
       "      <td>0.284985</td>\n",
       "      <td>0.060757</td>\n",
       "      <td>0.055885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.597919</td>\n",
       "      <td>0.754849</td>\n",
       "      <td>0.277283</td>\n",
       "      <td>0.459005</td>\n",
       "      <td>4</td>\n",
       "      <td>215</td>\n",
       "      <td>{'n_estimators': 215, 'max_depth': 4}</td>\n",
       "      <td>48</td>\n",
       "      <td>0.245208</td>\n",
       "      <td>0.364650</td>\n",
       "      <td>0.533970</td>\n",
       "      <td>0.553359</td>\n",
       "      <td>1.089219</td>\n",
       "      <td>0.557081</td>\n",
       "      <td>0.090738</td>\n",
       "      <td>0.094355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.484868</td>\n",
       "      <td>1.604440</td>\n",
       "      <td>0.271328</td>\n",
       "      <td>0.466698</td>\n",
       "      <td>4</td>\n",
       "      <td>464</td>\n",
       "      <td>{'n_estimators': 464, 'max_depth': 4}</td>\n",
       "      <td>49</td>\n",
       "      <td>0.237116</td>\n",
       "      <td>0.365180</td>\n",
       "      <td>0.545117</td>\n",
       "      <td>0.568216</td>\n",
       "      <td>2.413798</td>\n",
       "      <td>1.184266</td>\n",
       "      <td>0.096783</td>\n",
       "      <td>0.101518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7.376252</td>\n",
       "      <td>3.408659</td>\n",
       "      <td>0.270857</td>\n",
       "      <td>0.464905</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'n_estimators': 1000, 'max_depth': 4}</td>\n",
       "      <td>51</td>\n",
       "      <td>0.234131</td>\n",
       "      <td>0.349788</td>\n",
       "      <td>0.564756</td>\n",
       "      <td>0.580023</td>\n",
       "      <td>5.119468</td>\n",
       "      <td>2.522203</td>\n",
       "      <td>0.103892</td>\n",
       "      <td>0.115117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.716710</td>\n",
       "      <td>0.042907</td>\n",
       "      <td>0.151937</td>\n",
       "      <td>0.647320</td>\n",
       "      <td>215</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_estimators': 1, 'max_depth': 215}</td>\n",
       "      <td>77</td>\n",
       "      <td>0.136964</td>\n",
       "      <td>0.659766</td>\n",
       "      <td>0.271762</td>\n",
       "      <td>0.634874</td>\n",
       "      <td>0.545633</td>\n",
       "      <td>0.027280</td>\n",
       "      <td>0.042358</td>\n",
       "      <td>0.012446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.220232</td>\n",
       "      <td>0.045130</td>\n",
       "      <td>0.157656</td>\n",
       "      <td>0.689376</td>\n",
       "      <td>215</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_estimators': 2, 'max_depth': 215}</td>\n",
       "      <td>75</td>\n",
       "      <td>0.144657</td>\n",
       "      <td>0.687898</td>\n",
       "      <td>0.261677</td>\n",
       "      <td>0.690854</td>\n",
       "      <td>0.905039</td>\n",
       "      <td>0.034081</td>\n",
       "      <td>0.036771</td>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2.505547</td>\n",
       "      <td>0.057139</td>\n",
       "      <td>0.213902</td>\n",
       "      <td>0.885768</td>\n",
       "      <td>215</td>\n",
       "      <td>4</td>\n",
       "      <td>{'n_estimators': 4, 'max_depth': 215}</td>\n",
       "      <td>58</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.892251</td>\n",
       "      <td>0.341826</td>\n",
       "      <td>0.879286</td>\n",
       "      <td>1.918933</td>\n",
       "      <td>0.042095</td>\n",
       "      <td>0.045220</td>\n",
       "      <td>0.006482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6.205317</td>\n",
       "      <td>0.092561</td>\n",
       "      <td>0.322976</td>\n",
       "      <td>0.963847</td>\n",
       "      <td>215</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 215}</td>\n",
       "      <td>43</td>\n",
       "      <td>0.303376</td>\n",
       "      <td>0.970807</td>\n",
       "      <td>0.479830</td>\n",
       "      <td>0.956888</td>\n",
       "      <td>4.841328</td>\n",
       "      <td>0.066189</td>\n",
       "      <td>0.055447</td>\n",
       "      <td>0.006959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>13.021948</td>\n",
       "      <td>0.157917</td>\n",
       "      <td>0.395260</td>\n",
       "      <td>0.972537</td>\n",
       "      <td>215</td>\n",
       "      <td>21</td>\n",
       "      <td>{'n_estimators': 21, 'max_depth': 215}</td>\n",
       "      <td>38</td>\n",
       "      <td>0.373814</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.566879</td>\n",
       "      <td>0.970551</td>\n",
       "      <td>10.094939</td>\n",
       "      <td>0.107452</td>\n",
       "      <td>0.060667</td>\n",
       "      <td>0.001986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>28.241802</td>\n",
       "      <td>0.300488</td>\n",
       "      <td>0.470432</td>\n",
       "      <td>0.973465</td>\n",
       "      <td>215</td>\n",
       "      <td>46</td>\n",
       "      <td>{'n_estimators': 46, 'max_depth': 215}</td>\n",
       "      <td>27</td>\n",
       "      <td>0.451814</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.619427</td>\n",
       "      <td>0.972408</td>\n",
       "      <td>21.724622</td>\n",
       "      <td>0.218189</td>\n",
       "      <td>0.052669</td>\n",
       "      <td>0.001057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>68.616554</td>\n",
       "      <td>0.656379</td>\n",
       "      <td>0.524203</td>\n",
       "      <td>0.973101</td>\n",
       "      <td>215</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 215}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.506931</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.662420</td>\n",
       "      <td>0.971679</td>\n",
       "      <td>41.233220</td>\n",
       "      <td>0.472809</td>\n",
       "      <td>0.048859</td>\n",
       "      <td>0.001422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>132.024186</td>\n",
       "      <td>1.288007</td>\n",
       "      <td>0.556807</td>\n",
       "      <td>0.973697</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>{'n_estimators': 215, 'max_depth': 215}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.542150</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.674098</td>\n",
       "      <td>0.972873</td>\n",
       "      <td>101.841258</td>\n",
       "      <td>0.875164</td>\n",
       "      <td>0.041462</td>\n",
       "      <td>0.000825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>324.337766</td>\n",
       "      <td>2.893135</td>\n",
       "      <td>0.570367</td>\n",
       "      <td>0.973830</td>\n",
       "      <td>215</td>\n",
       "      <td>464</td>\n",
       "      <td>{'n_estimators': 464, 'max_depth': 215}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.556543</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.680998</td>\n",
       "      <td>0.973138</td>\n",
       "      <td>255.475284</td>\n",
       "      <td>1.853367</td>\n",
       "      <td>0.039107</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>704.003063</td>\n",
       "      <td>6.104645</td>\n",
       "      <td>0.580508</td>\n",
       "      <td>0.973830</td>\n",
       "      <td>215</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'n_estimators': 1000, 'max_depth': 215}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.567686</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.683121</td>\n",
       "      <td>0.973138</td>\n",
       "      <td>547.156450</td>\n",
       "      <td>4.141820</td>\n",
       "      <td>0.036273</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.869854</td>\n",
       "      <td>0.042362</td>\n",
       "      <td>0.181180</td>\n",
       "      <td>0.696306</td>\n",
       "      <td>464</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_estimators': 1, 'max_depth': 464}</td>\n",
       "      <td>64</td>\n",
       "      <td>0.169861</td>\n",
       "      <td>0.681529</td>\n",
       "      <td>0.271762</td>\n",
       "      <td>0.711083</td>\n",
       "      <td>0.610165</td>\n",
       "      <td>0.031333</td>\n",
       "      <td>0.032020</td>\n",
       "      <td>0.014777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.579726</td>\n",
       "      <td>0.046123</td>\n",
       "      <td>0.166087</td>\n",
       "      <td>0.692193</td>\n",
       "      <td>464</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_estimators': 2, 'max_depth': 464}</td>\n",
       "      <td>72</td>\n",
       "      <td>0.151953</td>\n",
       "      <td>0.675690</td>\n",
       "      <td>0.279193</td>\n",
       "      <td>0.708695</td>\n",
       "      <td>1.209743</td>\n",
       "      <td>0.033589</td>\n",
       "      <td>0.039982</td>\n",
       "      <td>0.016503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>3.371251</td>\n",
       "      <td>0.058657</td>\n",
       "      <td>0.214374</td>\n",
       "      <td>0.898337</td>\n",
       "      <td>464</td>\n",
       "      <td>4</td>\n",
       "      <td>{'n_estimators': 4, 'max_depth': 464}</td>\n",
       "      <td>57</td>\n",
       "      <td>0.197519</td>\n",
       "      <td>0.890658</td>\n",
       "      <td>0.349257</td>\n",
       "      <td>0.906016</td>\n",
       "      <td>2.671390</td>\n",
       "      <td>0.042112</td>\n",
       "      <td>0.047680</td>\n",
       "      <td>0.007679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>8.411028</td>\n",
       "      <td>0.095755</td>\n",
       "      <td>0.317021</td>\n",
       "      <td>0.971574</td>\n",
       "      <td>464</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 464}</td>\n",
       "      <td>45</td>\n",
       "      <td>0.297208</td>\n",
       "      <td>0.971338</td>\n",
       "      <td>0.475584</td>\n",
       "      <td>0.971811</td>\n",
       "      <td>6.630244</td>\n",
       "      <td>0.066679</td>\n",
       "      <td>0.056051</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>17.679275</td>\n",
       "      <td>0.164954</td>\n",
       "      <td>0.410176</td>\n",
       "      <td>0.974261</td>\n",
       "      <td>464</td>\n",
       "      <td>21</td>\n",
       "      <td>{'n_estimators': 21, 'max_depth': 464}</td>\n",
       "      <td>34</td>\n",
       "      <td>0.392319</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.553079</td>\n",
       "      <td>0.974000</td>\n",
       "      <td>13.860091</td>\n",
       "      <td>0.112313</td>\n",
       "      <td>0.050515</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>38.743477</td>\n",
       "      <td>0.322878</td>\n",
       "      <td>0.460055</td>\n",
       "      <td>0.974328</td>\n",
       "      <td>464</td>\n",
       "      <td>46</td>\n",
       "      <td>{'n_estimators': 46, 'max_depth': 464}</td>\n",
       "      <td>29</td>\n",
       "      <td>0.440539</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.616242</td>\n",
       "      <td>0.974133</td>\n",
       "      <td>31.042849</td>\n",
       "      <td>0.207562</td>\n",
       "      <td>0.055211</td>\n",
       "      <td>0.000195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>83.512448</td>\n",
       "      <td>0.647754</td>\n",
       "      <td>0.522375</td>\n",
       "      <td>0.974328</td>\n",
       "      <td>464</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 464}</td>\n",
       "      <td>19</td>\n",
       "      <td>0.507395</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.642251</td>\n",
       "      <td>0.974133</td>\n",
       "      <td>66.684539</td>\n",
       "      <td>0.426637</td>\n",
       "      <td>0.042375</td>\n",
       "      <td>0.000195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>179.509913</td>\n",
       "      <td>1.371666</td>\n",
       "      <td>0.551618</td>\n",
       "      <td>0.974328</td>\n",
       "      <td>464</td>\n",
       "      <td>215</td>\n",
       "      <td>{'n_estimators': 215, 'max_depth': 464}</td>\n",
       "      <td>14</td>\n",
       "      <td>0.537441</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.665074</td>\n",
       "      <td>0.974133</td>\n",
       "      <td>142.327253</td>\n",
       "      <td>0.938506</td>\n",
       "      <td>0.040106</td>\n",
       "      <td>0.000195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>390.102616</td>\n",
       "      <td>2.944901</td>\n",
       "      <td>0.570957</td>\n",
       "      <td>0.974328</td>\n",
       "      <td>464</td>\n",
       "      <td>464</td>\n",
       "      <td>{'n_estimators': 464, 'max_depth': 464}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.556676</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.685244</td>\n",
       "      <td>0.974133</td>\n",
       "      <td>310.778650</td>\n",
       "      <td>1.923183</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.000195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>839.968199</td>\n",
       "      <td>6.218472</td>\n",
       "      <td>0.576499</td>\n",
       "      <td>0.974328</td>\n",
       "      <td>464</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'n_estimators': 1000, 'max_depth': 464}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.564038</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.676221</td>\n",
       "      <td>0.974133</td>\n",
       "      <td>666.931300</td>\n",
       "      <td>4.239684</td>\n",
       "      <td>0.035251</td>\n",
       "      <td>0.000195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.765545</td>\n",
       "      <td>0.042560</td>\n",
       "      <td>0.172926</td>\n",
       "      <td>0.693421</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_estimators': 1, 'max_depth': 1000}</td>\n",
       "      <td>68</td>\n",
       "      <td>0.162632</td>\n",
       "      <td>0.680467</td>\n",
       "      <td>0.255308</td>\n",
       "      <td>0.706374</td>\n",
       "      <td>0.584882</td>\n",
       "      <td>0.026690</td>\n",
       "      <td>0.029122</td>\n",
       "      <td>0.012953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.634699</td>\n",
       "      <td>0.039956</td>\n",
       "      <td>0.171511</td>\n",
       "      <td>0.689672</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_estimators': 2, 'max_depth': 1000}</td>\n",
       "      <td>69</td>\n",
       "      <td>0.162698</td>\n",
       "      <td>0.672505</td>\n",
       "      <td>0.242038</td>\n",
       "      <td>0.706838</td>\n",
       "      <td>1.292210</td>\n",
       "      <td>0.027650</td>\n",
       "      <td>0.024931</td>\n",
       "      <td>0.017166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>2.908173</td>\n",
       "      <td>0.051590</td>\n",
       "      <td>0.197512</td>\n",
       "      <td>0.899631</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>{'n_estimators': 4, 'max_depth': 1000}</td>\n",
       "      <td>60</td>\n",
       "      <td>0.184122</td>\n",
       "      <td>0.894904</td>\n",
       "      <td>0.304671</td>\n",
       "      <td>0.904358</td>\n",
       "      <td>2.258620</td>\n",
       "      <td>0.048462</td>\n",
       "      <td>0.037880</td>\n",
       "      <td>0.004727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>7.347855</td>\n",
       "      <td>0.089241</td>\n",
       "      <td>0.303107</td>\n",
       "      <td>0.970049</td>\n",
       "      <td>1000</td>\n",
       "      <td>10</td>\n",
       "      <td>{'n_estimators': 10, 'max_depth': 1000}</td>\n",
       "      <td>47</td>\n",
       "      <td>0.282019</td>\n",
       "      <td>0.970276</td>\n",
       "      <td>0.471868</td>\n",
       "      <td>0.969822</td>\n",
       "      <td>5.820292</td>\n",
       "      <td>0.061237</td>\n",
       "      <td>0.059656</td>\n",
       "      <td>0.000227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>15.214570</td>\n",
       "      <td>0.157892</td>\n",
       "      <td>0.402571</td>\n",
       "      <td>0.974062</td>\n",
       "      <td>1000</td>\n",
       "      <td>21</td>\n",
       "      <td>{'n_estimators': 21, 'max_depth': 1000}</td>\n",
       "      <td>36</td>\n",
       "      <td>0.382569</td>\n",
       "      <td>0.973992</td>\n",
       "      <td>0.562633</td>\n",
       "      <td>0.974133</td>\n",
       "      <td>11.995245</td>\n",
       "      <td>0.108730</td>\n",
       "      <td>0.056581</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>33.657006</td>\n",
       "      <td>0.309425</td>\n",
       "      <td>0.475562</td>\n",
       "      <td>0.974328</td>\n",
       "      <td>1000</td>\n",
       "      <td>46</td>\n",
       "      <td>{'n_estimators': 46, 'max_depth': 1000}</td>\n",
       "      <td>26</td>\n",
       "      <td>0.458314</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.613588</td>\n",
       "      <td>0.974133</td>\n",
       "      <td>26.844601</td>\n",
       "      <td>0.210150</td>\n",
       "      <td>0.048792</td>\n",
       "      <td>0.000195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>72.520111</td>\n",
       "      <td>0.615893</td>\n",
       "      <td>0.523672</td>\n",
       "      <td>0.974328</td>\n",
       "      <td>1000</td>\n",
       "      <td>100</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': 1000}</td>\n",
       "      <td>18</td>\n",
       "      <td>0.508656</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.643843</td>\n",
       "      <td>0.974133</td>\n",
       "      <td>57.193478</td>\n",
       "      <td>0.416003</td>\n",
       "      <td>0.042480</td>\n",
       "      <td>0.000195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>156.554372</td>\n",
       "      <td>1.334744</td>\n",
       "      <td>0.554095</td>\n",
       "      <td>0.974328</td>\n",
       "      <td>1000</td>\n",
       "      <td>215</td>\n",
       "      <td>{'n_estimators': 215, 'max_depth': 1000}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.540028</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.974133</td>\n",
       "      <td>123.785718</td>\n",
       "      <td>0.919893</td>\n",
       "      <td>0.039794</td>\n",
       "      <td>0.000195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>337.463849</td>\n",
       "      <td>2.747738</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.974328</td>\n",
       "      <td>1000</td>\n",
       "      <td>464</td>\n",
       "      <td>{'n_estimators': 464, 'max_depth': 1000}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.558201</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.677282</td>\n",
       "      <td>0.974133</td>\n",
       "      <td>266.908679</td>\n",
       "      <td>1.849762</td>\n",
       "      <td>0.037419</td>\n",
       "      <td>0.000195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1692.178601</td>\n",
       "      <td>6.381766</td>\n",
       "      <td>0.576145</td>\n",
       "      <td>0.974328</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'n_estimators': 1000, 'max_depth': 1000}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.563176</td>\n",
       "      <td>0.974522</td>\n",
       "      <td>0.679936</td>\n",
       "      <td>0.974133</td>\n",
       "      <td>1539.788550</td>\n",
       "      <td>3.550188</td>\n",
       "      <td>0.036690</td>\n",
       "      <td>0.000195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0        0.034077         0.036555         0.054360          0.066836   \n",
       "1        0.039043         0.039605         0.059195          0.067798   \n",
       "2        0.036904         0.052898         0.077472          0.095692   \n",
       "3        0.054689         0.067648         0.077649          0.100103   \n",
       "4        0.087716         0.104019         0.099758          0.133702   \n",
       "5        0.155730         0.188178         0.115441          0.169222   \n",
       "6        0.315362         0.393215         0.116974          0.204273   \n",
       "7        0.610134         0.748993         0.116149          0.231266   \n",
       "8        1.282290         1.589282         0.124698          0.270896   \n",
       "9        2.757416         3.403080         0.122693          0.279087   \n",
       "10       0.035120         0.035186         0.065916          0.073239   \n",
       "11       0.035167         0.042889         0.075113          0.084117   \n",
       "12       0.043646         0.052117         0.076116          0.093670   \n",
       "13       0.071754         0.067163         0.105359          0.132941   \n",
       "14       0.116116         0.103272         0.173457          0.208532   \n",
       "15       0.223630         0.184531         0.183244          0.271080   \n",
       "16       0.444032         0.368696         0.184659          0.300660   \n",
       "17       0.920019         0.755425         0.170214          0.324564   \n",
       "18       1.977142         1.594763         0.173457          0.341942   \n",
       "19       4.201221         3.435231         0.176641          0.362173   \n",
       "20       0.041601         0.035492         0.063793          0.085310   \n",
       "21       0.054754         0.039149         0.078828          0.103420   \n",
       "22       0.065592         0.046164         0.107423          0.130853   \n",
       "23       0.093399         0.078450         0.150109          0.211549   \n",
       "24       0.188624         0.104712         0.218501          0.293874   \n",
       "25       0.374242         0.194903         0.238076          0.358681   \n",
       "26       0.780618         0.381128         0.263900          0.422127   \n",
       "27       1.597919         0.754849         0.277283          0.459005   \n",
       "28       3.484868         1.604440         0.271328          0.466698   \n",
       "29       7.376252         3.408659         0.270857          0.464905   \n",
       "..            ...              ...              ...               ...   \n",
       "70       0.716710         0.042907         0.151937          0.647320   \n",
       "71       1.220232         0.045130         0.157656          0.689376   \n",
       "72       2.505547         0.057139         0.213902          0.885768   \n",
       "73       6.205317         0.092561         0.322976          0.963847   \n",
       "74      13.021948         0.157917         0.395260          0.972537   \n",
       "75      28.241802         0.300488         0.470432          0.973465   \n",
       "76      68.616554         0.656379         0.524203          0.973101   \n",
       "77     132.024186         1.288007         0.556807          0.973697   \n",
       "78     324.337766         2.893135         0.570367          0.973830   \n",
       "79     704.003063         6.104645         0.580508          0.973830   \n",
       "80       0.869854         0.042362         0.181180          0.696306   \n",
       "81       1.579726         0.046123         0.166087          0.692193   \n",
       "82       3.371251         0.058657         0.214374          0.898337   \n",
       "83       8.411028         0.095755         0.317021          0.971574   \n",
       "84      17.679275         0.164954         0.410176          0.974261   \n",
       "85      38.743477         0.322878         0.460055          0.974328   \n",
       "86      83.512448         0.647754         0.522375          0.974328   \n",
       "87     179.509913         1.371666         0.551618          0.974328   \n",
       "88     390.102616         2.944901         0.570957          0.974328   \n",
       "89     839.968199         6.218472         0.576499          0.974328   \n",
       "90       0.765545         0.042560         0.172926          0.693421   \n",
       "91       1.634699         0.039956         0.171511          0.689672   \n",
       "92       2.908173         0.051590         0.197512          0.899631   \n",
       "93       7.347855         0.089241         0.303107          0.970049   \n",
       "94      15.214570         0.157892         0.402571          0.974062   \n",
       "95      33.657006         0.309425         0.475562          0.974328   \n",
       "96      72.520111         0.615893         0.523672          0.974328   \n",
       "97     156.554372         1.334744         0.554095          0.974328   \n",
       "98     337.463849         2.747738         0.571429          0.974328   \n",
       "99    1692.178601         6.381766         0.576145          0.974328   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "0                1                  1   \n",
       "1                1                  2   \n",
       "2                1                  4   \n",
       "3                1                 10   \n",
       "4                1                 21   \n",
       "5                1                 46   \n",
       "6                1                100   \n",
       "7                1                215   \n",
       "8                1                464   \n",
       "9                1               1000   \n",
       "10               2                  1   \n",
       "11               2                  2   \n",
       "12               2                  4   \n",
       "13               2                 10   \n",
       "14               2                 21   \n",
       "15               2                 46   \n",
       "16               2                100   \n",
       "17               2                215   \n",
       "18               2                464   \n",
       "19               2               1000   \n",
       "20               4                  1   \n",
       "21               4                  2   \n",
       "22               4                  4   \n",
       "23               4                 10   \n",
       "24               4                 21   \n",
       "25               4                 46   \n",
       "26               4                100   \n",
       "27               4                215   \n",
       "28               4                464   \n",
       "29               4               1000   \n",
       "..             ...                ...   \n",
       "70             215                  1   \n",
       "71             215                  2   \n",
       "72             215                  4   \n",
       "73             215                 10   \n",
       "74             215                 21   \n",
       "75             215                 46   \n",
       "76             215                100   \n",
       "77             215                215   \n",
       "78             215                464   \n",
       "79             215               1000   \n",
       "80             464                  1   \n",
       "81             464                  2   \n",
       "82             464                  4   \n",
       "83             464                 10   \n",
       "84             464                 21   \n",
       "85             464                 46   \n",
       "86             464                100   \n",
       "87             464                215   \n",
       "88             464                464   \n",
       "89             464               1000   \n",
       "90            1000                  1   \n",
       "91            1000                  2   \n",
       "92            1000                  4   \n",
       "93            1000                 10   \n",
       "94            1000                 21   \n",
       "95            1000                 46   \n",
       "96            1000                100   \n",
       "97            1000                215   \n",
       "98            1000                464   \n",
       "99            1000               1000   \n",
       "\n",
       "                                       params  rank_test_score  \\\n",
       "0         {'n_estimators': 1, 'max_depth': 1}              100   \n",
       "1         {'n_estimators': 2, 'max_depth': 1}               99   \n",
       "2         {'n_estimators': 4, 'max_depth': 1}               94   \n",
       "3        {'n_estimators': 10, 'max_depth': 1}               93   \n",
       "4        {'n_estimators': 21, 'max_depth': 1}               90   \n",
       "5        {'n_estimators': 46, 'max_depth': 1}               86   \n",
       "6       {'n_estimators': 100, 'max_depth': 1}               83   \n",
       "7       {'n_estimators': 215, 'max_depth': 1}               84   \n",
       "8       {'n_estimators': 464, 'max_depth': 1}               80   \n",
       "9      {'n_estimators': 1000, 'max_depth': 1}               81   \n",
       "10        {'n_estimators': 1, 'max_depth': 2}               97   \n",
       "11        {'n_estimators': 2, 'max_depth': 2}               96   \n",
       "12        {'n_estimators': 4, 'max_depth': 2}               95   \n",
       "13       {'n_estimators': 10, 'max_depth': 2}               88   \n",
       "14       {'n_estimators': 21, 'max_depth': 2}               66   \n",
       "15       {'n_estimators': 46, 'max_depth': 2}               63   \n",
       "16      {'n_estimators': 100, 'max_depth': 2}               62   \n",
       "17      {'n_estimators': 215, 'max_depth': 2}               70   \n",
       "18      {'n_estimators': 464, 'max_depth': 2}               66   \n",
       "19     {'n_estimators': 1000, 'max_depth': 2}               65   \n",
       "20        {'n_estimators': 1, 'max_depth': 4}               98   \n",
       "21        {'n_estimators': 2, 'max_depth': 4}               92   \n",
       "22        {'n_estimators': 4, 'max_depth': 4}               87   \n",
       "23       {'n_estimators': 10, 'max_depth': 4}               78   \n",
       "24       {'n_estimators': 21, 'max_depth': 4}               56   \n",
       "25       {'n_estimators': 46, 'max_depth': 4}               53   \n",
       "26      {'n_estimators': 100, 'max_depth': 4}               52   \n",
       "27      {'n_estimators': 215, 'max_depth': 4}               48   \n",
       "28      {'n_estimators': 464, 'max_depth': 4}               49   \n",
       "29     {'n_estimators': 1000, 'max_depth': 4}               51   \n",
       "..                                        ...              ...   \n",
       "70      {'n_estimators': 1, 'max_depth': 215}               77   \n",
       "71      {'n_estimators': 2, 'max_depth': 215}               75   \n",
       "72      {'n_estimators': 4, 'max_depth': 215}               58   \n",
       "73     {'n_estimators': 10, 'max_depth': 215}               43   \n",
       "74     {'n_estimators': 21, 'max_depth': 215}               38   \n",
       "75     {'n_estimators': 46, 'max_depth': 215}               27   \n",
       "76    {'n_estimators': 100, 'max_depth': 215}               17   \n",
       "77    {'n_estimators': 215, 'max_depth': 215}               11   \n",
       "78    {'n_estimators': 464, 'max_depth': 215}                8   \n",
       "79   {'n_estimators': 1000, 'max_depth': 215}                2   \n",
       "80      {'n_estimators': 1, 'max_depth': 464}               64   \n",
       "81      {'n_estimators': 2, 'max_depth': 464}               72   \n",
       "82      {'n_estimators': 4, 'max_depth': 464}               57   \n",
       "83     {'n_estimators': 10, 'max_depth': 464}               45   \n",
       "84     {'n_estimators': 21, 'max_depth': 464}               34   \n",
       "85     {'n_estimators': 46, 'max_depth': 464}               29   \n",
       "86    {'n_estimators': 100, 'max_depth': 464}               19   \n",
       "87    {'n_estimators': 215, 'max_depth': 464}               14   \n",
       "88    {'n_estimators': 464, 'max_depth': 464}                7   \n",
       "89   {'n_estimators': 1000, 'max_depth': 464}                3   \n",
       "90     {'n_estimators': 1, 'max_depth': 1000}               68   \n",
       "91     {'n_estimators': 2, 'max_depth': 1000}               69   \n",
       "92     {'n_estimators': 4, 'max_depth': 1000}               60   \n",
       "93    {'n_estimators': 10, 'max_depth': 1000}               47   \n",
       "94    {'n_estimators': 21, 'max_depth': 1000}               36   \n",
       "95    {'n_estimators': 46, 'max_depth': 1000}               26   \n",
       "96   {'n_estimators': 100, 'max_depth': 1000}               18   \n",
       "97   {'n_estimators': 215, 'max_depth': 1000}               13   \n",
       "98   {'n_estimators': 464, 'max_depth': 1000}                6   \n",
       "99  {'n_estimators': 1000, 'max_depth': 1000}                4   \n",
       "\n",
       "    split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0            0.053592            0.072718           0.060510   \n",
       "1            0.057041            0.075372           0.076433   \n",
       "2            0.076076            0.098195           0.088641   \n",
       "3            0.076010            0.102442           0.090764   \n",
       "4            0.093653            0.129512           0.148620   \n",
       "5            0.105857            0.143843           0.192144   \n",
       "6            0.098428            0.130042           0.265393   \n",
       "7            0.087484            0.116242           0.345541   \n",
       "8            0.088346            0.119958           0.415605   \n",
       "9            0.084964            0.118365           0.424628   \n",
       "10           0.067387            0.088110           0.054140   \n",
       "11           0.075347            0.094480           0.073248   \n",
       "12           0.076938            0.102972           0.069533   \n",
       "13           0.104132            0.139066           0.115180   \n",
       "14           0.170326            0.214437           0.198514   \n",
       "15           0.169463            0.230361           0.293524   \n",
       "16           0.160045            0.221868           0.381635   \n",
       "17           0.133515            0.179936           0.463907   \n",
       "18           0.133050            0.181529           0.496815   \n",
       "19           0.132652            0.191614           0.528662   \n",
       "20           0.060224            0.089703           0.092357   \n",
       "21           0.076010            0.105096           0.101380   \n",
       "22           0.107714            0.146497           0.105096   \n",
       "23           0.142800            0.208068           0.208599   \n",
       "24           0.212177            0.294586           0.269108   \n",
       "25           0.223387            0.331741           0.355626   \n",
       "26           0.242422            0.366242           0.435775   \n",
       "27           0.245208            0.364650           0.533970   \n",
       "28           0.237116            0.365180           0.545117   \n",
       "29           0.234131            0.349788           0.564756   \n",
       "..                ...                 ...                ...   \n",
       "70           0.136964            0.659766           0.271762   \n",
       "71           0.144657            0.687898           0.261677   \n",
       "72           0.197917            0.892251           0.341826   \n",
       "73           0.303376            0.970807           0.479830   \n",
       "74           0.373814            0.974522           0.566879   \n",
       "75           0.451814            0.974522           0.619427   \n",
       "76           0.506931            0.974522           0.662420   \n",
       "77           0.542150            0.974522           0.674098   \n",
       "78           0.556543            0.974522           0.680998   \n",
       "79           0.567686            0.974522           0.683121   \n",
       "80           0.169861            0.681529           0.271762   \n",
       "81           0.151953            0.675690           0.279193   \n",
       "82           0.197519            0.890658           0.349257   \n",
       "83           0.297208            0.971338           0.475584   \n",
       "84           0.392319            0.974522           0.553079   \n",
       "85           0.440539            0.974522           0.616242   \n",
       "86           0.507395            0.974522           0.642251   \n",
       "87           0.537441            0.974522           0.665074   \n",
       "88           0.556676            0.974522           0.685244   \n",
       "89           0.564038            0.974522           0.676221   \n",
       "90           0.162632            0.680467           0.255308   \n",
       "91           0.162698            0.672505           0.242038   \n",
       "92           0.184122            0.894904           0.304671   \n",
       "93           0.282019            0.970276           0.471868   \n",
       "94           0.382569            0.973992           0.562633   \n",
       "95           0.458314            0.974522           0.613588   \n",
       "96           0.508656            0.974522           0.643843   \n",
       "97           0.540028            0.974522           0.666667   \n",
       "98           0.558201            0.974522           0.677282   \n",
       "99           0.563176            0.974522           0.679936   \n",
       "\n",
       "    split1_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "0             0.060954      0.012018        0.028534        0.002174   \n",
       "1             0.060224      0.016984        0.030581        0.006094   \n",
       "2             0.093188      0.021250        0.041868        0.003948   \n",
       "3             0.097765      0.023606        0.044720        0.004636   \n",
       "4             0.137892      0.042595        0.077949        0.017272   \n",
       "5             0.194601      0.078554        0.140029        0.027114   \n",
       "6             0.278504      0.165778        0.283310        0.052465   \n",
       "7             0.346289      0.325535        0.555561        0.081089   \n",
       "8             0.421835      0.692105        1.183326        0.102834   \n",
       "9             0.439809      1.465024        2.522992        0.106733   \n",
       "10            0.058367      0.014041        0.028170        0.004163   \n",
       "11            0.073755      0.013082        0.027242        0.000659   \n",
       "12            0.084367      0.031490        0.042071        0.002327   \n",
       "13            0.126816      0.039575        0.051119        0.003472   \n",
       "14            0.202627      0.064947        0.079133        0.008858   \n",
       "15            0.311799      0.133515        0.137332        0.038984   \n",
       "16            0.379452      0.274060        0.273474        0.069630   \n",
       "17            0.469191      0.576384        0.558397        0.103819   \n",
       "18            0.502355      1.260473        1.181155        0.114306   \n",
       "19            0.532732      2.683573        2.534546        0.124438   \n",
       "20            0.080918      0.019541        0.027470        0.010097   \n",
       "21            0.101744      0.024432        0.030123        0.007972   \n",
       "22            0.115209      0.027522        0.036110        0.000823   \n",
       "23            0.215030      0.062125        0.051479        0.020676   \n",
       "24            0.293162      0.123450        0.079645        0.017889   \n",
       "25            0.385620      0.262484        0.144917        0.041554   \n",
       "26            0.478013      0.502881        0.284985        0.060757   \n",
       "27            0.553359      1.089219        0.557081        0.090738   \n",
       "28            0.568216      2.413798        1.184266        0.096783   \n",
       "29            0.580023      5.119468        2.522203        0.103892   \n",
       "..                 ...           ...             ...             ...   \n",
       "70            0.634874      0.545633        0.027280        0.042358   \n",
       "71            0.690854      0.905039        0.034081        0.036771   \n",
       "72            0.879286      1.918933        0.042095        0.045220   \n",
       "73            0.956888      4.841328        0.066189        0.055447   \n",
       "74            0.970551     10.094939        0.107452        0.060667   \n",
       "75            0.972408     21.724622        0.218189        0.052669   \n",
       "76            0.971679     41.233220        0.472809        0.048859   \n",
       "77            0.972873    101.841258        0.875164        0.041462   \n",
       "78            0.973138    255.475284        1.853367        0.039107   \n",
       "79            0.973138    547.156450        4.141820        0.036273   \n",
       "80            0.711083      0.610165        0.031333        0.032020   \n",
       "81            0.708695      1.209743        0.033589        0.039982   \n",
       "82            0.906016      2.671390        0.042112        0.047680   \n",
       "83            0.971811      6.630244        0.066679        0.056051   \n",
       "84            0.974000     13.860091        0.112313        0.050515   \n",
       "85            0.974133     31.042849        0.207562        0.055211   \n",
       "86            0.974133     66.684539        0.426637        0.042375   \n",
       "87            0.974133    142.327253        0.938506        0.040106   \n",
       "88            0.974133    310.778650        1.923183        0.040400   \n",
       "89            0.974133    666.931300        4.239684        0.035251   \n",
       "90            0.706374      0.584882        0.026690        0.029122   \n",
       "91            0.706838      1.292210        0.027650        0.024931   \n",
       "92            0.904358      2.258620        0.048462        0.037880   \n",
       "93            0.969822      5.820292        0.061237        0.059656   \n",
       "94            0.974133     11.995245        0.108730        0.056581   \n",
       "95            0.974133     26.844601        0.210150        0.048792   \n",
       "96            0.974133     57.193478        0.416003        0.042480   \n",
       "97            0.974133    123.785718        0.919893        0.039794   \n",
       "98            0.974133    266.908679        1.849762        0.037419   \n",
       "99            0.974133   1539.788550        3.550188        0.036690   \n",
       "\n",
       "    std_train_score  \n",
       "0          0.005882  \n",
       "1          0.007574  \n",
       "2          0.002504  \n",
       "3          0.002338  \n",
       "4          0.004190  \n",
       "5          0.025379  \n",
       "6          0.074231  \n",
       "7          0.115024  \n",
       "8          0.150939  \n",
       "9          0.160722  \n",
       "10         0.014872  \n",
       "11         0.010363  \n",
       "12         0.009303  \n",
       "13         0.006125  \n",
       "14         0.005905  \n",
       "15         0.040719  \n",
       "16         0.078792  \n",
       "17         0.144628  \n",
       "18         0.160413  \n",
       "19         0.170559  \n",
       "20         0.004392  \n",
       "21         0.001676  \n",
       "22         0.015644  \n",
       "23         0.003481  \n",
       "24         0.000712  \n",
       "25         0.026940  \n",
       "26         0.055885  \n",
       "27         0.094355  \n",
       "28         0.101518  \n",
       "29         0.115117  \n",
       "..              ...  \n",
       "70         0.012446  \n",
       "71         0.001478  \n",
       "72         0.006482  \n",
       "73         0.006959  \n",
       "74         0.001986  \n",
       "75         0.001057  \n",
       "76         0.001422  \n",
       "77         0.000825  \n",
       "78         0.000692  \n",
       "79         0.000692  \n",
       "80         0.014777  \n",
       "81         0.016503  \n",
       "82         0.007679  \n",
       "83         0.000237  \n",
       "84         0.000261  \n",
       "85         0.000195  \n",
       "86         0.000195  \n",
       "87         0.000195  \n",
       "88         0.000195  \n",
       "89         0.000195  \n",
       "90         0.012953  \n",
       "91         0.017166  \n",
       "92         0.004727  \n",
       "93         0.000227  \n",
       "94         0.000071  \n",
       "95         0.000195  \n",
       "96         0.000195  \n",
       "97         0.000195  \n",
       "98         0.000195  \n",
       "99         0.000195  \n",
       "\n",
       "[100 rows x 16 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame.from_dict(clf_test.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 58   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2\n",
      "    1   0]\n",
      " [  0  93   0   0   0   2   0   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   1]\n",
      " [  0   2 101   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   0   0  78   0   1   0   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0  88   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   1   0   0  95   0   1   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0  76   0   1   0   0   0   0   0   0   0   0   0\n",
      "    0   1]\n",
      " [  0   0   0   0   0   0   0 100   0   0   0   1   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  4   4   5   4   8   1   4  11  86   9   5   6   1   7   6   1   5   3\n",
      "    4   7]\n",
      " [  0   0   0   0   0   0   0   0   0  96   0   1   0   0   0   0   0   0\n",
      "    1   2]\n",
      " [  0   0   0   0   0   0   0   0   1   0 100   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 105   1   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1  95   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   0  82   0   0   0   0\n",
      "    1   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0  84   0   0   0\n",
      "    0   0]\n",
      " [  4   0   0   0   0   0   0   0   0   0   0   0   0   0   0  91   0   0\n",
      "    0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0  74   0\n",
      "    0   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  93\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0\n",
      "   91   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0  54]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxIAAAKOCAYAAAAyFy9HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XmcFNW9/vHPAwgugIIYhAFEFkEGEHTABTEmxmAUt7gR\nXFBU4o1i1Osvm8YtmhhN4hr1xlwjxjVeifuGGgVxYVE2t2AUhAFUcAM3YPj+/ugabMcZZobp7unu\ned6vV7/srq46z6mqoa3Tp85pRQRmZmZmZmb10ayxK2BmZmZmZoXHDQkzMzMzM6s3NyTMzMzMzKze\n3JAwMzMzM7N6c0PCzMzMzMzqzQ0JMzMzMzOrNzckzMyaOEmbSXpA0seS7m5AOUdLejyTdWsskoZL\neqOx62Fmls/k35EwMysMkkYDZwF9gZXALOCSiHi2geUeC4wH9oiItQ2uaJ6TFEDviHizsetiZlbI\n3CNhZlYAJJ0FXAn8FugIdAP+DByUgeK3A/7dFBoRdSGpRWPXwcysELghYWaW5yRtCVwEnBoREyPi\n04hYExEPRsTPknVaSbpS0pLkcaWkVsl7e0taLOm/Jb0naamkE5L3LgTOA46StErSiZIukHRrWn53\nSVF5gS3peElvSVop6W1JR6ctfzZtuz0kTU9umZouaY+0956W9BtJU5NyHpfUoYb9r6z/z9Lqf4ik\n/SX9W9IHkn6Vtv5QSc9L+ihZ91pJLZP3JierzU7296i08n8uaRnwt8plyTY9k4ydk9edJb0vae8G\nnVgzswLnhoSZWf7bHdgU+OcG1jkH2A0YBOwEDAXOTXt/W2BLoAQ4EfizpHYRcT6pXo67IqJ1RPzv\nhioiaQvgauAHEdEG2IPULVZV12sPPJSsuzXwJ+AhSVunrTYaOAH4FtASOHsD0duSOgYlpBo+NwLH\nALsAw4FfS9o+WbcCOBPoQOrY7QP8BCAi9krW2SnZ37vSym9PqndmXHpwRPwH+Dlwq6TNgb8BEyLi\n6Q3U18ys6LkhYWaW/7YGltdy69HRwEUR8V5EvA9cCByb9v6a5P01EfEwsAros5H1WQf0l7RZRCyN\niFeqWecAYH5E/D0i1kbEHcDrwIFp6/wtIv4dEZ8D/yDVCKrJGlLjQdYAd5JqJFwVESuT/FdJNaCI\niJkR8UKSuwD4H+Dbddin8yPiy6Q+XxMRNwJvAi8CnUg13MzMmjQ3JMzM8t8KoEMt9+53BhamvV6Y\nLFtfRpWGyGdA6/pWJCI+BY4CTgGWSnpIUt861KeyTiVpr5fVoz4rIqIieV55of9u2vufV24vaQdJ\nD0paJukTUj0u1d42leb9iPiilnVuBPoD10TEl7Wsa2ZW9NyQMDPLf88DXwKHbGCdJaRuy6nULVm2\nMT4FNk97vW36mxHxWETsS+qb+ddJXWDXVp/KOpVvZJ3q43pS9eodEW2BXwGqZZsNTmEoqTWpwe7/\nC1yQ3LplZtakuSFhZpbnIuJjUuMC/pwMMt5c0iaSfiDpsmS1O4BzJW2TDFo+D7i1pjJrMQvYS1K3\nZKD3LyvfkNRR0sHJWIkvSd0ita6aMh4GdpA0WlILSUcB/YAHN7JO9dEG+ARYlfSW/FeV998FetSz\nzKuAGRFxEqmxHzc0uJZmZgXODQkzswIQEX8k9RsS5wLvA4uA04B7k1UuBmYAc4C5wEvJso3JmgTc\nlZQ1k69f/DdL6rEE+IDU2IOqF+pExApgJPDfpG7N+hkwMiKWb0yd6ulsUgO5V5LqLbmryvsXABOS\nWZ2OrK0wSQcD+/HVfp4F7Fw5W5WZWVPlH6QzMzMzM7N6c4+EmZmZmZnVmxsSZmZmZmZWb25ImJmZ\nmZlZvbkhYWZmZmZm9bahHzcyy6lWbdpF6w6dsprRvf3mta9kZmZWg2KZoqa2H1ZpqIULF7B8+fJs\nx+St5m23i1j7ee0rNkB8/v5jEbFfVkNq4YaE5Y3WHTox4qLbs5px048GZbV8MzMrbmsrqvvZlMxq\npuxffzdrlt2MYbuWZbX8fBdrP6dVn1pnl26QL2b9uUNWA+rAtzaZmZmZmVm9uUfCzMzMzCyjBCr+\n7+uLfw/NzMzMzCzj3CNhZmZmZpZJAnIw1qWxuUfCCsZVh/bj0pF9+O0Bfbh4/x0A2K7dZly4X+/1\ny3punblZmR5/7FEGlvahtG8vLr/s0oyV6wxnOKNpZeQqxxn5k7F40SL2//4+lA3qz5DBA7ju2qsz\nnnHKuLFs16UjZYMHZLzsdLn6N2KFSRHFMpGZFbqtt+8XG5q16apD+3Huw2+w8suK9ct+sU8PHnnt\nfWYvWcmgzm0YWdqRiye9WWMZdZ21qaKiggH9duChRyZR0qULe+42hAm33sGO/frVfYec4QxnNPmM\nXOU4I3cZdZm1adnSpSxbtpRBg3dm5cqVDN99CHfePZG+O9Ytoy6zNj07ZTJbtG7NyWPHMOPluXUq\n92sZdZi1qSHHatiuZcycOaP4v5KvQbMtOkarHUdnNeOLmVfOjIhGnR7LPRJW8DbbpHnqvy2b8+Hn\nazJS5vRp0+jZsxfb9+hBy5YtOeKoUTz4wH0ZKdsZznBG08nIVY4z8itj206dGDR4ZwDatGlDn759\nWVJentGMPYfvRft27TNaZlW5+jdihcsNCSsYQfDL7/Xikv134Lu9twbglunljN6lM9f8sB9H79KZ\nu15ekpGsJUvK6dKl6/rXJSVdKM/w/wSc4QxnFH9GrnKckV8Z6RYuWMCcWbMoG7pr1jKyJdfHquhI\n2X3kAQ+2LhCSFgBlwFpgdERcV8/tzwD+EhGfJa9XRUTremx/ENAvIhrtBskLH32TDz9fQ9tNW/DL\nfXqy5OMvGLrdVvx9RjnT3/mYXbfbinG7d+O3T/ynsapoZma23qpVqzjmR0dw6R/+RNu2bRu7OmYZ\n5x6JwrMV8JON2O4MYKNHIkfE/Y3ZiADW37b0yRdrmbHoY3p22Jy9erRn+jsfA/Diwo/okaHB1p07\nl7B48aL1r8vLF1NSUpKRsp3hDGc0nYxc5TgjvzIA1qxZwzGjDufIUaM5+JAfZrz8XMjVsSpOye9I\nZPORB/KjFvY1ku6VNFPSK5LGVXn7UqCnpFmSLq9m2+slzUi2vTBZdjrQGfiXpH+lrXuJpNmSXpDU\nMVm2jaR7JE1PHsOS5cdLujZ5foSkecm2k9Pev1fSJEkLJJ0m6SxJLyflN+hGzlYtmrFpi2brnw/o\n1IZFH33Bh5+vYceOqY6V0m1b8+7KLxsSs17ZkCG8+eZ8Frz9NqtXr+buu+7kgJEHZaRsZzjDGU0n\nI1c5zsivjIjg1B+fRJ++OzL+p2dmtOxcytW/EStcvrUpP42NiA8kbQZMl3RP2nu/APpHRE3TD52T\nbNsceFLSwIi4WtJZwHciYnmy3hbACxFxjqTLgJOBi4GrgCsi4llJ3YDHgB2rZJwHjIiIcklbpS3v\nDwwGNgXeBH4eEYMlXQEcB1xZtbJJQ2kcwOZbd6rxgGy5aQvO/Pb2ADRvBlPf/og5S1by1+cXcdyQ\nEppJrFm3jr++sKjGMuqjRYsWXHHVtRx4wAgqKioYc/xY+pWWZqRsZzjDGU0nI1c5zsivjOefm8od\nt99Kaf8B7DE0Nej6/IsuZsR++2csY8yxo5ky+WlWLF9O7x5dOffXFzDmhBMzVj7k7t9I0cqTcQzZ\n5Olf85CkC4BDk5fdgRHAnaTGSLQGHoyI/jVsewqpC/MWQCdgfETcWTnGorIhIelLYNOICElHAftG\nxEmS3gPSRyxvA/QBDk+2P03SDUBP4B/AxIhYIel4YFhEnJyU/w6we9LYGAsMjIgzNrTftU3/mgl1\nnf7VzMysOnWZ/rWh6jL9a4Mz6jD9a0N4+tdto1X/Y7Oa8cW0PzT69K/ukcgzkvYGvkfqIvwzSU+T\n+oa/LttuD5wNDImIDyXdvIFt18RXrcgKvvpbaAbsFhFfVCl7/fOIOEXSrsABwExJuyRvpd9XtC7t\n9Tr8t2ZmZmZNhcibcQzZVPx7WHi2BD5MGhF9gd2qvL8SaFPDtm2BT4GPkzEPP6jjdukeB8ZXvpD0\nja/wJfWMiBcj4jzgfaBr1XXMzMzMrLi5IZF/HgVaSHqN1MDqF9LfjIgVwNRksPPlAJJmJe/NBl4G\nXgduB6ambfoX4NH0wdY1OB0okzRH0qvAKdWsc7mkuZLmAc8Bs+u7k2ZmZmbFK8u/IZEn4y88RsLy\nhsdImJlZvvMYibpp8mMkWneKVgPGZDXjixd+7zESZmZmZmZFpwmMkXBDwszMzMws0/Lk9qNsKv6m\nkpmZmZlZEyPpJknvJWNaK5e1T348eH7y33Zp7/1S0puS3pA0oi4ZbkiYmZmZmWWUUrc2ZfNRu5uB\n/aos+wXwZET0Bp5MXiOpHzAKKE22uS75ceMNckPCzMzMzKzIRMRk4IMqiw8GJiTPJwCHpC2/MyK+\njIi3gTeBobVleIyE5Y3u7TfP+qxK7Q67IavlA3x4T3Uz5lp1cjH7CUCL5v7OxDIvF3+//tvNP8Vy\nTtaty+6snU1+TlCRizESHSTNSHv9l4j4Sy3bdIyIpcnzZUDH5HkJX//JgcXJsg1yQ8LMzMzMrPAs\nb8j0rxERkhrU5nNDwszMzMws0/Jz+td3JXWKiKWSOgHvJcvLga5p63VJlm1QXu6hmZmZmZll3P1A\n5S/ljQHuS1s+SlIrSdsDvYFptRXmHgkzMzMzs4xSo/dISLoD2JvUWIrFwPnApcA/JJ0ILASOBIiI\nVyT9A3gVWAucGhEVtWW4IWFmZmZmVmQi4kc1vLVPDetfAlxSnww3JMzMzMzMMq2Zf9naLC89/tij\nDCztQ2nfXlx+2aUZK/fUkQOYcfWRzLzmSE47cAAA540ewrSrjuCFKw7ngQsOoFP7zTOWl639KLaM\nxYsWsf/396FsUH+GDB7AdddenfEMKI5j5Yz8y/DfrzMKNeOUcWPZrktHygYPyHjZVhwU0eRn+rU8\nscsuZTH1xRm1rldRUcGAfjvw0COTKOnShT13G8KEW+9gx379at12Q78j0a9bO245e1+Gnz2R1Wsr\nuP+CAxh/3WTe//hzVn6+BoCfjOxP367tOP36KTWWU9ffkWjIftRVvmfUdR7+ZUuXsmzZUgYN3pmV\nK1cyfPch3Hn3RPruWLf9qMu87/l+rJyRfxm5+Put628WFMLxckZ+ZdTldySenTKZLVq35uSxY5jx\n8tx61W/P3Yfw0swZxf+VfA2atS2JVmU/yWrGF/86d2ZDpn/NBPdIWMGZPm0aPXv2YvsePWjZsiVH\nHDWKBx+4r/YNa9G3Szum//tdPl+9lop1wZR5Szhk9x7rGxEAm7fahEy1vbO1H8WYsW2nTgwavDMA\nbdq0oU/fviwpr3VWunoplmPljPzKAP/9OqNwM/Ycvhft27XPaJlWXNyQsIKzZEk5Xbp8NdVxSUkX\nyjPwP+VX3vmAYf060b5NKzZr2YL9dulGlw5bAHDBMUOZ/7/HMOrbvfnN7dMbnAXZ249izEi3cMEC\n5syaRdnQXTNabrEcK2fkV0ZV/vt1RiFlWANJ2X3kATckbIMk3Szp8GqWd5b0f41Rp2x5Y/FH/HHi\nLB64YCT3X7A/s99eQUXS9XvBrdPofeKt3PnMfE45oH8j17TpWrVqFcf86Agu/cOfaNu2bWNXx6xe\n/PdrZsXGDYkmQikZO98RsSQivtHAyIXOnUtYvHjR+tfl5YspKSnJSNkTnnidYf99D/v+6n4+WvUl\n85d8/LX373pmPofs3iMjWdncj2LLAFizZg3HjDqcI0eN5uBDfpjx8ovlWDkjvzIq+e/XGYWYYQ2R\n/I5ENh95ID9qUUQkHSdpjqTZkv4uqbukp5JlT0rqlqx3s6TrJb0g6S1Je0u6SdJrkm5OK2+VpCsk\nvZJsv001mdtImpSs81dJCyV1SLLfkHQLMA/ommTOSNa9MK2MBZIukzRX0jRJvdIi9pL0XFLPw5P1\nu0ualzxvLukPkuYl+zk+WX6ppFeTZX/I1DEuGzKEN9+cz4K332b16tXcfdedHDDyoIyUvc2WmwLQ\ntUNrDt59e+6aPJ+enbZc//7IXbvz7/IPM5KVzf0otoyI4NQfn0Sfvjsy/qdnZrTsSsVyrJyRXxng\nv19nFG6GWW38OxIZJKkUOBfYIyKWS2oPTAAmRMQESWOBq4FDkk3aAbsDB5H6afJhwEnAdEmDImIW\nsAUwIyLOlHQeqV8lPK1K9PnAUxHxO0n7ASemvdcbGBMRLyR1PCciPpDUHHhS0sCImJOs+3FEDJB0\nHHAlMDJZ3gnYE+ib1LPqLU3jgO7AoIhYK6m9pK2BQ4G+ERGStqrhmI1Ltqdrt241H9w0LVq04Iqr\nruXAA0ZQUVHBmOPH0q+0tE7b1uaOn4+gfdtWrFm7jjP+51k+/nQ1N5y2N71LtmJdBO+8t3KDMzbV\nRzb3o9gynn9uKnfcfiul/Qewx9DUoNXzL7qYEfvtn7GMYjlWzsivDPDfrzMKN2PMsaOZMvlpVixf\nTu8eXTn31xcw5oQTa9/QUvJkHEM2efrXDEq+id82Is5JW7Yc6BQRayRtAiyNiA5Jr8OkiLhNUg/g\nsYjonWxzCzAxIu6VVAG0Si7QeyTLB1XJnQUcGhFvJ68/AHYAWgP/iojt09Y9hdSFewtSDYTxEXGn\npAXAdyPiraSeyyJi6/R6JtuvjIg2kroDD0ZEf0n3ADdExKS0nBbAzOTxYLLu6g0dv7pO/9oQG5r+\nNVPqOv2r1X36zIaq6xSaZvWRi79f/+1attRl+teG8PSvXaLVruOzmvHFE7/w9K9N3JfJf9elPa98\nXVNvUX3/5X9a+UTS9sDZwD4RMRB4CNi0hrLTn6fXrU4fChGxFhhKqvdiJPBo/aptZmZmVsA8RsLq\n6SngiOS2HpJbm54DRiXvHw3U976YZkDloObRwLPVrDMVODLJ/D6pW6aq05ZUw+JjSR2BH1R5/6i0\n/z5fjzpOAn6c9EKQ3NrUGtgyIh4GzgR2qkd5ZmZmZpbnPEYigyLiFUmXAM8ktyS9DIwH/ibp/wHv\nAyfUs9hPgaGSzgXeI7nYT25RIiJuAC4E7pB0LKkGwDJgJalbm9LrN1vSy8DrwCJSDZB07STNIdUD\n8aN61PGvpG6lmiNpDXAjcA9wn6RNSfVinFWP8szMzMwKVx791kM2uSGRYRExgdQA63TfrWa949Oe\nLwD6V/de8vobF+FJA6LSx8CIZBzF7sCQiPgS+Fq51ZVdxeUR8fMNrR8RravWObmN6Sy+2VgYuoEs\nMzMzMytgbkgUh27AP5LfiVgNnNzI9TEzMzNr2vJkHEM2uSGR5yp7AGpZZz4wuIE53RuyvZmZmZk1\nLW5ImJmZmZllWhMYI1H8fS5mZmZmZpZx7pEwMzMzM8soeYyEWbF5/x/jsp7Rbq9fZj3jw8m/y3pG\nLjRrAt2+Vrz8q9OWLdn+1WmAZs2y+/nrT/emwQ0JMzMzM7NMawJflvnrFDMzMzMzqzf3SJiZmZmZ\nZZJoEmMkin8PzczMzMws49wjYWZmZmaWUU1j1qbi30MzMzMzM8s4NySsID3+2KMMLO1Dad9eXH7Z\npRkvf/GiRez//X0oG9SfIYMHcN21V290WTf86jAWPnQOM2796fpl7dpsxoNXjmXuXf/Ng1eOZas2\nmwLQvu3mPHrNSbz/xAVccdZBDd4PyP6xylXGKePGsl2XjpQNHpCV8qF4jpUz8isjVznOaFoZufhM\nhNz9GylKUnYfecANCSs4FRUVnHH6qdz3wCO8POdV7r7zDl579dWMZrRo0YLf/v5yZsyax1OTn+Mv\nN1zH669tXMbfH57JwWf+7WvLzj722zw98z8MOOqPPD3zP5x97N4AfLF6DRfdOIlfXvtwQ3cByM2x\nykUGwDHHHs+9DzyS8XIrFcuxckZ+ZeQqxxlNLyPbn4mQu38jVrjckLCCM33aNHr27MX2PXrQsmVL\njjhqFA8+cF9GM7bt1IlBg3cGoE2bNvTp25cl5eUbVdbUWQv44JPPvrZs5PB+3PrwSwDc+vBLHDi8\nHwCffbGG5+Ys5IvVaxtQ+6/k4ljlIgNgz+F70b5d+4yXW6lYjpUz8isjVznOaHoZ2f5MhNz9Gyla\napbdRx7Ij1qY1cOSJeV06dJ1/euSki6Ub+RFfl0sXLCAObNmUTZ014yV+a32rVm2YiUAy1as5Fvt\nW2es7HS5OFa5Ph/ZUizHyhn5lZGrHGc0vYxcKJb9sOxxQ6KRSCqTVK8b7yVdIOnsbNWpPiQdJOkX\njV2PbFu1ahXH/OgILv3Dn2jbtm3WciKyVrSZmZk1hiYwRsLTvzaSiJgBzGjsemysiLgfuL8xsjt3\nLmHx4kXrX5eXL6akpCTjOWvWrOGYUYdz5KjRHHzIDzNa9nsfrGLbrduwbMVKtt26De9/uCqj5VfK\nxbHK1fnItmI5Vs7Ir4xc5Tij6WXkQrHsh2VP1nskJB0naY6k2ZL+Lqm7pKeSZU9K6pasd7Ok6yW9\nIOktSXtLuknSa5JuTitvlaQrJL2SbL9NNZntJd2bZLwgaWCy/NuSZiWPlyW1qWbbVZIuT8p/QtJQ\nSU8ndTooWadU0rSknDmSeldTzlxJWyllhaTjkuW3SNo32b8Hk2UXJPtamXN6WjnnSPq3pGeBPmnL\nByX7NkfSPyW1k/QtSTOT93eSFGnH9z+SNpd0hKR5yfmYXE29u0t6PTkf/5Z0m6TvSZoqab6kocl6\nx0u6Nnn+jTIlNZf0h2T5HEnja/9rqZuyIUN48835LHj7bVavXs3dd93JASMzM8NRpYjg1B+fRJ++\nOzL+p2dmtGyAh559jWP2T43BOGb/nXlwSnYGr+XiWOUiIxeK5Vg5I78ycpXjjKaXkQvFsh+NQmoS\nYySy2iMhqRQ4F9gjIpZLag9MACZExARJY4GrgUOSTdoBuwMHkfq2exhwEjBd0qCImAVsAcyIiDMl\nnQecD5xWJfpC4OWIOETSd4FbgEHA2cCpETFVUmvgi2qqvQXwVET8P0n/BC4G9gX6JXW/HzgFuCoi\nbpPUEmheTTlTk/ovBN4Chif12B34L2BIlfX7At8B2gBvSLoeGAiMSureAngJmJmsfwswPiKekXQR\ncH5EnCFpU0ltk7wZwPCkEfJeRHyWHLMREVEuaatq6g3QCzgCGAtMB0YDe5I6L7/iq/NVqboyxwHd\ngUERsTY5998gaVyyLl27dauhOl/XokULrrjqWg48YAQVFRWMOX4s/UpL67RtXT3/3FTuuP1WSvsP\nYI+hqQv+8y+6mBH77V/vsiZcOIrhg7enw1Zb8Oa9v+A3f32CP/z9GW69+EeMGVnGO8s+4phzb1+/\n/uv3/Iw2W7SiZYvmHLhXP0aecROvL3hvo/YjF8cqFxkAY44dzZTJT7Ni+XJ69+jKub++gDEnnJix\n8ovlWDkjvzJyleOMppeR7c9EyN2/EStciizenJ18C71tRJyTtmw50Cki1kjaBFgaER2SXodJycV5\nD+CxiOidbHMLMDEi7pVUAbRKLk57JMsHVcl9GTgsIt5KXi8CSoGfAIcCtyXbLa6mzl8Cm0ZEJBfo\nX0bEJZKaAR9ExFaSRgPnkLqYnxgR86sp52hSDYGFpBos44DDgH9GxFBJewNnR8RISRcAayLikmTb\n10g1Xg4H2kfEecnyPwFLgBuBuRFR2dvQE7g7InaWdCMwETgBuAPYD5gCDIyIn0m6AegJ/COp+4oq\n9e6enIf0Y/9Y2nmZGBGDJB0PlEXEadWVKeke4IaImFT12NRkl13KYuqL2b3ba23FuqyWD7DNd86p\nfaUG+nDy77KekQvr1uVmcEizZvlxL6mZWV3k4rMx25+Lw3YtY+bMGU32w7dZu+7R6ju/zmrGF/88\naWZElGU1pBb50S/ylS+T/65Le175uqbekzr/a4uIS0n1cGwGTJXUt5rV1sRXrav19YiI9XWIiNtJ\nfTv/OfCwpO9KOjXttqnOwGRSvQLDgaeB90k1DKbUUL30/a1g43uLKnO3A+4DdiLVmzAlqfsppHqJ\nugIzJW1dS13Sz0W156GOZZqZmZk1GZKy+sgH2W5IPAUcUXlhmdze8hyp23UAjqbmC+uaNCN1QQ6p\nW26erWadKUnZJN/8L4+ITyT1jIi5EfF7UrfsVNeQqFXyzfxbEXE1qYv1gRHx54gYlDyWRMQioAPQ\nO+kZeZbUrVXfGJewAZOBQyRtptR4jgMBIuJj4ENJw5P1jgWeSdv3Y4D5SePnA2D/JJ/kGLyY9HK8\nT+riv0FqKHMS8GNJLZJ1sjvZtZmZmZnlVFbHSETEK5IuAZ5Jbkl6GRgP/E3S/yN10XlCPYv9FBgq\n6VzgPeAoAEmnJJk3ABcAN0maA3wGjEm2PUPSd0h9s/4K8Eiy7ayqt0fV4kjgWElrgGXAb2tY70W+\nGj8xBfgd1Td8qhURL0m6C5hNal+np709BrhB0uakxmCckGyzQKlmamWD5VmgS0R8mLy+XKnB4QKe\nBGYnPSh/jYj6DwCooUxgHrADMCc5TjcC125k+WZmZmYFQ5A3vQbZlNUxEtkgaVVEZOfXu6xReYxE\n3XmMRP14jISZFRKPkSh8zdt1j033OT+rGZ/dM7bRx0j4dyTMzMzMzDJJyaPI5dtg61q5N8LMzMzM\nrPG5R8LMzMzMLKPyZ2albCq4HgkzMzMzM2t87pEwMzMzM8uwptAj4YaENSktmme/Ey4XMyqV/vzh\nrGe88vuNnQ3YrGnIxSxwufjMsvzjmeasULghYWZmZmaWYU2hR8JfdZiZmZmZWb25R8LMzMzMLMPc\nI2FmZmZmZlYN90iYmZmZmWWSf9nazMzMzMysem5IWEF6/LFHGVjah9K+vbj8skudkWb7bbbggbP2\nXP+Ydcm+HD+8Ozt2bsP/nb47D5y1J/eeMYyBXbfMSB7k5lidMm4s23XpSNngAVkpHwr3nDsjvzMW\nL1rE/t/fh7JB/RkyeADXXXt1VnKK5Xg5o+llFCMlv2ydzUc+UEQ0dh3MANhll7KY+uKMWterqKhg\nQL8deOiRSZR06cKeuw1hwq13sGO/fhmrS75n1PV3JJoJnjtvH3549VR+e8QA/jZ5Ac+8/j57992G\nk7/Tg6M/IVx1AAAgAElEQVSvf7HGbev6OxIN2Y916+r++fPslMls0bo1J48dw4yX59Z5O6jbnOz5\nfs6dkX8Zdf0diWVLl7Js2VIGDd6ZlStXMnz3Idx590T67lh7Tl1/R6IQjpczmlbGsF3LmDlzRn5c\n7TaCFlv3iNb7XZTVjI9vP3ZmRJRlNaQW7pGwgjN92jR69uzF9j160LJlS444ahQPPnCfM6qxR+8O\nvLPiU5Z8+AUBtN40NSyqzWYteO+TLzOSkYv9ANhz+F60b9c+4+VWKpZz7oz8ygDYtlMnBg3eGYA2\nbdrQp29flpSXZzSjWI6XM5peRjFrCj0SbkhYwVmypJwuXbquf11S0oXyDP9PuVgyRg7uxAMvLwXg\n4ntf5Rcj+/Lsr7/DLw7ckcsffj0jGbnYj1wolnPujPzKqGrhggXMmTWLsqG7ZrTcYjlezmh6GVbY\n3JCohaS/SuqXPF/V2PUBkLRAUocMlnezpMMzVZ7lh02ai31KO/Lw7FRD4ug9tuPi+15jz9/8i0vu\ne5VLjxzYyDU0a1pWrVrFMT86gkv/8Cfatm3b2NUxsyxzj4QRESdFxKuNXY9MkZT1KX+zndG5cwmL\nFy9a/7q8fDElJSXOqOLbfbfhlcUfs2LVagB+WFbCY3OXAfDw7GUM7JaZwda5OFa5UAzn3Bn5l1Fp\nzZo1HDPqcI4cNZqDD/lhxssvluPljKaXYYUtqw0JScdJmiNptqS/S+ou6alk2ZOSuiXr3Szpekkv\nSHpL0t6SbpL0mqSb08pbJekKSa8k229TTea3Jc1KHi9LaiPpFkmHpK1zm6SDJZVKmpasO0dS72rK\ne1pSWdrrb+Qn61whaUZS5yGSJkqaL+niGo7NAkmXSZqb1KFXsryjpH8mx2y2pD1qOLzjJb2UbN83\n2XaopOeT/X5OUp9k+fGS7pf0FPCkUq6V9IakJ4BvJesNkTQxeX6wpM8ltZS0qaS3kuUnS5qe1O0e\nSZunncMbJL0IXCZpi+QcTkvqc3AN+1FvZUOG8Oab81nw9tusXr2au++6kwNGHpSp4osm48DBndff\n1gTw7idfsmvP1BiDPXpvzcL3P8tITi6OVS4Uwzl3Rv5lAEQEp/74JPr03ZHxPz0z4+VD8RwvZzS9\njGLWFHoksvbNsaRS4Fxgj4hYLqk9MAGYEBETJI0FrgYqL/DbAbsDBwH3A8OAk4DpkgZFxCxgC2BG\nRJwp6TzgfOC0KtFnA6dGxFRJrYEvgP8FzgTulbQlsAcwBrgCuCoibpPUEmhey25tKH91RJRJ+ilw\nH7AL8AHwH0lXRMSKasr7OCIGSDoOuBIYmRyTZyLiUEnNgdY11GV5ROws6SfJPp8EvA4Mj4i1kr4H\n/BY4LFl/Z2BgRHwg6YdAH6Af0BF4FbgJeBkYlKw/HJgHDCH1d1I5vc/EiLgRIGkknQhck7zXhdT5\nrpD0W+CpiBgraStgmqQnIuLT9J2QNA4YB9C1W7cadvXrWrRowRVXXcuBB4ygoqKCMcePpV9paZ22\nratCz9isZXOG7dCBc/5v3vplv7p7Lucd3I/mzcWXa9Zxzv/Vb+ajmuTiWAGMOXY0UyY/zYrly+nd\noyvn/voCxpxwYsbKL/Rz7oz8zAB4/rmp3HH7rZT2H8AeQ1ODrs+/6GJG7Fe3mdHqoliOlzOaXoYV\ntqxN/yppPLBtRJyTtmw50Cki1kjaBFgaER2U6nWYlFzQ9wAei4jeyTa3kLp4vVdSBdAquVDukSwf\nVCX3F8ChwG3J+4uT5a8Ae5O6sO4VEWdLGg2cA1RmzK9mP54Gzo6IGTXlJ+uckzRevgv8MiL2Tbaf\nDJyeNITSy10AfDci3kqOxbKI2FrS+0CXiKhxSp1k22ERUS5pV+CSiPiepK6kGiK9gQA2iYi+ko4H\nvh0RJyTbXwnMiYibktcTgdsj4v8kTQJOB/4HuB7oTqqB9UFEXCfp28DFwFakGjmPRcQpyTn8V0RM\nSMqcAWwKrE2q3R4YERGv1bRfdZ3+1eo+/WtD1HX614aoz/SvDVGX6V/N6quu0782RF2nfzXLN01+\n+tcOPWLLAy7JasYHt4z29K9pKi+c16U9r3xdU8/JN65CIuJSUt/ObwZMrbzth1Rj4RjgBFLfvhMR\nt5PqAfkceDhpBNRHen5D61/fK6rKjIq08n9D6mK+P3AgqQv5Sl/rCdiAycAPgDXAE8CeyWNK8v7N\nwGkRMQC4cAMZAg6LiEHJo9uGGhFmZmZmVliy2ZB4CjhC0tYAya1NzwGjkveP5quL07pqBlTOLjQa\neLbqCpJ6RsTciPg9MB2obEjcDJwBUDl4OulVeCsiriZ1O1Jt09jUml9PR6X99/nk+ZPAfyX1a57c\nilVXWwKV87Idv4H1JgNHJeV3Ar6T9t4UUsfp+Yh4H9ia1G1QlffItAGWJr0oR28g4zFS4ziU7Mvg\neuyHmZmZWUFrCmMkstaQiIhXgEuAZyTNBv4EjAdOkDQHOBb4aT2L/RQYKmke8F3gIgBJp0g6JVnn\nDEnzkow1wCNJfd4FXgP+llbekcA8SbOA/qR6LZD0sKTOdc2vq2rKbZfU86ekxnCQPP+OpLnATFLj\nGDZUp3SXAb+T9DIbHv/yT2A+qbERt/BVIwZSYyE6kmpsAMwB5sZX98D9OllnKqkxGTX5DbAJMCe5\nrew3tdTdzMzMzApI1sZIZIOkVRFR0+Dj2rbdHJgL7BwRH2e2ZhtVnwVAWUQsb+y65AuPkag7j5Go\nH4+RsGzwGAmzmjX1MRKbdOgZWx3426xmLL95lMdI5EIyg9FrwDX50IgwMzMzMyt0Wf9xskza2N6I\niHgC2C7D1WmQiOje2HUwMzMzs+zIl3EM2dQkeiTMzMzMzCyzCqpHwszMzMysIBR/h4R7JMzMzMzM\nrP7cI2FWgHIxo9K2Y27NesayCcdkPcMsWzyjkpnVSB4jYWZmZmZmVi33SJiZmZmZZZh7JMzMzMzM\nzKrhHgkzMzMzswxzj4SZmZmZmVk13CNhZmZmZpZBQu6RMMtXjz/2KANL+1DatxeXX3apMxoh4yf7\n9eX534/kuUtH8tdT96TVJs0YsF07Jl04gim/3Z9//eYH7Nxj64zlFfKxckbTzshVjjOcUagZVrgU\nEY1dBzMAdtmlLKa+OKPW9SoqKhjQbwceemQSJV26sOduQ5hw6x3s2K9fxurijA3/jkSndpvx6Hkj\n2PVnD/DFmgr+Nn44k2aXc/ge3bnukdd5YvYS9t2pMz8dWcrISybVWE5df0ci34+VM5zR2DnOcEa+\nZQzbtYyZM2cU/1fyNWi5Ta/ocNhlWc1Y+j+HzYyIsqyG1MI9ElZwpk+bRs+evdi+Rw9atmzJEUeN\n4sEH7nNGjjOaNxebtmxO82Zis1bNWfrh50RAm802AaDt5i1Z+tFnGckq9GPljKabkascZzijUDOs\nsLkhYQVnyZJyunTpuv51SUkXysvLnZHDjKUffs61D73KvKsP5Y0/H8Ynn63hX3OX8su/z+CiH+3M\nvKsP5Tejd+aiu2Y1OAsK+1g5o2ln5CrHGc4o1IyilfyydTYf+cANiQIj6XhJ12awvO6S5mWqPGsa\ntty8Jfvv0pWdzriXvqfdwxatWnDksO058Xs7cM6tM+h/+j/51a0zuObk3Rq7qmZmZpYlbkg0MZKa\nZ7l8Scrq31XnziUsXrxo/evy8sWUlJQ4I4cZe/ffloXvr2LFyi9ZWxE8MP0dhvbuwKjhPbh/eirv\n3hffYeeemRlsXcjHyhlNOyNXOc5wRqFmFDP3SOSYpOMkzZE0W9Lfk2/Ln0qWPSmpW7LezZKul/SC\npLck7S3pJkmvSbo5rbxVkq6Q9Eqy/TbVZH5b0qzk8bKkNpJukXRI2jq3STpYUqmkacm6cyT1rqa8\nBZJ+l6wzQ9LOkh6T9B9JpyTrtE7q85KkuZIOrukY1HCoOkt6VNJ8SZelbXt9kvmKpAur1On3kl4C\njpC0S1L+bODUtPUekjQwef6ypPOS5xdJOrmmeifn6Q1JtwDzgK6Svi/p+WTduyW13vDZr7uyIUN4\n8835LHj7bVavXs3dd93JASMPylTxzqiDxSs+paxXBzZrmWqXfrt0W/695BOWffg5e+7YEYC9Srfl\nrWUrG5wFhX2snNG0M3KV4wxnFGpGMWsKDYm8+R0JSaXAucAeEbFcUntgAjAhIiZIGgtcDVRe4LcD\ndgcOAu4HhgEnAdMlDYqIWcAWwIyIODO5KD4fOK1K9NnAqRExNbnY/QL4X+BM4F5JWwJ7AGOAK4Cr\nIuI2SS2Bmr7dfyciBkm6Arg5qdumpC6yb0gyDo2ITyR1AF6QdD/Qr5pjUJ1BwGDgS+ANSddExCLg\nnIj4QKlehyclDYyIOck2KyJi5+RYzwFOi4jJki5PK3cKMFzSQmBtUm+A4cApG6g3QG9gTES8kLx3\nLvC9iPhU0s+Bs4CLqu6IpHHAOICu3brVsLtf16JFC6646loOPGAEFRUVjDl+LP1KS+u0bV05Y8Nm\n/mcF9097h2cu2Z+1FcHchR9w81PzmbPgAy49rowWzZrxxZoKfvrXFzOwF4V9rJzRtDNyleMMZxRq\nhhW2vJn+VdJ4YNuIOCdt2XKgU0SskbQJsDQiOiS9DpOSC/oewGMR0TvZ5hZgYkTcK6kCaBURa5P1\nJkbEoCq5vwAOBW5L3l+cLH8F2Bs4DOgVEWdLGg2cA1RmzK9mPxYAwyKiPGn87B4RJyfvvQMMBD4l\n1SjZC1gH9AG2B46oegyqKf/4pPzKMh8BLomIZ5Mej3GkGoidgPERcWdSp29HxEJJWwFzIqKyd2cg\ncHtE9Jc0DDidVANuKLBv8ng1Iron56C6em8K/Csitk/KHEmqAbU4qXZL4PmIOLGm/YK6T/9qubGh\n6V8zpa7Tv5qZWWFp8tO/fqtXdDzyj1nNWPznQxp9+te86ZHYCF8m/12X9rzydU379Y1WU0RcKukh\nYH9gqqQREfE6qcbCMcAo4IRk3dslvQgcADws6ccR8dRG1O1oYBtgl6SRtIDUxXhdpZdZAbSQtD2p\n3pUhEfFh0thKL/PTOpQ7HSgD3gImAR2Ak4GZyfsbqnd6+SLV0PtRPfbJzMzMzApIPo2ReIrU/ftb\nAyS39TxH6kIeUhexU+pZZjPg8OT5aODZqitI6hkRcyPi96QupPsmb90MnAEQEa8m6/YA3oqIq4H7\nSPUubIwtgfeSi/HvANsly6s7BnXVltTF/MeSOgI/qG6liPgI+EjSnsmio9PeWw0sItUz8jyp4302\nMLmWelf1AjBMUq9kP7aQtEM99sXMzMysoDX2GAlJZyo1bnaepDskbSqpvaRJSo2znSSpXUP2MW8a\nEhHxCnAJ8IxSg4D/BIwHTkju6T8W+Gk9i/0UGKrU9KbfJblHX9IpyW1AAGckB3gOsAZ4JKnPu8Br\nwN/SyjsSmCdpFtCfVK8Fkh6W1Lke9boNKJM0FzgOeH0DxwBJB0n6xviCdBExG3g5Ket2YOoGVj8B\n+HOyH1X/EqeQaix8njzvwlcNuGrrXU1d3geOB+5IjuvzfNVAMzMzM7MsklRC6nb1sojoT2pc7yjg\nF8CTyZCAJ5PXG5+TL2MkskHSqojYqNmCJG0OzAV2joiPM1szq47HSOQXj5EwM7ON1dTHSLTq2Du2\nPepPWc1455qDahwjkTQkXgB2Aj4B7iU1adE1wN4RsVRSJ+DpiOizsXXImx6JfCLpe6R6I65xI8LM\nzMzMCklElAN/AN4BlgIfR8TjQMeIWJqstgzo2JCcQh5sXauN7Y2IiCeo+f5/MzMzM7MNqss4hgbq\nICn9Vo6/RMRfkux2wMGkZtf8CLhb0tduA4iIkNSgW5OKuiFhZmZmZlaklm9g+tfvAW8n41aRNJHU\n76K9K6lT2q1N7zWkAr61yczMzMwswxp51qZ3gN0kba7UyvuQum3/flI/skzy3/saso/ukTAzMzMz\nKyIR8aKk/wNeAtaSmtnzL0Br4B+STgQWkpqRdKO5IWFNyrp1xTFLWbNm2Z8IIxczKrU78MqsZwB8\n+MAZWc/Ixd9WLs67mZllSCN/ZEfE+cD5VRZ/Sap3IiN8a5OZmZmZmdWbeyTMzMzMzDIsB7M2NTr3\nSJiZmZmZWb25R8LMzMzMLJPkHgkzMzMzM7NquUfCzMzMzCyDBDSBDgn3SFhhevyxRxlY2ofSvr24\n/LJLM17+KePGsl2XjpQNHpDxsnOdk+1jlc2MUw8exIzrj2HmDcdy2iGDATjn6N34z99P4oVrj+aF\na49mxJDuGcvLxbHyOW96GbnKcYYzCjXDCpcbElZwKioqOOP0U7nvgUd4ec6r3H3nHbz26qsZzTjm\n2OO594FHMlpmY+Tk4lhlK6Pfdltzwn79GX7GnQz9ya38YOj29Oi0JQDX3PsSu512G7uddhuPTV/Q\n4CzIzbECn/OmlpGrHGc4o1Azild2f9U6X8ZfuCFhBWf6tGn07NmL7Xv0oGXLlhxx1CgefKBBv/D+\nDXsO34v27dpntMzGyMnFscpWRt+u7Zn+xjI+/3ItFeuCKXMXc8iwXhmocfVycazA57ypZeQqxxnO\nKNQMK2xuSFjBWbKknC5duq5/XVLShfLy8kasUf7KxbHKVsYrC5czrLSE9m02ZbNWLdhvyPZ02aYN\nAP914CCmXXc0N5y5L1u1btXgLCiev6tCPufFmJGrHGc4o1AzipmU3Uc+cEOiwEkqk3R1Pbe5QNLZ\n2aqTWSa8sehD/nj3DB645FDu/80hzH7rfSrWBTc+NIcdx/6NXU+9jWUffMqlJ+/V2FU1MzNrkjxr\nU4GLiBnAjMauRy517lzC4sWL1r8uL19MSUlJI9Yof+XiWGUzY8LjrzDh8VcAuHDMHpQvX8V7H322\n/v2bHpnHxAsPykhWsfxdFfo5L7aMXOU4wxmFmlHM8mUcQza5R6IeJB0naY6k2ZL+Lqm7pKeSZU9K\n6pasd7Ok6yW9IOktSXtLuknSa5JuTitvlaQrJL2SbL9NNZlzJW2llBWSjkuW3yJp36TsB5NlFyQ5\nTye5p6eVc46kf0t6FuiTtnxQUs85kv4pqZ2kb0mamby/k6RI27f/SNpc0hGS5iXHYnI19R6SlLmp\npC2SfeyfifNQNmQIb745nwVvv83q1au5+647OWBkZi4mi00ujlU2M7bZcjMAum7ThoOH9eKup99g\n23abr3//4D168urCFRnJKpa/q0I/58WWkascZzijUDOssLlHoo4klQLnAntExHJJ7YEJwISImCBp\nLHA1cEiySTtgd+Ag4H5gGHASMF3SoIiYBWwBzIiIMyWdB5wPnFYlemqy7ULgLWA4cEtS9n8BQ6qs\n3xf4DtAGeEPS9cBAYBQwiNQ5fwmYmax/CzA+Ip6RdBFwfkSckTQA2iZ5M4DhSSPkvYj4LKnviIgo\nl7RV1eMVEdMl3Q9cDGwG3BoR86o5ruOAcQBdu3Wr7tB/Q4sWLbjiqms58IARVFRUMOb4sfQrLa3T\ntnU15tjRTJn8NCuWL6d3j66c++sLGHPCiRnNyEVOLo5VNjPuOHck7dtuypq16zjjun/x8adf8qf/\nGsHAHtsQBAvf/YTxVz+ZkaxcHCvwOW9qGbnKcYYzCjWjaOXROIZsUkQ0dh0KgqTxwLYRcU7asuVA\np4hYI2kTYGlEdEh6HSZFxG2SegCPRUTvZJtbgIkRca+kCqBVRKxN1psYEYOq5B5NqiGwEPiC1EX3\nYcA/I2KopL2BsyNipKQLgDURcUmy7WvAvsDhQPuIOC9Z/idgCXAjMDciKnsbegJ3R8TOkm4EJgIn\nAHcA+wFTgIER8TNJNwA9gX8k9f7G18KSWgLTk3rvEREVGzrGu+xSFlNfzO5dWuvWFcffe7NmxfHp\n1O7AK3OS8+EDZ2Q9Ixd/W8Vy3s2s+A3btYyZM2c02Q+tTTvtEN3HXJPVjDd+v9/MiCjLakgtfGtT\n9nyZ/Hdd2vPK1zX1BFV3JTKZVK/AcOBp4H1SDYMpteQCVGwgqzaVudsB9wE7AXtW5kbEKaR6aLoC\nMyVtXU0ZWwOtSfWObLqR9TAzMzMrKCL15U82H/nADYm6ewo4ovKCObm16TlStwwBHE3NF/c1aUaq\nUQAwGni26goRsQjoAPSOiLeSdc4mdaFfV5OBQyRtJqkNcGBS9sfAh5KGJ+sdCzyTPJ8CHAPMj4h1\nwAfA/pV1lNQzIl5MejneJ9WgqOp/gF8DtwG/r0d9zczMzCzPeYxEHUXEK5IuAZ5Jbkl6GRgP/E3S\n/yN1MX1CPYv9FBgq6VzgPeAoAEmnJJk3JOu9CDRPnk8Bfkc1jY4N1P0lSXcBs5Oc6WlvjwFukLQ5\nqTEYJyTbLFBquoHKBsuzQJeI+DB5fbmk3qQa3U8CsyV1Bv4aEfsng8LXRMTtkpoDz0n6bkQ8Vdd6\nm5mZmRUqj5GwrJK0KiJaN3Y98oXHSNRdvnRpNpTHSNRPsZx3Myt+TX2MxGaddogeY6/Nasarvx3R\n6GMk3CNhZmZmZpZh/h0Jyyr3RpiZmZlZoXKPhJmZmZlZJjWR35Fwj4SZmZmZmdWbeyTMzMzMzDJI\neIyEmZmZmZlZtdwjYU1KsUyfWSxTjeZiWlaAQec+lvWMWRePyHpGsZx3M2t82f48KY7J1htC7pEw\nMzMzMzOrjnskzMzMzMwyrAl0SLhHwszMzMzM6s89EmZmZmZmGeYxEmZmZmZmZtVwQ8IK0uOPPcrA\n0j6U9u3F5Zdd6owNOGXcWLbr0pGywQOyUj4U7rHavsPm/PP03dc/ZlywD8cN2w6AY/boxsNnDeOB\nM4dx9g92yEge+Jw3xYxc5TjDGZmWi8+SopX8snU2H/nADQkrOBUVFZxx+qnc98AjvDznVe6+8w5e\ne/VVZ9TgmGOP594HHsl4uZUK+Vi9vfwzDr36eQ69+nkOu+Z5Pl9TwROvvMuuPdrz3R2/xcFXPceB\nV0zlpskLGr4T+Jw3xYxc5TjDGdnIyPZnSTGr/EG6bD7ygRsSVnCmT5tGz5692L5HD1q2bMkRR43i\nwQfuc0YN9hy+F+3btc94uZWK5Vjt3mtrFq34jCUffcGo3bpy4zNvsaYiNRP6B5+uzkiGz3nTy8hV\njjOckY2MbH+WWOFzQ8IKzpIl5XTp0nX965KSLpSXlzujkRTLsdp/p215aPYyALp32Jyy7u246ye7\n8vdxQ+jfpW1GMnzOm15GrnKc4YxC/TwpZr61yfKCpIMk/aKBZWwj6UVJL0sanqm6Vck4XtK12Sjb\nLJs2aS6+u+O3eHRuqiHRvJnYcvNNOOq6F7ns4X9z5eidGrmGZmZm+cfTvxaAiLgfuL+BxewDzI2I\nk+q6gaTmEVHRwNyM69y5hMWLF61/XV6+mJKSEmc0kmI4VsP7dODV8k9YsSp1C9O7H3/JpHnvATB3\n8cesC2i3xSZ8+OmaBuX4nDe9jFzlOMMZhfp5UszyZRxDNrlHopFJ6i7pdUk3S/q3pNskfU/SVEnz\nJQ1N/6Zf0hGS5kmaLWlysqy5pD8ky+dIGl8lYxBwGXCwpFmSNpP0I0lzk21+n7buKkl/lDQb2F3S\npZJeTcr9Q7LOgWm9G09I6ljNfm0j6R5J05PHsEwds7IhQ3jzzfksePttVq9ezd133ckBIw/KVPFF\nlZELxXCsDtipEw/NXrr+9ROvvsvQnqn7grt32JxNmqvBjQjwOW+KGbnKcYYzCvXzxAqbeyTyQy/g\nCGAsMB0YDewJHAT8Crg3bd3zgBERUS5pq2TZOKA7MCgi1kr62sioiJgl6TygLCJOk9QZ+D2wC/Ah\n8LikQyLiXmAL4MWI+G9JWwP/C/SNiEjLexbYLVl2EvAz4L+r7NNVwBUR8aykbsBjwI5Vd1zSuKT+\ndO3WrU4Hq0WLFlxx1bUceMAIKioqGHP8WPqVltZp27oqlgyAMceOZsrkp1mxfDm9e3Tl3F9fwJgT\nTsxY+YV+rDbbpDnDem3N+RO/mu1k4oxyLjm8P/efsQdrKoJf3D0vI1k+500vI1c5znBGNjKy/VlS\n7JpAhwSKiMauQ5MmqTswKSJ6J69vAR6LiNsk9QAmAlfyVSPgBqAn8A9gYkSskHQPcENETNpAzvFp\nZRwMHBYRxyXvnQiURsRZktYCrSKiQlILYGbyeBB4MCJWSxoA/BHoBLQE3o6I/apkvAcsSavCNkCf\niFhVUx132aUspr44o17Hr6laty77/26bNSueT8BB5z6W9YxZF4/IeobPu5llSrY/T/bcfQgvzZzR\nZD9QtijpE6Wn/n/27jw+qur+//jrE8Iqu6BCAiK7AsoSQAUUFSsKClgpqCCLSulXxeVnW1upUqst\nFb/WrUrVbxWLBUVbEZBNFkFANllFURSsBKmsKggSks/vj7mhMU0ggbmTmcn7yWMeZO7ce97nLpPM\nmXPPvX8JNWP5vRetdPeMUEOOQac2xYfv8/yck+d5Dvl6jdx9ODASqAesDHoNoulg7rgIdz8MdABe\nA3oCM4J5ngSecvdWwE+BCgWUk0Kk16J18Eg7WiNCREREJGmY7iMhccjMGrn7Une/D9hBpEExG/hp\n0INA/lObCrAMuNDMaplZGeBa4J0CsioD1dz9LeBOIPfSNdWA3GvMDSokYxZwZKxGME5DRERERJKE\nxkgknjFm1oTITRPnAGuA9UBTYK2ZZQHPAU+Z2QPAiuCqT0e4+5fB5WTnBeVMc/eC7mJTBZhsZhWC\n+e4Kpo8CJpnZHmAucEYBy44A/mxma4kcZwuA4ce/2iIiIiKJIXJn65KuRfjUkChh7r4FaJnn+eBC\nXnsxmHZ1AcUcJvIh/668E4Nei9yfX8wtI3g+AZhQQH0q5/n5SyKnNuWfZzLwXw2PvBnuvhPoV0Bd\nRURERCQJqCEhIiIiIhJV8TOOIUwaIyEiIiIiIsWmHgkRERERkSgrBR0S6pEQEREREZHiU4+EiIiI\niEiUaYyEiIiIiIhIAdQjIZKAUlKS/1uOaFr94GWhZ9S46L5jz3SC9sx7IPSMZJGT4+FnePgZqWX0\nfdXwwmYAACAASURBVJ+EI+y/I6X+r5RpjISIiIiIiEiB1CMhIiIiIhJFkTtbJ3+XhHokRERERESk\n2NQjISIiIiISZeqREBERERERKYAaEpKQZs2cwdktmtGieWPGPDxaGcqIq4yx9/Tm8zd/wYpxtxyZ\nVqNKRaY+Ooh1f7+dqY8OonrlCkdeu3tAF9ZPuJ01L4+gW4fGJ7QOkFjbqqQzhg8byunpp5LRplUo\n5QNs/eILrvjRJWS0bkn7Nq14+qknQslJln2ijNKXkazMwn3EAzUkJOFkZ2dzx4hbmDxlOqvWbmDS\nxAl8uGGDMpQRNxl/m76KXnf/7QfT7h7QhfkrP6PVdY8zf+Vn3D2gCwDNG9Sm7yWtaHvDU1x190s8\nflfPE7osY6Jtq5LMABgwcDBvTJke9XLzSk1N5fd/HMOK1euZu2Axz459mo8+TLztpQxlhPU+lMSl\nhoQknOXLltGoUWPOaNiQcuXK0bdff6ZOmawMZcRNxqI1n7P7mwM/mNazc3PGz1gFwPgZq7iyy5lH\npk+as45DWdl8/uVePs3cTfsz0+NiPZI9A6BzlwuoWaNm1MvN67Q6dWjdpi0AVapUoVnz5mzLzIxq\nRrLsE2WUvoxkZmahPuKBGhKScLZtyyQ9vd6R52lp6WRG+Y+yMpQR7YxTapzE9l37ANi+ax+n1Dgp\nklOrKlu/+vrIfJlffU3d2lWOOycZtlWsMkrC51u2sHb1ajI6dIxqucmyT5RR+jIksemqTVJsZtYb\n+Njd1b8pcpzCvyeyxJt9+/Yx4Nq+jH7kUapWrVrS1RGRMMXROIYwqUciCVhELPdlb+CsGOb9QN26\naWzd+sWR55mZW0lLS1OGMuI646s9+znt5MoAnHZyZXbs2R/J2fkN6adUOzJf2inV2Lbj2+POSYZt\nFauMWMrKymJA/2v4Sf/r6NX76qiXnyz7RBmlL0MSmxoSCcrMGpjZRjN7CVgPDDSzJWb2vplNMrPK\nwXztzWyxma0xs2Vm9l/nTJjZG2a20sw+MLNheabvy/PzNWb2opmdD1wFjDGz1WbWyMxam9l7ZrbW\nzP5pZjWCZUaY2YZg+sRorXtG+/Zs2vQJWzZv5tChQ0x6ZSI9el4VreKVoYxQMqYt+ogB3dsAMKB7\nG6a++1Fk+rsf0feSVpQrW4bT61SncXpNln+49bhzkmFbxSojVtydW356E82an8ltt98ZSkay7BNl\nlL6MZGWEOz4iXsZI6NSmxNYEGARsAv4BdHP3/Wb2S+AuMxsNvAL0c/flZlYVOFBAOUPdfbeZVQSW\nm9nr7r6roEB3X2xmbwJT3f01ADNbC9zm7u+Y2QPA/cAdwD3AGe7+vZlVL6i8oOEyDKBe/fpFWunU\n1FT+9PhTXNnjMrKzsxk0eChntWhRpGWLShnKOJGMcfdfQ5c2Z1CrWiU2vf7/+N1f5/HI+IWMf6Af\ng3q05V//3suA+14F4MMtO3h97npW/e02DmfncMej08jJOf4TnxJtW5VkBsCggdexcMF8du3cSZOG\n9Rj5m1EMGnJjVDOWLF7EhL+Pp0XLVpzfITLo+v4HHuSy7ldELSNZ9okySl+GJDZz15m6icjMGgDz\n3P0MM+sJvAjkfo1ZDlgCPAaMdfdOxyhrFNAneNoAuMzd3zOzfe6e27NxDdDT3Qeb2YsEDQkzqwas\nc/f6wXyNgEnu3tbMZgD7gDeAN9x9H0fRrl2GL1q6ohhbQSR+1LjovtAz9sx7IPSMZHEijbEiZ8Tg\n72dqGZ04IImpU8cMVq5cER9fm5eAqvXP9PY//2uoGXNHnL/S3TNCDTkG9Ugktv3B/wbMdvdr875o\nZse8w5KZdQW6Aee5+3dmNh/IvVNW3r+SFSi+HsAFwJXAvWbWyt0PH0c5IiIiIhJn9FVHcngP6GRm\njQHM7CQzawpsBOqYWftgehUzy994rAbsCRoRzYFz87z2bzM7MxjI3SfP9G+BKgDu/jWwx8y6BK8N\nBN4Jlqnn7vOAXwY5laO4ziIiIiJxK8Us1Ec8UEMiCbj7DmAwMCEYr7AEaO7uh4B+wJNmtgaYDVQw\ns7pm9law+Awg1cw+BEYTaZTkugeYCiwGvswzfSLwczNbFZzKNIjI4Ou1QGvgAaAMMN7M1gGrgCfc\nfW8Iqy8iIiIiJUCnNiUod98CtMzzfC7QvoD5lvPDXgaIjFu4Inj9e+DyQjJeA14rYPoi/vvyr/kz\nADoXugIiIiIiSSxOOg1CpR4JEREREREpNvVIiIiIiIhEkRlxc6+HMKlHQkREREREik09EiIiIiIi\nUZaS/B0S6pEQEREREZHiU4+EiIiIiEiUlYYxEmpISNzIcTiYlV3S1ThhOTl+7JlOUEoM+ku/z8oJ\nPaNKhdj8CjqUHf66fDz53tAz0m+aGHrGusevDj2jfGr4neEe/tuQiuXKhJ7xzYGs0DPKxODDTiy2\nFcTmb0hqmfCP35wYHMCHDof7ezE7Fm9CKXFqSIiIiIiIRFkp6JBQQ0JEREREJJoMMJK/JaHB1iIi\nIiIiUmzqkRARERERiTJd/lVERERERKQAakhIwsrOzuaCczPod/VVCZvR+qzGdO7QmgvPa8fFXTqG\nkgHhr8df/vw4F3Q8hwvPbc3woQM4ePBg1DOGDxvK6emnktGmVdTLziuMbXX3bcNo06we3Tq1PTJt\n6uTXueT8NpxeqyJrVq084YzhP2rKuw9dzsIHu/Ps8PMoXzby6/2mbk1Y8ocrePehy7n/J+eccE6u\n5595kovOa0PXc1vz3NNPRK3c/GLxHvl6716GDOjHuW1bcl67VixfuiSq5cfi2N30yUYu7pRx5NEo\n7WT+8ufo75dk2FYQ/nF18OBBLup8Lp06tKFj21b8/nejop6RK8zf77E6rpKSGRbyIx6oISEJa+yf\nn6Bp8+YJnzH5rbd5Z8lK5i5cGlpGmOvx5bZMnh/7Z2bOf4933ltNdnY2b7z+atRzBgwczBtTpke9\n3PzC2FZ9rx3IS6+++YNpzZq34Nlxr9Dx/M4nXP5p1Sty86VN6TZqFl1GziAlxejT8XQ6Nz+Fy9uk\nceFvZtD53un8efpHJ5wF8NGGD3j5pb8ybc4i3n53BbNnvsXmzzZFpeyChP0e+fUv7uTibj/ivffX\n886SlTRtdmZUy4/Fsdu4STPmLlrB3EUrmL1gKRUrVuKKK3tFPScZtlWuMI+r8uXLM2XG2yxatop3\nl77P27Nmsnzpe1HPgXB/v8fquJLEpYaEJKTMrVuZNeMtbhg8NKEzYiEW65GdfZiDBw5w+PBhDhw4\nwGmn1Yl6RucuF1CzRs2ol5tXWNuq4/ldqF6jxg+mNWnWnEZNmkYtIzUlhQrlylAmxahUrgzb9xxg\n8MWNeXzah0euF7/z2++jkvXJxx/Rpl0HKlWqRGpqKud1uoC3prwRlbJj7Zuvv2bJ4ncZMCiyz8uV\nK0e16tWjmhGLYzevhfPn0uCMhtSrf3pUy03GbRUWM6Ny5coAZGVlkXU4K5RvkGP5dyqs4yqZmYX7\niAdqSEhC+vUv7uK3D44mJSW8QzgWGWbG1VdexsWdOzDur8+FkhH2etSpm8bPbruTdi0bcXbT+lSt\nWpWul1waSlbYYrHPw7B97wH+POMjVv/vlXzwWC++OZDF/A+20+i0KpzXtDYzf3Mpb95zMW3OiM4H\ntOZnnsWyJe+ye/cuvvvuO+bOnsG2rVujUnZ+Yb9HPv98MyfXqsVtw2/kok4Z3H7LMPbv3x/1nFj6\n5+uv0ueaflEvN5m2VSx+92ZnZ9O5Y1sa1z+Niy7uRkaH6J9CFcvfWWEdV5LYEuuvpfyAmXU1s/NL\nKHuUmd1dEtkz3ppKrdqn0Lptu4TOAJg2ez7vLFnJK/+Yyv89+wyL310Y1fJjsR579+xhxrQpLFv7\nMWs2fs533+3ntVdeDi0vLLHa52GoVqksl7dJo93Pp9LyzslUKp9K3/NOJzXFqF65HJf9bjb3v7Ka\n5/8nOr8umjQ7k/+5/W6u7dOD6398JS1anU2ZMuHcuTjs98jhw4dZu3oVQ276KfMWreCkk07iiUcf\njmpGLB06dIhZb03lyj4/jnrZybStwj6uAMqUKcO7S99nw6Z/8f6K5Wz4YH1Uy4/l76wwj6tkZUCK\nWaiPeKCGxAmyiJLajl2BYn0yMLOEv+Tv0vcWM2PaFM5u3ogbb7iehe/MY9jQGxIuA6Bu3TQAap9y\nCj2u7M37K5dHtfxYrMeC+XOof3oDatWqTdmyZbniyt6hnQscpljt8zBc2OI0Pt+5n13ffs/hbGfq\niq20b1yLbXsOMG1FpKdg1ebd5DicXKV8VDKvu2EIM995j39On0O16jVo2LhJVMrNL+z3SN20dOqm\npdOufeTb4it7/Zg1q1dFNSOW5syeQatz2nDKKadGvexk2lZhH1d5Va9enS4XduXtWTOjWm4sf2eF\neVxJYlND4jiYWQMz22hmLwHrgYFmtsTM3jezSWZWOZivvZktNrM1ZrbMzKoUUM5HZvaimX1sZi+b\nWTczW2Rmn5hZh2C+mmb2hpmtNbP3zOxsM2sADAfuNLPVZtYlKG9uMN8cM6sfLP+imY01s6XAw2ZW\n2cxeMLN1wbw/NrOhZvZYnrrdbGZ/Cn6+IZhvjZn9rYDt0cjMZpjZSjNbaGbNg+l9zWx9sNyCaG3/\n+x/4PR9s+py1H33K/730Ml0uvIhn//pStIqPWcb+/fv59ttvj/w8b+5szjyrRVQzYrEe6fXqs3LF\nUr777jvcnYXvzKNJs3AHqIchFtsqLFt37Sej0clULBfpFbjgrFP5+MtvmP7+VjqfeQoAjU6tQrky\nKeyK0jiJnTu+imR/8S/emvIGfa7pH5Vy84rFe+TUU08jLS2dTz7eCMCCd+bSrHl0BxDH0j8nvUKf\nvuGcfpIs2yoWx9XOHTvYu3cvAAcOHGDenLdp2qxZVDNi+TsrzOMqmcXDGAkzq25mrwWfNz80s/OC\nz5Wzg8+as82sxrFLKljCfztdgpoAg4BNwD+Abu6+38x+CdxlZqOBV4B+7r7czKoCBwoopzHQFxgK\nLAeuAzoDVwG/BnoDvwVWuXtvM7sYeMndW5vZWGCfuz8CYGZTgHHuPs7MhgJPBMsDpAPnu3u2mf0R\n+NrdWwXL1QCygHvN7OfungUMAX5qZi2AkcGyO82soJOsnwWGu/snZtYReBq4GLgPuMzdM82swBF5\nZjYMGAaRD6SlyY6v/s0N114DwOHD2fz4J/255NLLSrhWxdc2owM9e13Njy7oQJnUVFqd3ZqBg2+K\nes6ggdexcMF8du3cSZOG9Rj5m1EMGnJj1HPCcOvNA1myaCF7du2kQ8tG3HXPSKpXr8l999zF7l07\nGHJtH85qeTbjX5t6XOW//9lupiz/grm/vYzD2Tms+9deXpr/Ke7wxI0dWPhgd7IO53Dr89HrKbrp\nhv7s2b2Lsqll+f0jj0d90C3E7j3yh0ceY/hNN5B16BCnN2jIk888H9XyY3Xs7t+/nwXz5vDI409H\nvexcybCtYnFcbd/+JcNvHkJOdjY5OTn0+XFful/RM6oZsRKL40pC9Tgww92vMbNyQCUiny/nuPto\nM7sHuAf45fEUbu4evaqWEkFvwDx3P8PMegIvArkjDcsBS4DHgLHu3ukY5cx29ybB85eAme7+spk1\nBP4RNBhWAT9298+C+b4AWgB38cOGxE6gjrtnmVlZ4Et3r2VmLwb1HRfMtxLo7+6f5KvPc8BbwIfA\n39y9vZndBpzm7vfmm3cUsA8YC+wANuZ5uby7nxk0dBoBrwbrsuto27VN2wyftyi8S6DGSk5O+O+p\nlBjcLvP7rJzQM6pUiM13GYeyw1+Xbw8cDj2jzV3/DD1j3eNXh55RPjX8zvBY/GnL7QEK077vwz+u\nysTgXOtYbCuAg1nZoWeklgn/+M2JwQGcezW3sPzownNZ/f7K+DiRvwTUaHCWX3L/+FAzXh/abqW7\nZxT2uplVA1YDDT3PB34z2wh0dfcvzawOMN/dj6vLTD0Sxy/3UhVGpDFwbd4Xzayod9PJe55BTp7n\nOUR3/xTl0hrPE2mlfgS8UMRyU4C97t46/wvuPjzooegBrDSzdsdqTIiIiIhIkdQysxV5nj/r7s/m\neX4GkS97XzCzc4CVwO3Aqe7+ZTDPduC4B79ojMSJew/oZGaNAczsJDNrSuQb+jpm1j6YXuUEBjov\nBK4PyukK7HT3b4BvgbzjLhYDuScqXx8sV5DZwC25T3LPjXP3pUA9IqdXTQhengv0NbOTg3l/cGpT\nUI/NZtY3eN2CgxUza+TuS939PiIHcr1ir7mIiIhIggl7fETQkbjT3TPyPJ7NV41UoC3wjLu3IfKl\n8j15Zwh6Ko67C0wNiRPk7juAwcAEM1tL5LSm5u5+COgHPGlma4h8eK9gZnXN7K1ixowC2gXljyYy\nNgNgCtAnd7A1cBswJJhvIJFWZ0EeBGrkDoQGLsrz2qvAInffE6zfB8BDwDvBvI8WUN71wI3B6x8A\nube9HBMM6F5PpJGzppjrLSIiIiLHZyuwNfiiGOA1Ig2LfwenNBH8/9XxBujUpuPg7luAlnmezwXa\nFzDfcuDcfJP3AVcUUs7ggjLcfTf/GTSdt/yPgbPzTb64gPkG53u+j/80RvLrDPwp3/zjgHH5po3K\n8/NmoHsBueGfbC0iIiISh0r6Xg/uvt3MvjCzZu6+EbgE2BA8BvGfL6cnH2+GGhICRC4PBiwD1rj7\nnJKuj4iIiIicsNuAl4MrNn1G5KqcKcCrZnYj8Dnwk+MtXA0JAcDd9wJNS7oeIiIiIskgHi5Z5e6r\ngYKu7HRJNMrXGAkRERERESk29UiIiIiIiESZlfAYiVhQj4SIiIiIiBSbeiRERERERKLIgJTk75BQ\nQ0LiR4pBhbJlSroaEkimfVEhJfx1icX22vp8/2PPdIJqXPFI6Bl73ro79IycnOO+v1KRpcTgU0LV\nimVDz0gmlconx8eaWBy/qSEfvyV96VOJjeR4x4mIiIiIxAszjZEQEREREREpSKE9EmZW9WgLuvs3\n0a+OiIiIiEjiKwUdEkc9tekDwPnh/TRynztQP8R6iYiIiIhIHCu0IeHu9WJZERERERGRZKExEgEz\n629mvw5+TjezduFWS+ToZs2cwdktmtGieWPGPDxaGcpQRgwybundlhXPDmbls4O5tU/bI9N/1qsN\nq/9vCCufHcxDN10QtbxYbKvhw4ZyevqpZLRpFUr5uRJ5vysjPjNicexu/eILrvjRJWS0bkn7Nq14\n+qknQsuSxHTMhoSZPQVcBAwMJn0HjA2zUiJHk52dzR0jbmHylOmsWruBSRMn8OGGDcpQhjJCzDir\nQS2GXHE2XW4bT4fh47i8YyMa1q3OBefUo+d5jekw/CXaDXuRx15bEYW1iM22AhgwcDBvTJke9XLz\nSuT9roz4zYjFsZuamsrv/ziGFavXM3fBYp4d+zQffRj992Eyyr2PRJiPeFCUHonz3f2nwEEAd98N\nlAu1ViJHsXzZMho1aswZDRtSrlw5+vbrz9Qpk5WhDGWEmNG8Xk2Wf/QlB74/THaOs3DdF/Tu1IRh\nPVvzyCtLOZSVDcCOvd+dcBbEZlsBdO5yATVr1Ix6uXkl8n5XRvxmxOLYPa1OHVq3ifQ+VqlShWbN\nm7MtMzPUTEksRWlIZJlZCpEB1pjZyUBOqLUSOYpt2zJJT//PEJ60tHQyo/yLTRnKUMYPfbBlJ51a\nplGzSgUqlk+le/uGpNeuQuP0GnRqmc6CJ65n1iP9aNf0tBPOgthsq1hJ5P2ujPjNiLXPt2xh7erV\nZHToWNJVSRgW3EsirEc8KEpD4s/A60BtM/st8C7wx1BrJQUys65mdn4hrw0OTkMLK/tFM7umGPM3\nMLP1wc9dzWxqWHUTkfBt/GI3//vqMqaMvoY3f/9j1nz6Fdk5TmqZFGpWqcAFI17m18+9w/iRV5Z0\nVUUkyvbt28eAa/sy+pFHqVr1qHcHkFLmmHe2dveXzGwl0C2Y1Nfd14dbrZJnkaaeuXtc9L6YWSrQ\nFdgHLC7Z2pSsunXT2Lr1iyPPMzO3kpaWpgxlKCPkjHEz1jNuRuTX/2+HdCZz5z6a1qvJG4s+AWDF\nxu3k5Di1qlVk59cHTigrFtsqVhJ9vysjPjNiJSsriwH9r+En/a+jV++rS7o6CSU++gzCVdQ7W5cB\nsoBDxVgm4QTfom80s5eA9cBAM1tiZu+b2SQzqxzM197MFpvZGjNbZmZV8pVzkplNC15fb2b9gulb\nzOxhM1sXLNc4T+5cM1trZnPMrH4w/UUzG2tmS4FXgeHAnWa22sy6FLAK9cxsvpl9Ymb356nPG2a2\n0sw+MLNhwbQyQfnrg/rcGUxvZGYzgvkXmlnzPOV3M7MVZvaxmfXMU/eFwTZ6v7Aek2jKaN+eTZs+\nYcvmzRw6dIhJr0ykR8+rlKEMZYScUbt6JQDq1a5Cr85NeGXuh0xZvIkLz4ncVqhxWg3KlU054UYE\nxGZbxUqi73dlxGdGLLg7t/z0Jpo1P5Pbbr+zpKsjceiYPRJmdi9wHfBPIo2rv5vZy+7+h7ArV0Ka\nAIOATcA/gG7uvt/MfgncZWajgVeAfu6+PLgDeP6/mt2Bbe7eA8DMquV57Wt3b2VmNwCPAT2BJ4Fx\n7j7OzIYCTwC9g/nTiQx4zzazUcA+d3+kkLp3AFoSubLWcjOb5u4rgKHuvtvMKgbTXwcaAGnu3jKo\nY/WgjGeB4e7+iZl1BJ4GLg5eaxBkNALmBQ2hr4BL3f2gmTUBJgAZx9jGRwQNm2EA9eoX7R6Hqamp\n/Onxp7iyx2VkZ2czaPBQzmrRoqiRylCGMo7ThN9cRc2qFck6nM0dT87h6/3fM27mOv7y/7qz4tnB\nHMrK5qYx0bmKTCy2FcCggdexcMF8du3cSZOG9Rj5m1EMGnJjVDMSfb8rIz4zYnHsLlm8iAl/H0+L\nlq04v0Nk0PX9DzzIZd2viGpOMjKDlDgZxxAmc/ejz2C2EWjj7t8FzysBq9y9WQzqF1Nm1gCY5+5n\nBN+4vwhsDV4uBywh8uF/rLt3Oko5TYFZRBocU919YTB9C3Cxu39mZmWB7e5+spntBOq4e1Yw/Ut3\nr2VmLwb1GRcsP4pCGhJmNjgo+4bg+QPAbnd/LFiuTzBrA+AyYCOwAngLmBbUtxKwI3gtV3l3PzOo\nywJ3/2tQ/gJgBLAZeApoDWQDTd29UrAtp7p7SzPrCtzt7j0L22YA7dpl+KKl0bl0pIgcnxpXFPY9\nRfTseevu0DNyco7+ty0aUuLl+ouSdGJx/OYc4/Pfibrg/A68v3JFqX2T1G7Uwnv9/pVQM/6vf6uV\n7l7kL2/DcMweCeDLfPOlBtOS1f7gfwNmu/u1eV80s2Pe+cXdPzaztsAVwINmNsfdH8h9Oe+sxahP\nUeQvz4MP8d2A89z9OzObD1Rw9z1mdg6RRsVw4CfAHcBed29d1PKBO4F/A+cQOe3tYDHqKyIiIpKU\nSkGHROHjHczsT2b2KLAb+MDMnjez54B1wM5YVbAEvQd0yjOO4aSgp2EjUMfM2gfTqwQDoY8ws7rA\nd+4+HhgDtM3zcr88/y8Jfl4M9A9+vh5YWEidvgWqFPIawKVmVjM4hak3sAioBuwJGhHNgXODOtYC\nUtz9dWAk0NbdvwE2m1nfYB4LGhu5+ppZipk1AhoG26IakR6UHCI3LSxzlPqJiIiISJI4Wo9E7pWZ\nPiBy6kuu98KrTvxw9x3B6UITzKx8MHlk0NvQD3gy+MB+gMgg5KrA8+5+BdAKGGNmOUQGqf8sT9E1\nzGwt8D2Q29txG/CCmf2cyKlFQwqp1hTgNTPrFSxTA8hw9/uC15cRuVRvOjDe3VeY2TpguJl9SOSD\nf+7+SwsycxuTvwr+vx54xsxGAmWBicCa4LV/BRlViYyjOGhmTwOvB2M+ZlC8HhQRERGRpGSloEvi\nmGMkJHqCMRIZ7l4aenSKTWMkREqexkgUncZISFg0RiLx1W7UwvuMfjXUjOd+0jL+x0gEp7E8BJwF\nVMid7u5NQ6yXiIiIiEjCKgUdEkW6J8SLwAtEBh9fTuR+BuEOQ09S7t5AvREiIiIikgyK0pCo5O4z\nAdz9U3cfSaRBISIiIiIi+RhGioX7iAdFufzr98GA3E/NbDiQydGvHCQiIiIiIkmuKA2JO4GTiNx8\n7CEil/scGmalREREREQSlpWOMRLHbEi4+9Lgx2+J3CdARERERESOojRc/rXQhoSZ/ZOj3HnZ3a8O\npUYiIlKgWFwSMhaXZq1x0X3HnukE7Zn3QOgZusSshCUW+z2FcDN05JYOR+uReCpmtRARERERSSJF\nuaJRoiu0IeHuc2JZERERERERSRxFGWwtIiIiIiJFZJSOMRKloddFRERERESirMg9EmZW3t2/D7My\nIiIiIiLJoDRcK+GYPRJm1sHM1gGfBM/PMbMnQ6+ZyFHMmjmDs1s0o0Xzxox5eLQylKGMPIYPG8rp\n6aeS0aZVKOVDdNdj7D29+fzNX7Bi3C1HptWoUpGpjw5i3d9vZ+qjg6heucKR1+4e0IX1E25nzcsj\n6Nah8QllJ8v+gOQ5fpVR+jIkcRXl1KYngJ7ALgB3XwNcFGalRI4mOzubO0bcwuQp01m1dgOTJk7g\nww0blKEMZQQGDBzMG1OmR73cXNFej79NX0Wvu//2g2l3D+jC/JWf0eq6x5m/8jPuHtAFgOYNatP3\nkla0veEprrr7JR6/q+dxXyozWfYHJM/xq4zSl5HMUizcRzwoSkMixd0/zzctO4zKiBTF8mXLaNSo\nMWc0bEi5cuXo268/U6dMVoYylBHo3OUCataoGfVyc0V7PRat+Zzd3xz4wbSenZszfsYqAMbPWMWV\nXc48Mn3SnHUcysrm8y/38mnmbtqfmR4X61GYsPcHJM/xq4zSlyGJrSgNiS/MrAPgZlbGzO4APg65\nXiKF2rYtk/T0ekeep6Wlk5mZqQxlKCNGYrEep9Q4ie279gGwfdc+TqlxUiSrVlW2fvX1kfky5tB3\nPQAAIABJREFUv/qaurWrHFdGsuwPSJ7jVxmlLyNZmUWu2hTmIx4UpSHxM+AuoD7wb+DcYJrECTPr\nambnF/LaYDM74ZsLmtkoMwv/lrciIgUI/x7SIiJSXMe8apO7fwX0j0FdEoJFmoDm7jklXRcAM0sF\nugL7gMUlW5vYqFs3ja1bvzjyPDNzK2lpacpQhjJiJBbr8dWe/Zx2cmW279rHaSdXZsee/ZGsnd+Q\nfkq1I/OlnVKNbTu+Pa6MZNkfkDzHrzJKX0Yyi5dxDGEqylWbnjOzZ/M/YlG5eGFmDcxso5m9BKwH\nBprZEjN738wmmVnlYL72ZrbYzNaY2TIzq5KvnJPMbFrw+noz6xdM32JmD5vZumC5xnly55rZWjOb\nY2b1g+kvmtlYM1sKvAoMB+40s9Vm1qWAVahrZjPM7BMzezhPfa4NMteb2R/zTO8erNsaM/uvO5yb\n2c1mNt3MKppZo6DslWa20Myam1kVM9tsZmWD+avmfX6iMtq3Z9OmT9iyeTOHDh1i0isT6dHzqmgU\nrQxlJHxGLMRiPaYt+ogB3dsAMKB7G6a++1Fk+rsf0feSVpQrW4bT61SncXpNln+49bgykmV/QPIc\nv8oofRmS2IpyH4m38/xcAegDfFHIvMmsCTAI2AT8A+jm7vvN7JfAXWY2GngF6Ofuy82sKnAgXxnd\ngW3u3gPAzKrlee1rd29lZjcAjxG5UtaTwDh3H2dmQ4lcQat3MH86cL67Z5vZKGCfuz9SSN1bA22A\n74GNweV7s4E/Au2APcAsM+sNLAKeAy5w981m9oMRgmZ2K3Ap0Nvdvw8alcPd/RMz6wg87e4Xm9l8\noAfwBpEerX+4e1b+ipnZMGAYQL369Qup/g+lpqbyp8ef4soel5Gdnc2gwUM5q0WLIi1bVMpQRqJm\nAAwaeB0LF8xn186dNGlYj5G/GcWgITdGrfxor8e4+6+hS5szqFWtEpte/3/87q/zeGT8QsY/0I9B\nPdryr3/vZcB9rwLw4ZYdvD53Pav+dhuHs3O449Fp5OQc34lPybI/IHmOX2WUvoxkFifDGEJl7sX7\nBWxmKcC77l7gOfnJyMwaAPPc/Qwz6wm8COR+BVYOWELkw/9Yd+90lHKaArOINDimuvvCYPoW4GJ3\n/yz41n67u59sZjuBOu6eFUz/0t1rmdmLQX3GBcuPopCGhJkNBjq5+83B8+nAQ8DJwI/d/YZg+o1A\nC2Ae0N/dr89XzijgaiKNyN5BnSoDO4CNeWYt7+5nmlkn4Bfu3svMlgA3u/v6Qjcy0K5dhi9auuJo\ns4iUasf7gbk4jvdSqsVR46L7Qs/YM++B0DOSZX+IhKFTxwxWrlxRag/g05q09Bseez3UjDE9m690\n94xQQ46hyHe2zuMM4NRoVyQB7A/+N2C2u1+b90UzO+adhtz9YzNrC1wBPGhmc9w9969d3r9IRfnr\ntP/YsxyR947k2RzffgdYR6R3Ix3YTOTUuL3u3jr/jO6+KDg1qytQ5liNCBEREZFkYUBKKeiSKMoY\niT1mtjt47AVmA78Kv2px6z2gU55xDCcFPQ0bgTpm1j6YXiUYCH2EmdUFvnP38cAYoG2el/vl+X9J\n8PNi/jPQ/XpgYSF1+hYo7vUPlwEXmlktMysDXAu8E6zfBWZ2RlDnvKc2rQJ+CrxpZnXd/Rtgs5n1\nDeY1Mzsnz/wvAX8HXihm3UREREQkzh31m+ngCkXnALkXDc7x4p4LlWTcfUdwutAEMysfTB4Z9Db0\nA540s4pExkd0C8ZKPO/uVwCtgDFmlgNk8cPL6NYws7VEeg9yeztuA14ws58TOYVoSCHVmgK8Zma9\ngmVqABnuXuj5A+7+pZndQ+RUJgOmuftkODJu4R/BaWxfERkTkbvcuxa5DOw0M7uUSAPnGTMbCZQF\nJgJrgtlfBh4EJhRWDxEREZFkVJR7LCS6Y46RMLP17t4yRvUplYIxEhnuvrOk6xJNZnYN0MvdBxZl\nfo2REDm6ZDknX2Mkik5jJCRRlfYxEnWatPQhj/8j1Iw/9GiWEGMkVptZG3dfFXptJGkEV4a6nMh4\nEBEREZFSpRQMkSi8IWFmqe5+mMhlQ5eb2adEBvga4O7etrBlpXjcvUFJ1yHa3P22kq6DiIiIiITn\naD0Sy4gMBtadR0REREREisjMSsVVm47WkDAAd/80RnUREREREZEEcbSGRG0zu6uwF9390RDqIyIi\nIiKS8EpBh8RRGxJlgMoEPRMiIiIiIiK5jtaQ+DLPXZdFRKSEJculQGNxadYa7W8NPWPX0idDz5Di\n0SV5JZ6UhkPlaPfKKAWrLyIiIiIix+NoPRKXxKwWIiIiIiJJwqBUXLWp0B4Jd98dy4qIiIiIiEji\nKMqdrUVEREREpBhKQYfEUcdIiIiIiIiIFEgNCUlIs2bO4OwWzWjRvDFjHh6tDGUoQxlHNfb+6/l8\nzh9YMenXR6bVqFqJqc/cyrrJ9zH1mVupXqUiAPXr1GT3kkd5b+I9vDfxHp64t/8JZQ8fNpTT008l\no02rEyrnWBJtn5RUhvZH/GUkJYtctSnMRzxQQ0ISTnZ2NneMuIXJU6azau0GJk2cwIcbNihDGcpQ\nRqH+NuU9et3y5x9Mu3vIpcxftpFWvR5g/rKN3D3kR0de+2zrTs7tP5pz+49mxEMTjzsXYMDAwbwx\nZfoJlXEsibhPSipD+yO+MiSxqSEhCWf5smU0atSYMxo2pFy5cvTt15+pUyYrQxnKUEahFr3/Kbu/\n/u4H03p2PZvxU5YCMH7KUq686OwTqnNhOne5gJo1aoZSdq5E3CcllaH9EV8ZycxC/hcP1JCQhLNt\nWybp6fWOPE9LSyczM1MZylCGMorllJOrsH3nNwBs3/kNp5xc5chrDdJO5r2J9zDr+dvp1KZRVHPD\nkCz7JBYZsZAs2ypZ9oeER1dtKmXMLAO4wd1HxCivK3DI3RfHIk9E5Hh5cFPk7Tu/oenl97H76/20\nObMerz46jLbXPMS3+w+WbAVFJGFE7iNR0rUIn3okShl3X3G8jQgzO56GZ1fg/OPJK0zdumls3frF\nkeeZmVtJS0uLZoQylKGMUpDx1a5vOa1WVQBOq1WVHbu/BeBQ1mF2f70fgFUffsFnW3fS5PRTopod\nbcmyT2KREQvJsq2SZX9IeNSQSBJmdpKZTTOzNWa23sz6mVl7M1scTFtmZlXMrKuZTS2kjF+a2bpg\n/tHBtPlm9piZrQDuNbPNZlY2eK1q7vNgvsfNbHWQ38HMGgDDgTuD6V2isa4Z7duzadMnbNm8mUOH\nDjHplYn06HlVNIpWhjKUUYoypr2zjgFXdgRgwJUdmTp/LQC1alQmJfgqsUHayTSuX5vNW3dGNTva\nkmWfxCIjFpJlWyXL/igppeGqTTq1KXl0B7a5ew8AM6sGrAL6uftyM6sKHChsYTO7HOgFdHT378ws\n70i0cu6eEczXAOgBvAH0B/7h7lkWuetKJXdvbWYXAH9195ZmNhbY5+6PRGtFU1NT+dPjT3Flj8vI\nzs5m0OChnNWiRbSKV4YylJGEGeP+MJgu7ZpQq3plNs34Hb8b+xaPvDCb8X8cyqDe5/GvL3cz4Bd/\nBaBz28b85mc9yDqcTU6Oc9tDE9nzzXfHSCjcoIHXsXDBfHbt3EmThvUY+ZtRDBpy43GXV5BE3Ccl\nlaH9EV8ZktjMc08KlYRmZk2BWcArwFRgLzDW3Tvlm68rcLe798w3/X+Bj9z9uXzT5wP3u/s7wfNO\nwC/cvZeZLQFudvf1wXwPuPvcYL5/AWcDd3CUhoSZDQOGAdSrX7/dx59+fvwbQUQkUKP9raFn7Fr6\nZOgZKfHytWOCyMkJ/zON9knRdOqYwcqVK0rtxkpv1spv/0u4V7j6xUWNVuZ+0VtSdGpTknD3j4G2\nwDrgQeDqKBa/P0/OIqBB0CAp4+7r81Yjf7WOVbC7P+vuGe6eUbtW7ahUVkRERETCp4ZEkjCzusB3\n7j4eGAN0BOqYWfvg9SrHGCw9GxhiZpWC+Y92ke2XgL8DL+Sb3i9YtjPwtbt/DXwLVEFERESklMi9\nalOyj5FQQyJ5tAKWmdlq4H7gPiIf7J80szVEGgoV8i5gZhlm9jyAu88A3gRWBGXcfZSsl4EawIR8\n0w+a2SpgLJB7wukUoE80B1uLiIiISMnTYOsk4e4zgZkFvHRuvufzgwfuvgK4KU8Zo4HR+crtWkCZ\nnYHX3H1vvunj3f2OfMt/TGSshIiIiEjpYGBx0msQJjUkpFjM7EngcuCKkq6LiIiISLxKKQUtCTUk\npFjc/bZCpneNcVVEREREpASpISEiIiIiEkW5g62TnQZbi4iIiIhIsalHQkREREQkykrBEAn1SIiI\niIiISPGpR0JERJLOrqVPhp5xcq/HQ8/YM+WOY88kR6SUhpPSJUEYKST/8ageCRERERERKTb1SIiI\niIiIRJGhMRIiIiIiIiIFUo+EiIiIiEg0me4jISIiIiIiUiA1JCQhzZo5g7NbNKNF88aMeXi0MpSh\nDGUcl+HDhnJ6+qlktGkV1XJv6dWaFc8MYOXYgdzauw0A915/Lp/+7Sbee+p63nvqei5r3yCqmcmy\nT5RR+jKSVYpZqI94oIaEJJzs7GzuGHELk6dMZ9XaDUyaOIEPN2xQhjKUoYxiGzBwMG9MmR7VMs86\n/WSGdG9Jlzsm0uF/xnN5hzNoWKcaAE++8T7n3voy5976MjOXb4laZrLsE2WUvgwJl5mVMbNVZjY1\neF7TzGab2SfB/zVOpHw1JCThLF+2jEaNGnNGw4aUK1eOvv36M3XKZGUoQxnKKLbOXS6gZo2aUS2z\neb2aLN+4nQPfHyY7x1m4biu9OzWOakZ+ybJPlFH6MpJV7lWbwnwU0e3Ah3me3wPMcfcmwJzg+XFT\nQ0ISzrZtmaSn1zvyPC0tnczMTGUoQxnKiAsffL6TTi3SqFmlAhXLp9K9/Rmk164CwM+ubM2yp69n\n7J2XUr1y+ahlJss+UUbpy5DwmFk60AN4Ps/kXsC44OdxQO8TyVBDogSZWYaZPVHMZfYdZ1ZvMzvr\neJYtpLy6ZvZatMoTEUkWG7/Yw/9OWsGUh/rw5u96s+azHWTnOM9NW8uZQ1+g4y0vs333fkbffEFJ\nV1VEQhQHYyQeA34B5OSZdqq7fxn8vB049YTW8UQWlhPj7ivcfUSM4noDBTYkzKzYlwF2923ufs0J\n1+o41K2bxtatXxx5npm5lbS0NGUoQxnKiBvjZn1ApxETuPQXr7H324N8snUPX+39jpwcxx3+On09\nGU1P6O/3DyTLPlFG6cuQE1LLzFbkeQzLfcHMegJfufvKwhZ2dwf8RCqghkQIzOwkM5tmZmvMbL2Z\n9TOz9ma2OJi2zMyqmFnX3MEvBZTxczNbbmZrzey3xZnHzG4Ipq0xs7+Z2fnAVcAYM1ttZo3MbL6Z\nPWZmK4DbzayBmc0NlptjZvWDsl40syeCun9mZtcE0xuY2frg5zJm9kiwrmvN7LZg+mgz2xBMeyRa\n2zejfXs2bfqELZs3c+jQISa9MpEePa+KVvHKUIYySklGmGpXqwhAvdpV6NWpMa/M38hpNSodeb3X\n+Y3Y8PmuqOUlyz5RRunLSGYxGCOx090z8jyezRPfCbjKzLYAE4GLzWw88G8zqxOpn9UBvjqRddQN\n6cLRHdjm7j0AzKwasAro5+7LzawqcKCwhc3sR0AToAOR8TpvmtkF7r7gWPMAu4CRwPnuvtPMarr7\nbjN7E5jq7q8FywOUc/eM4PkUYJy7jzOzocAT/Oe8uTpAZ6A58CaQ/5SmYUADoLW7Hw6uCHAy0Ado\n7u5uZtWLuxELk5qayp8ef4ore1xGdnY2gwYP5awWLaJVvDKUoYxSkgEwaOB1LFwwn107d9KkYT1G\n/mYUg4bceMLlThjZk5pVK5B1OIc7np7H1/u/59GfXcbZDWvjOJ//+xtue2JOFNYgIln2iTJKX4aE\nw91/BfwKwMy6Ane7+wAzGwMMAkYH/5/Q6HmL9GpINJlZU2AW8AowFdgLjHX3Tvnm60pkx/bMN/0R\n4JpgOYDKwB/c/f/MbJ+7Vy5sHqAScJq735uvzBf5YUNiPnC/u78TPN8J1HH3LDMrC3zp7rWC5Wa7\n+8vBfN+6exUzaxCU19LMXg/Wb3aevFRgZfCYGsx7qIBtNYxIQ4R69eu3+/jTz4++cUVEiiAnJ/y/\nbSf3ejz0jD1T7gg9QyQMnTpmsHLlivi42UEJOOPMs/3+lwo86SRqhnQ4fWXuF8JHk/fzZvBF76tA\nfeBz4Cfuvvt466AeiRC4+8dm1ha4AngQmFvMIoxIw+EvxZ0n97SiItpfxPm+z5d7TEHPRAfgEiIN\nnluBiwuY71ngWYB27TLUqhURERGJInefD8wPft5F5LNZVGiMRAjMrC7wnbuPB8YAHYE6ZtY+eL3K\nMQY4zwSGmlnlYP40MzuliPPMBfoGLU7MLPcC6d8CVY6SuRjoH/x8PbCwaGsLwGzgp7nrFJzaVBmo\n5u5vAXcC5xSjPBEREZHEZZHTyMN8xAP1SISjFZGBzTlAFvAzIt/kP2lmFYmMj+iWdwEzywCGu/tN\n7j7LzM4ElgQHyj5gAHkGxBQ2j7t/YGYPAe+YWTaRsRmDiQy0ec7MRhDpIcjvNuAFM/s5sAMYUoz1\nfR5oCqw1syzgOeB1YLKZVQjW/a5ilCciIiIicU5jJCRutGuX4YuWrijpaohIEtAYCZGSVerHSJx1\ntv/2pWmhZgxqX79IYyTCpFObRERERESk2HRqk4iIiIhIFBkU9e7TCU09EiIiIiIiUmzqkRARERER\nibLk749Qj4SIiIiIiBwH9UiIiIiIiERZKRgioYaEiIj8Rywum5qSEv5f11hkxOLSrI1G/DP0jE+f\n6BN6hhRPLN6Hh0POyAm1dIkXakiIiIiIiERV/Nx9OkwaIyEiIiIiIsWmHgkRERERkSgySse39aVh\nHUVEREREJMrUIyEiIiIiEmUaIyESp2bNnMHZLZrRonljxjw8WhnKUEaMM4YPG8rp6aeS0aZVKOVD\n8myrsHJuvrgRc0dewpyRl/DnIRmUT01hZJ+WvHNfN2bfezHPD+tI1Yplo5KVK1n2STJkxOI9ePDg\nQS7qfC6dOrShY9tW/P53o0LLksSkhoQknOzsbO4YcQuTp0xn1doNTJo4gQ83bFCGMpQRowyAAQMH\n88aU6VEvN1cybaswck6rVoGhXRtxxR/nccmDcyiTYvTKSGfBR19x8YNzuPShuXz21T5uvaxplNYi\nefZJsmSE/R4EKF++PFNmvM2iZat4d+n7vD1rJsuXvhdqZjKxkB/xQA0JSTjLly2jUaPGnNGwIeXK\nlaNvv/5MnTJZGcpQRowyADp3uYCaNWpGvdxcybStwspJLWNUKFuGMilGxXKpbP/6IAs+/Irs4P4A\n72/eTZ3qFU84J1ey7JNkyQj7PQiRU3MqV64MQFZWFlmHs0rF6TpSdGpISMLZti2T9PR6R56npaWT\nmZmpDGUoI0YZsZBM2yqMnO1fH2Ts25tY9mB3Vv3hcr45kMWCD7/6wTz9zz+deRv+fUI5eSXLPkmW\njFjJzs6mc8e2NK5/Ghdd3I2MDh1LukqJwSINsTAf8UANiVLOzDLM7IkY5Gwxs1ph54iIlAbVKpbl\nsrPrcO59M2n7q+lUKl+Gqzv854PriO5NOZzt/GPZFyVYS0kGZcqU4d2l77Nh0794f8VyNnywvqSr\nJHFEDYlSzt1XuPuIkq5HcdStm8bWrf/545iZuZW0tDRlKEMZMcqIhWTaVmHkdGlem3/t2s/ufYc4\nnONMX72NjIaR01x+cm59urWsw60vrDihjPySZZ8kS0asVa9enS4XduXtWTNLuioJIfc+EmE+4kG8\n1EOizMxOMrNpZrbGzNabWT8za29mi4Npy8ysipl1NbOpBSzf1czeMbPJZvaZmY02s+uD5daZWaNg\nvtpm9rqZLQ8enYLpJ5vZLDP7wMyeJ4rjgjLat2fTpk/Ysnkzhw4dYtIrE+nR86poFa8MZSgjDiTT\ntgojJ3PPAdo2qEmFsmUA6NzsFD7Z/i1dzzqFn13ahMFjl3AwKzsa1T8iWfZJsmTEws4dO9i7dy8A\nBw4cYN6ct2narFkJ10riie4jkby6A9vcvQeAmVUDVgH93H25mVUFDhyjjHOAM4HdwGfA8+7ewcxu\nB24D7gAeB/7k7u+aWX1gZrDM/cC77v6AmfUAbozWiqWmpvKnx5/iyh6XkZ2dzaDBQzmrRYtoFa8M\nZSijCAYNvI6FC+aza+dOmjSsx8jfjGLQkKi9zZNqW4WRs2rLHqatymTmry7icI7zwRd7efndLcwd\neQnly6Yw8bZOALy/ZQ/3TFgdjdVImn2SLBlhvwcBtm//kuE3DyEnO5ucnBz6/Lgv3a/oGdWMZBYv\n4xjCZO5e0nWQEJhZU2AW8AowFdgLjHX3Tvnm6wrc7e49C5h+r7tfGjxfAPzK3ReZ2cXACHfvbWZf\nAdvyLFobaAa8C1zt7p8Fy+8Gmrr7znw5w4BhAPXq12/38aefR2P1ReQ45eSE/zchJSX5/7hGS6MR\n/ww949Mn+oSeIcUTi/fh4ZAzLuzUgVUrV5TaN3vjFuf4IxPCPQ2szzl1Vrp7Rqghx6AeiSTl7h+b\nWVvgCuBBYO5xFPN9np9z8jzP4T/HTgpwrrsfzLtgUVvh7v4s8CxAu3YZatWKiIiIJAiNkUhSZlYX\n+M7dxwNjgI5AHTNrH7xexcyi0ZCcReQ0p9zc1sGPC4DrgmmXAzWikCUiIiKSEMzCfcQD9Ugkr1bA\nGDPLAbKAnxEZ8PykmVUkMj6iW94FzCwDGO7uNxUjZwTwZzNbS+R4WgAMB34LTDCzD4DFwL9OcH1E\nREREJI6oIZGk3H0mkYHP+Z2b7/n84IG7rwBuCn4+Mj143jXPz3mX2Qn0KyB/F/Cj46u9iIiISOKK\nXP41TroNQqRTm0REREREpNjUIyEiIiIiEmXxMo4hTOqREBERERGRYlOPhIiIiIhIVBmmMRIiIiIi\nIiL/TT0SIiIiIiJRVhrGSKghISIiR6SklIK/fFGSk+OhZ3z6RJ/QMxr87LXQM7Y8c03oGckkFu/D\nciFn6JSX0kENCRERERGRKNJ9JERERERERAqhHgkRERERkWiy0jFGQj0SIiIiIiJSbOqREBERERGJ\nMvVIiMSpWTNncHaLZrRo3pgxD49WhjKUoYy4zRg+bCinp59KRptWoZSfK4x1GdatCe/89lLmj7qU\nZ27uQPnU/3xsGH5pE7Y/dw01K5eLSlauZNnvypDSQA0JSTjZ2dncMeIWJk+Zzqq1G5g0cQIfbtig\nDGUoQxlxlwEwYOBg3pgyPerl5hXGupxWvQI3XdKYyx6cQ9dRsymTYvTuUA+AujUqcmGLU9m6a380\nqn9Esux3ZQjk3ts6vH/xQA0JSTjLly2jUaPGnNGwIeXKlaNvv/5MnTJZGcpQhjLiLgOgc5cLqFmj\nZtTLzSusdSmTYlQoW4YyKUbFcqls33sQgAf6ncPvXluHR/lWGsmy35UhpYUaEpJwtm3LJD293pHn\naWnpZGZmKkMZylBG3GXEShjrsn3vQZ6Z9TEr/9iDtY/05JsDWbyz4d9cdk4dvtxzgA1bvz7Rav+X\nZNnvyhADUizcRzxQQyLJmVmGmT1R0vUQEZHEUq1SWbq3rkuHX73FOT+fSqVyZeh7Xn1uv+JMHn7z\ng5KunojEAV21Kcm5+wpgRUnXI5rq1k1j69YvjjzPzNxKWlqaMpShDGXEXUashLEuF5x5Cv/auZ9d\n+w4B8NaqTPqf34D6tSox975LAahToyKzRnbj8t/PYcc3359QHiTPflfG/2fvvOOsKs7///7QUVBA\nEGXpRZAmwgIqiqhYQbFjL2D82n4mGpOYqBGNJpZEYjfGaDTWGCvYUCMWLDQRFAtG0LDYC0qCgsvz\n+2Pmuoebu429d/fe3ee9r/Pac+acM585c+aeM888M3McIG/GMeQS90gUKJI2lvSIpNckvS5poqTh\nkl6MYbMltZY0RtL0DOdvKek5SQvi+TvF8FWSpkp6Q9LTkjrE8B9JmhPjvk/SRjG8o6QHYvhrknaI\n4UfFNCyQ9CdJjbN17cXDh/Puu0tYtnQpa9as4d577mbc+P2yFb1ruIZruEbBkYtrWf7Faob1bEfL\nZuHxvVO/zXn01RIG/nQ6w3/5GMN/+RgffrmaPS56KitGBNSf++4aTkPBPRKFy17ACjMbByBpU+BV\nYKKZzZG0CbC6gvOPAJ4ws4tjJX+jGL4xMNfMzpD0a+B84DTgfjP7c9S6CJgMXA1cBTxrZgfEeFpJ\n2hqYCIwys7WSrgOOBG7LxoU3adKEqVdew77j9qS0tJRjj5tE/wEDshG1a7iGa7hGVjUAjj36CJ5/\nbiaff/YZfXp24dzzpnDs8ZOzqpGLa3l16RdMn1fCjHN3o3SdseiDr/jbc0uzlOLM1Jf77hoONIzv\nSMiyPeWCUytI2gqYAdwDTAe+Am4ws1Fpx40BzjKz8Wnho4GbgduBB81sQQwvBZqb2feSehIMiCGS\ndgYuAtoArQhGyEmSPgU6m9l3ibhPA34FfBKDWgJ3mdmUDNdxInAiQJeuXYe986/3a5ArjuM4tce6\ndbl/fzaqhRGV3U/+R841ll1/cM41nPxi1Mhi5s2b2wCq0pnpO3CI3XDf0znV2LVf+3lmVpxTkUrw\nrk0Fipm9AwwFFhEq+AdW8/zngNFACfBXSceUd2j8/1fgNDMbBFwAtKggegG3mtmQuPTNZETEdNxo\nZsVmVtyhfYfqXILjOI7jOE7e4t+RcPIWSZ2A/5rZ7cDlwEhgS0nD4/7WksrtuiapG/Bx7K50E8Eo\ngVAmUk1HRwAvxPXWwIeSmhK6KaV4Gjg5xtk4drF6GjhY0uYxvF3UcxzHcRzHceoJPkaicBkEXC5p\nHbCWUJkXcLWkloTxEWOTJ0gqBk4ysxOAMcDPJK0FVgEpj8R/gBGSziV0TZoYw88DXgHBwycQAAAg\nAElEQVQ+jf9bx/AfAzdKmgyUAieb2Uvx/BmSGsX0nQp4vyXHcRzHceo9qe9I1HfckChQzOwJ4IkM\nu7ZL254Zl9RUsCfE9VuBW8uJ+8wMYdcD12cI/xiYkCH8HsL4DcdxHMdxHKce4oaE4ziO4ziO42SV\n/BnHkEt8jISzHmbWqq7T4DiO4ziO4+Q/7pFwHMdxHMdxnGyihvEdCfdIOI7jOI7jOI5Tbdwj4TiO\n4ziO4zhZpgE4JNwj4TiO4ziO4zhO9XGPhJM3rDP4dm1pTjVaNG2c0/gdx2k4NKonk8Qvu/7gyg+q\nIe0OuznnGgBf3D2pVnTqA2u+X5fT+HMbe/4TviNRP54RFeEeCcdxHMdxHMdxqo17JBzHcRzHcRwn\ny9R/f4R7JBzHcRzHcRzH2QDcI+E4juM4juM42aYBuCTcI+E4juM4juM4TrVxj4TjOI7jOI7jZBk1\nAJeEeyScgqW0tJTR2xUz8cD9chL/jCceZ/CAvgzo15vLL7vENVzDNVwjr3UKWeOUffoz54oDmDv1\nAE4d1x+AA7bvztypB7Dq78cztNdmWdOCws6r2tT49ttv2WXH7Rg1YltGDh3Eb38zJesaTmHjhoRT\nsNxw7VVs1a9fTuIuLS3lJ6efykPTHuPVhYu59+67eHPxYtdwDddwjbzUKWSN/l3acPzYvow++2FG\n/vRB9h7WlZ5btGbxB19y+OVP88KbH2Uh9WUUcl7Vtkbz5s2Z9vhTzJr9Ki+8Mp+nZjzBnFdezqpG\nfUbK7ZIPuCHhFCQly5cz4/FHOea43Hx8aM7s2fTq1ZsePXvSrFkzDpl4GNOnPeQaruEarpGXOoWs\n0bdzG+Yu+ZTVa0opXWe8sPhDJozsztslK1my4usspHx9CjmvaltDEq1atQJg7dq1rP1+LcqXGqyT\nF7gh4RQkv/r5mVxw0SU0apSbIrxiRQmdO3f5YbuoqDMlJSWu4Rqu4Rp5qVPIGos/+JIdtu5Iu1bN\nadmsMXtu24XOm21c43jLo5DzqrY1IHg+dhw5lN5dt2CXXcdSPGJk1jXqK8rxkg+4IZElJM2UVBzX\nH5XUppLjL5Q0tjbSU8lxnST9o4L9bSSdUtXja4PHH51O+w6bM2TosLpMhuM4jpMF3i5ZyRUPLmTa\neXvy0Ll7snDZ55Sus7pOlhNp3LgxL7wyn8XvfsD8uXNY/MbrdZ2kwqEBWBJuSFQRBaqUX2a2j5l9\nVckxvzazp7KTug1DUhMzW2FmB1dwWBvgB0OiCsfnnFdefpHHH5nG4H69mHzMkTz/7DOcOOmYrGp0\n6lTE8uX//mG7pGQ5RUVFruEaruEaealT6Bq3/nMJo37xMHv8+lG++s8a3v1wZVbizUSh51VtaiRp\n06YNO+08hqdmPJEzDafwqHeGhKSNJT0i6TVJr0uaKGk3Sa9KWiTpZknN47HDJb0Yj50tqXVaXN0l\nvS3pNuB1oIukPSS9JGm+pHsltcqQhmWS2sf182IcL0i6S9JZMfyvkg6O6+Wlb5mkC6LWIkkZRxZL\n+kXc/5qk5LQNh8TrekfSTvHY4yQ9LOmfwNPxGl+P+wbE4xdIWiipD3AJ0CuGXZ52fHdJz8f0zZe0\nQwwfEz0i/5D0lqQ7lMVOledf+FveePd9Fr71L/5y2x3stPMu3HjzbdmKHoDi4cN5990lLFu6lDVr\n1nDvPXczbnx2Z4dyDddwjfqvUVs6ha7RYZMWAHRuvzH7jezGPc+/l5V4M1HoeVWbGp99+ilffRXa\nRVevXs0zTz/FVn37ZlWjvhKcBrn9ywfq43ck9gJWmNk4AEmbEoyA3czsnWgUnCzpOuAeYKKZzZG0\nCbA6Q3x9gGPN7OVoHJwLjDWz/0j6BXAmcGGmhEgaDhwEbAM0BeYD89KOaQH8NT19wB/jIZ+Z2dDY\nvegs4IS08/cGJgAjzey/ktoldjcxsxGS9gHOB1JdqYYCg83sC0ndE8efBFxpZndIagY0Bs4GBprZ\nkKiXPP4TYHcz+zYaHXcBqe5U2wIDgBXALGAU8EKGPDoROBGgc5eu6bvrjCZNmjD1ymvYd9yelJaW\ncuxxk+g/YIBruIZruEZe6hS6xp0/25V2rZqzttQ446aXWPnfNew3oht/mLwd7TdpwX2/3IOFyz5n\nwkUzaqxV6HlVmxofffQhJ/3oeNaVlrJu3ToOOOgQ9tpnfFY1nMJGZvWrH6KkrYAZBCNhOvA1cLWZ\njY77dwNOJVSsbzCzURXE1R14xsx6xO3xhEr/8nhIM+AlM5ssaSZwlpnNlbSMUKE+CmhrZufH868g\nGDm/l/TXmL4lmdJnZgfGeEaZWYmkkcDFZrbeuApJfwDeMrM/p4XPBM4xs1mSOgKzzKy3pOOAnc3s\n+MQ1TjezgZKOAM4BbgPuN7Mlyf0Zjt8UuAYYApQCW5nZRpLGRO3d4znXR/3by8trgG2HFtszs16p\n6JAa06Jp45zG7ziO4/wv7Q67uVZ0vrg7NzP51UfWfL8up/HvPGoEr86bmx/N5nVA/8Hb2t8efjan\nGsU9Np1nZpWOh80l9c4jEVv1hwL7ABcB/6xhlP9JrAt40swOr2Gc1eG7+L+U6t+v8s79T4ZjMbM7\nJb0CjAMelfR/QEX+5TOAjwkel0bAtxm0M+k7juM4juM4BU59HCPRCfhvbP2+HNge6C6pdzzkaOBZ\n4G1gy9j9CEmtJVVW2X0ZGJWKK47H2KqC42cB+0pqEcdSZPIHvl1O+qrKk8DxkjaKaWpXyfHlIqkn\n8J6ZXQU8BAwGvgFal3PKpsCHZrYuptub+x3HcRzHcWgQkzbVP0MCGATMlrSA0H3pXOB44F5Ji4B1\nhC5Na4CJwNWSXiNUyFsoTG/6aKaIzexT4DjgLkkLgZeAcj+tbGZzgIeBhcBjwCJgZdox32ZKX0UX\nKKlY0k3x/Mejxtx4zWdVdG4lHAq8HuMZCNxmZp8DsxQGrl+edvx1wLEx//pRjqfDcRzHcRzHqX/U\nuzES+YakVma2KnoMngNONLP5dZ2ufMTHSDiO49RPfIxE/uFjJHJL/8Hb2u3TcjtGYlh3HyPRELhR\nUn+gBXCrGxGO4ziO4zhOfcANiRxjZkfUdRocx3Ecx3Gc2iR/vvWQS+rjGAnHcRzHcRzHcXKMeyQc\nx3Ecx3EcJ8uo/jsk3CPhOI7jOI7jOE71cY+E4ziO4ziO42SRfPrWQy5xQ8LJGxrJp2d1HKdwWLcu\n99OnN2pUP6oitTUt6/ALnsq5xpzzx+ZcozZokuOyVT9KrlMZbkg4juM4juM4TrZpANaUj5FwHMdx\nHMdxnHqGpC6SnpG0WNIbkn4cw9tJelLSkvi/7YZquCHhOI7jOI7jOFlGOf6rAt8DPzWz/sB2wKnx\nI8lnA0+bWR/g6bi9Qbgh4TiO4ziO4zj1DDP70Mzmx/VvgDeBImACcGs87FZg/w3V8DESjuM4juM4\njpNlauE7Eu0lzU1s32hmN2ZOi7oD2wKvAB3N7MO46yOg44YmwD0STkEy44nHGTygLwP69ebyyy5x\nDddwDdfIW42TTpxEt84dKd52UE7iT1Ff8isXGt3bb8S9p4z8YXnpnDEctX2XH/Yfs0NXFv1mLG02\napoVPSjcvEpSW2XX2WA+M7PixFKeEdEKuA/4iZl9ndxnZgZs8BR0bkg4BUdpaSk/Of1UHpr2GK8u\nXMy9d9/Fm4sXu4ZruIZr5J0GwFFHH8eD0x7LerxJ6kt+5Upj2Wf/5ZDrXuGQ615h4vWv8O3aUp5e\n/CkAHTdpzg69N2PFV6trrJOikPMqSW2U3fqMcrxUKQ1SU4IRcYeZ3R+DP5a0Zdy/JfDJhl6jGxJO\nwTFn9mx69epNj549adasGYdMPIzp0x5yDddwDdfIOw2AHXcaTbu27bIeb5L6kl+1oTGyZzv+/cVq\nPlz5LQA/32crrpixBMviZ0HqS17VRtl1cockAX8B3jSzKxK7HgaOjevHAhtccNyQcAqOFStK6Ny5\nzCVdVNSZkpIS13AN13CNvNOoLepLftWGxt6DtuCxRR8BsEu/Dnzy9Xe889GqrGrUl7xyakCu3RFV\nc0mMAo4GdpW0IC77AJcAu0taAoyN2xuEGxIVIKmNpFOqcNyq+H+MpOk5SMcySe3j+ovxf3dJRySO\nKZZ0Vba1HcdxHKe+0KSxGNOvPTNe/4QWTRtxwujuXPv0v+o6WY6TE8zsBTOTmQ02syFxedTMPjez\n3cysj5mNNbMvNlTDDYmKaQNUakjUJma2Q1ztDhyRCJ9rZqfXSaJqmU6dili+/N8/bJeULKeoqMg1\nXMM1XCPvNGqL+pJfudbYqU973vzwGz7/zxq6tGtJUduW/OPU7Xj8zFF03KQ5fz95JJu1alZjnfqQ\nV07NyYPvSOQcNyQq5hKgV3QFTZX0tKT5khZJmlDRiZKGS3pVUq+08DGSnpP0iKS3Jd0gqVHcd3iM\n+3VJl5YTb8r/egmwU0zbGUlviKRWkm6JcS2UdJCkxpL+GuNeJOmMDHF3iF84fEPSTZLel9Q+ej9e\nTxx3lqQpcX2mpEslzZb0jqSdYviAGLYgpqFPVTO9MoqHD+fdd5ewbOlS1qxZw7333M248ftlK3rX\ncA3XcI2Co77kV6419h7ckccWhm5NSz7+D2MufY69rpjFXlfM4uOvv+PQ61/h81VraqxTH/LKcaqC\nf0eiYs4GBprZEElNgI3M7OvYzehlSQ/HabPWQ9IOwNXABDP7IEO8I4D+wPvA48CBscvSpcAw4Etg\nhqT9zezBCtJ2lpmNj5pjEvvOA1aa2aC4ry0wBCgys4ExrE2GOM8H/mlmv5O0FzC5/KxZjyZmNiL2\nuzuf0N/uJOBKM7tDUjOgcaYTJZ0InAjQpWvXqok1acLUK69h33F7UlpayrHHTaL/gAFVTGrVcA3X\ncA3XyBbHHn0Ezz83k88/+4w+Pbtw7nlTOPb4qj5eq0Z9ya9carRs2ojte7XjwofezEp8FVHoeZWi\nNspufUXUynck6hxlqAc7kfjxjulmNjBOnzUVGA2sA/oCPczsI0mrzKxVrMz/BVgN7GFmKzLEOQa4\n0MxGx+1JwGDgGeAgMzsmhk8GBpjZmZKWAcVm9lmaVrohcZaZjZc0DzjMzJYkdNsCc4FHgUeAGWa2\nLi1tC4ADzGxp3P4C2ApolcqHGH4W0MrMpkiaCZxjZrMkdQRmmVnvOH7jHOA24P5kWspj2LBim/XK\n3MoOcxzHyQvWrcv9+7NRowZQE8kiwy94Kucac84fm3ON2iDX5XfH7Yczf97cBluAB24z1P7+2PM5\n1RhQ1GqemRXnVKQSvGtT1TkS6AAMM7MhwMdAiwzHfQh8S/h6YHmk/3pz/jYysy+BbYCZBG/BTdU4\n/XvWLyvp1/1d/F9K9HKZ2Z3AfgSj6lFJu1Y/1Y7jOI7jOIVJ3U/alHvckKiYb4DWcX1T4BMzWytp\nF6BbOed8BYwDfpfW3SjJCEk94tiIicALwGxg5zgmoTFwOPBsFdOWzpPAqakNSW1jd6xGZnYfcC4w\nNMN5s4BD4zl7AG1j+MfA5pI2k9QcGF9BulKaPYH3zOwqwvzEgys7x3Ecx3Ecxykc3JCoADP7HJgV\nBxoPAYolLQKOAd6q4LyPCZXtayWNjFOzJj0Ac4BrgDeBpcADZvYhYdzDM8BrwDwzq+gDIQuBUkmv\nZRg4fRHQNg6sfg3YBSgCZsbuS7cDvwSQdJKkk+J5FwB7xOs9BPgI+MbM1gIXEoydJyu69gSHAq9H\nvYGELk6O4ziO4zgNgwbgkvDB1pVgZkdU4ZhW8f9MQtch4iDr5KinExLrX6fGNqTFcxdwV4bw7hm0\n1gLp3YVS2qso+2Jhkv/xQpjZDYnNlcCeZva9pO2B4Wb2XTzuKuB/vlNhZmMS658RpqXFzC6hBh84\ncRzHcRzHcfIbNyScJF2Bv8cuV2uAH9VxehzHcRzHcQqSfPnWQy5xQ6KWSXot8o04s1JFg8Qdx3Ec\nx3EcB3BDwnEcx3Ecx3GyTkP4joQPtnYcx3Ecx3Ecp9q4R8JxHMdxHMdxskwDcEi4R8JxHMdxHMdx\nnOrjHgnHcRzH2QAaNWoI7Y2FxZzzx+Zco+1BN1R+UA358r6TKj+ohuS6/PqvgwaRCe6RcBzHcRzH\ncRyn2rhHwnEcx3Ecx3GySPj4dP13SbhHwnEcx3Ecx3GcauMeCcdxHMdxHMfJJmoY35FwQ8JxHMdx\nHMdxskwDsCO8a5NTmMx44nEGD+jLgH69ufyyS1zDNVzDNfJWo7Z0XKPuNU4dP4i5Vx3KvKsP5bR9\nBwHw6yOGM/vKQ3h56sFMmzKOLdttlDW9Qs4rp34gM6vrNDgOAMOGFdusV+ZWelxpaSmD+m/FI489\nSVHnzuy43XBuvf0utu7fP2tpcQ3XcA3XKCQd16g9jfKmf+3ftS23nbU7O511P2u+L+XhKeP4f9c9\nx6crV/PN6rUAnDJ+IP26tOX065+vUKMq07/me16NGlnMvHlzG0KjfEYGDRlqDz45K6cavTffaJ6Z\nFedUpBLcI+EUHHNmz6ZXr9706NmTZs2accjEw5g+7SHXcA3XcI2806gtHdeoe41+ndsy552PWb3m\ne0rXGc+/voL9t+/5gxEBsFHzpmSr/baQ88qpP7gh4RQcK1aU0Llzlx+2i4o6U1JS4hqu4RqukXca\ntaXjGnWv8cYHXzCq/5a0a92cls2asNewrnRuvzEAU44awZK/HMVhO/fhN3fOqbEWFHZeNQyU8798\noF4bEpLaSDqlCsetiv/HSJqeg3Qsk9Q+rr8Y/3eXdETimGJJV2Vbu5z03CTpf/ySko6TdE0WdaqU\n/47jOI5T6Ly9/Cv+cP8Cpk0Zz8NT9uG1pZ9Tui64H6bcPps+k2/n7meXcNK4gXWcUsfJHvXakADa\nAHlVkTWzHeJqd+CIRPhcMzu9ltJwgpktrgWpnOR/p05FLF/+7x+2S0qWU1RU5Bqu4RqukXcataXj\nGvmhcetTbzHqp/ex+68e5qtV37Fkxcr19t/z7BL2375nVrQKPa8aAlJul3ygvhsSlwC9JC2QNFXS\n05LmS1okaUJFJ0oaLulVSb3SwsdIek7SI5LelnSDpEZx3+Ex7tclXVpOvKsSadsppu2MpDdEUitJ\nt8S4Fko6SFJjSX+NcS+SdEaGuKdIulXS85Lel3SgpMvi8Y9LahqPmympOK4fL+kdSbOBUYm4Oki6\nT9KcuIyK4SMkvRTz5kVJfWP4AEmz4/UslNQnLf8vr/x2VY3i4cN5990lLFu6lDVr1nDvPXczbvx+\n2YreNVzDNVyj4HRcIz80OmzaAoAu7VsxYfse3PPcEnptuekP+8eP7M47JV9mRavQ88qpH9T370ic\nDQw0syGSmgAbmdnXsZvRy5IetgzTVknaAbgamGBmH2SIdwTQH3gfeBw4MHZZuhQYBnwJzJC0v5k9\nWEHazjKz8VFzTGLfecBKMxsU97UFhgBFZjYwhrUpJ95ewC4xfS8BB5nZzyU9AIwDfkiPpC2BC2Ka\nVwLPAK/G3VcCU83sBUldgSeArYG3gJ3M7HtJY4HfAgcBJwFXmtkdkpoBjUnkfzlpRdKJwIkAXbp2\nLe+w9WjSpAlTr7yGfcftSWlpKcceN4n+AwZU6dyq4hqu4RquUUg6rpEfGnf9Yk/abdKctd+v4yd/\neoGV/1nDDaeNoU9RG9aZ8cEn31Q6Y1NVKfS8qu+IhvEdiXo9/auk7sB0MxsYW+OnAqOBdUBfoIeZ\nfSRplZm1ipX5vwCrgT3MbEWGOMcAF5rZ6Lg9CRhMqIQfZGbHxPDJwAAzO1PSMqDYzD5L00o3JM4y\ns/GS5gGHmdmShG5bYC7wKPAIMMPM1qWlbQqw1swujl6S1UALMzNJFwJfmNkfJc0EzgI6Awcm0nw6\nsJWZnSbpEyB5/R1inrUFrgL6AAY0NbN+cbzHOcBtwP1mtiSZ/5XcKqDq0786juM4Tl1R3vSv2aQq\n07/mOw19+tfBQ4bZw0/ldvrXHh1a+vSvtciRhMrwsNhC/jHQIsNxHwLfAttWEFe69ZVza8zMvgS2\nAWYSWv9vKufQ7+Lx6whGRSpt66ieB6oRsJ2ZDYlLkZmtAn4DPBONg32JeWhmdwL7EYyXRyXtWp3r\ncxzHcRzHqVcox0seUN8NiW+A1nF9U+ATM1sraRegWznnfEXoAvS7tO5GSUZI6hFb/ScCLwCzgZ0l\ntZfUGDgceLaKaUvnSeDU1IaktrE7ViMzuw84FxhaQdxV5ZWY5s2ix+aQxL4ZwP9LpCHVPWlTIDX3\n23GJ/T2B98zsKuAhgpemomt0HMdxHMdxCph6bUiY2efALEmvE8YYFEtaBBxD6Otf3nkfA+OBayWN\nVJiaNekBmANcA7wJLAUeMLMPCWMCngFeA+aZWUVfbVkIlEp6LcPA6YuAtnFg9WuEMQ9FwExJC4Db\ngV8CSDpJ0gb5QGOapxDGUsyK15PidEJ+LZS0mOAFAbiMYGS9yvoejkOB12P6BgK3JfM/m4OtHcdx\nHMdx8p2G8B2Jej1GIhekj21wsoePkXAcx3HyHR8jUTV8jMQwm/b0iznV6N6+RZ2PkajvszY5juM4\njuM4Tq2TL996yCVuSFQTM5tJGPDsOI7jOI7jOA0WNyQcx3Ecx3EcJ8s0AIdE/R5s7TiO4ziO4zhO\nbnCPhOM4juM4juNkEzWMMRLukXAcx3Ecx3Ecp9q4R8JxHMdxGjDr1tWfaeAbNcp9E3BtTM3a7rCb\nc67xxd2Tcq7h1H+XhHskHMdxHMdxHMepNu6RcBzHcRzHcZwsInyMhOM4juM4juM4TkbcI+E4juM4\njuM4WaYBOCTcI+E4juM4juM4TvVxQ8IpSGY88TiDB/RlQL/eXH7ZJa7hGq7hGnmrUVs6udY46cRJ\ndOvckeJtB2U97trUgMK+H6fs0585VxzA3KkHcOq4/gAcsH135k49gFV/P56hvTbLmhbU3m+kPiLl\ndskH3JBwCo7S0lJ+cvqpPDTtMV5duJh7776LNxcvdg3XcA3XyDuN2tKpDY2jjj6OB6c9ltU460Kj\nkO9H/y5tOH5sX0af/TAjf/ogew/rSs8tWrP4gy85/PKneeHNj7KQ+jJq6zfiFC5uSDgFx5zZs+nV\nqzc9evakWbNmHDLxMKZPe8g1XMM1XCPvNGpLpzY0dtxpNO3atstqnHWhUcj3o2/nNsxd8imr15RS\nus54YfGHTBjZnbdLVrJkxddZSPn61NZvpL6iHP/lA25IOAXHihUldO7c5YftoqLOlJSUuIZruIZr\n5J1GbenU1rXUBwr5fiz+4Et22Loj7Vo1p2Wzxuy5bRc6b7ZxjeMtDy9XTmW4IVEJktpIOqUKx62K\n/8dImp6DdCyT1D6uvxj/d5d0ROKYYklXZVu7nPTcJKl/bWg5juM4jgNvl6zkigcXMu28PXno3D1Z\nuOxzSuvRl8nrHcrxkgf49K+V0wY4BbiurhOSwsx2iKvdgSOAO2P4XGBuLaXhhNrQyUSnTkUsX/7v\nH7ZLSpZTVFTkGq7hGq6Rdxq1pVNb11IfKPT7ces/l3DrP5cAcMERwyj5/D9ZiTcTXq6cynCPROVc\nAvSStEDSVElPS5ovaZGkCRWdKGm4pFcl9UoLHyPpOUmPSHpb0g2SGsV9h8e4X5d0aTnxrkqkbaeY\ntjOS3hBJrSTdEuNaKOkgSY0l/TXGvUjSGRniniLpVknPS3pf0oGSLovHPy6paTxuZvSAZIxTUm9J\nT0l6LeZXr3StDaV4+HDefXcJy5YuZc2aNdx7z92MG79ftqJ3DddwDdcoOJ3aupb6QKHfjw6btACg\nc/uN2W9kN+55/r2sxJsJL1c1owE4JNwjUQXOBgaa2RBJTYCNzOzr2M3oZUkPm9n/+BUl7QBcDUww\nsw8yxDsC6A+8DzwOHBi7LF0KDAO+BGZI2t/MHqwgbWeZ2fioOSax7zxgpZkNivvaAkOAIjMbGMPa\nlBNvL2CXmL6XgIPM7OeSHgDGAcn0lBfnHcAlZvaApBaUY7RKOhE4EaBL167lJGd9mjRpwtQrr2Hf\ncXtSWlrKscdNov+AAVU6t6q4hmu4hmsUkk5taBx79BE8/9xMPv/sM/r07MK5503h2OMnF5xGod+P\nO3+2K+1aNWdtqXHGTS+x8r9r2G9EN/4weTvab9KC+365BwuXfc6Ei2bUWKu2fiNO4aIMdWAngaTu\nwHQzGxhb46cCo4F1QF+gh5l9JGmVmbWKlfm/AKuBPcxsRYY4xwAXmtnouD0JGAw8Q6i0HxPDJwMD\nzOxMScuAYjP7LE0r3ZA4y8zGS5oHHGZmSxK6bQldnx4FHgFmmNm6tLRNAdaa2cXRS7IaaGFmJulC\n4Asz+6OkmcBZwL/S4wQ2Bt40s87Vyethw4pt1iu10jPLcRzHiayrR33sGzXKl3bamtHusJtzrvHF\n3ZNyGv+okcXMmze3ftyQDWDI0GE249mXc6rRcZNm88ysOKcileBdm6rHkUAHYJiZDQE+BlpkOO5D\n4Ftg2wriSn9y5/xJbmZfAtsAM4GTgJvKOfS7ePw6glGRSts60rxY1YjTcRzHcRzHqUe4IVE53wCt\n4/qmwCdmtlbSLkC3cs75itAF6Hdp3Y2SjJDUI7b6TwReAGYDO0tqL6kxcDjwbBXTls6TwKmpDUlt\nY3esRmZ2H3AuMLSCuKtEpjjN7BtguaT94zHNJW1UUy3HcRzHcZxCwb8j4WBmnwOzJL1OGA9QLGkR\ncAzwVgXnfQyMB66VNDIOTE621s8BrgHeBJYCD5jZh4RxD88ArwHzzKyiL78sBErjgOb0gdMXAW3j\nIOjXCGMeioCZkhYAtwO/BJB0kqSTqpQh/0vGOIGjgdMlLQReBLbYwPgdx3Ecx3GcPMQHW1cBMzui\nCse0iv9nErr5EAdZJ0clJadM/To1tiEtnruAuzKEd8+gtRbYNe3QlPYq4NgMSf0fL4SZ3ZBYn5K2\nr1WmfWY2ppI4l2RIm+M4juM4TsMgP5wGOcU9Eo7jOI7jOI7jVBv3SNQBSa+F4ywtpw0AACAASURB\nVDiO4ziOU/9oAA4J90g4juM4juM4jlN93CPhOI7jOI7jOFlGDcAl4R4Jx3Ecx3Ecx3GqjXskHMdx\nHMdxHCer5M+3HnKJGxKO4ziO04Bp1Kj+V3YKjS/unpRzjR8/8EZO4//gq9U5jd/JD9yQcBzHcRzH\ncZwsInyMhOM4juM4juM4TkbckHAcx3Ecx3Ecp9p41ybHcRzHcRzHyTLetclxHMdxHMdxHCcDbkg4\nBcmMJx5n8IC+DOjXm8svu8Q1XMM1XCNvNWpLxzVcI1tcvE8ffr1HL87dvSe/2q3nevvGbrUZfzpk\nABs3a5w1vfqKcvyXD8jM6joNjgPAsGHFNuuVuZUeV1payqD+W/HIY09S1LkzO243nFtvv4ut+/fP\nWlpcwzVcwzUKScc1XKO6GhVN/3rxPn347VPv8Z81peuFt23ZhKOLi9iidTMuzrA/yf1nH8qn/3oj\nP2q7dcC2Q4tt5qzZOdVos1HjeWZWnFORSnCPhFNwzJk9m169etOjZ0+aNWvGIRMPY/q0h1zDNVzD\nNfJOo7Z0XMM1clV+kxwyZAvuX/gR3gRdBRTGSORyyQfckHAKjhUrSujcucsP20VFnSkpKXEN13AN\n18g7jdrScQ3XyLbGGTt341dje7JTj7YAbNOpNV+t/p7lK7/LmoZT+LghUQ6S2kg6pQrHrYr/x0ia\nnkX9ZZLax/UXq3D8TZKy64+vWO9CSWNrS89xHMdxnNrh8n8u5aIn3+Pq599n597t6NN+I/bu156H\nX/+krpNWMKgWlnzAp38tnzbAKcB1uYhcUhMz+74qx5rZDlU45oSap6rqmNmva1MvSadORSxf/u8f\ntktKllNUVOQaruEarpF3GrWl4xqukU2Nr74N1ZNvvitlQcnX9OmwMZtt3Izz9ugFQNuWTTl39578\n7qmlfP1dlaoyTj3FPRLlcwnQS9ICSVMlPS1pvqRFkiZUdKKk4ZJeldQrLXyMpOclPQwsjmFHSZod\ndf4k6X+mQUh4PRpJuk7SW5KelPSopIPjvpmSiuP64TGdr0u6NBmPpIslvSbpZUkdM2gdJ+nBGP8y\nSadJOjNez8uS2sXj/prQvkTSYkkLJf0+hnWU9EDUek1SpcZQVSkePpx3313CsqVLWbNmDffeczfj\nxu+XrehdwzVcwzUKTsc1XCNbGs0ai+ZNGv2w3r9jK97/YjU/m/Y25zy6hHMeXcKXq9dy0ZPvuRFR\nGQ3AJeEeifI5GxhoZkMkNQE2MrOvY3ejlyU9bBmmvIoV5quBCWb2QYZ4h8Z4l0raGpgIjDKztZKu\nA44EbisnTQcC3YH+wObAm8DNafqdgEuBYcCXwAxJ+5vZg8DGwMtmdo6ky4AfARdl0BkIbAu0AN4F\nfmFm20qaChwD/DGhtxlwANDPzExSm7jrKuBZMzsgGketMl2QpBOBEwG6dO1azmWvT5MmTZh65TXs\nO25PSktLOfa4SfQfMKBK51YV13AN13CNQtJxDdfIlsYmLZpw0g7hfdxYMPuDlbzx8aoax+vUT3z6\n13KQ1B2YbmYDJTUFpgKjgXVAX6CHmX0kaZWZtZI0BvgLsBrYw8xWZIhzDHC+me0St08DfgWkOh22\nBO4ysymSlgHFZvZZQuOPwGtmdks8/37gTjP7h6SZwFlAEXCQmR0Tj5kMDDCzMyV9B7SIFf6JwO7p\nXaIkHUcwbH4Utz8AtjezEkmTgMFm9hNJfwWmAw8C8+IyPebZGkmfAp3NrMqjsqo6/avjOI7jODWj\noulfs0FDn/516LBie+7FOTnVaN2iUZ1P/+oeiapxJNABGBY9B8sIrfXpfBjDtwX+x5CI/CexLuBW\nM/tlFtNaEWsTXpRSyr//ycr/usT2uvRzzOx7SSOA3YCDgdOAXbOWYsdxHMdxHCcv8TES5fMN0Dqu\nbwp8Eo2IXYBu5ZzzFTAO+F30PlTG08DBkjYHkNROUnlxA8wCDopjJToCmTRmAztLah+7FB0OPFuF\ntGwQkloBm5rZo8AZwDZx19PAyfGYxpI2zVUaHMdxHMdx8g3/jkQDxsw+B2ZJeh0YAhRLWkQYI/BW\nBed9DIwHrpU0UlKxpJvKOXYxcC5hHMNC4ElgywqSdR+wnDBQ+3ZgPrAyLc4PCeM7ngFeA+aZWYVf\nqJG0n6QLKzqmAloD02P6XwDOjOE/BnaJeTaPMK7DcRzHcRzHqSf4GIkCQ1IrM1sVBznPJoxn+Kiu\n05UNfIyE4ziO49QOPkYitwwdVmwvvJTbMRIbN/cxEk71mR5nRmoG/Ka+GBGO4ziO4zhOYeGGRIFh\nZmPqOg2O4ziO4zhOJTQAf4yPkXAcx3Ecx3Ecp9q4IeE4juM4juM4WUY5/qtUX9pL0tuS3pV0di6u\n0Q0Jx3Ecx3Ecx6lHxE8AXAvsTZg583BJWZ9B08dIOI7jOI7jOE4WEXX+rYcRwLtm9h6ApLuBCYRP\nCGQNNyScvGH+/HmftWyq96txSnvgs1ylp5Z1XMM1XMM1XMM16pNGRR/YrffMnz/viZZN1T7HMi0k\nJefNv9HMbozrRcC/E/uWAyOznQA3JJy8wcw6VOd4SXNrY/7k2tBxDddwDddwDddoyBr1DTPbq67T\nUBv4GAnHcRzHcRzHqV+UAF0S251jWFZxQ8JxHMdxHMdx6hdzgD6SekhqBhwGPJxtEe/a5BQyN1Z+\nSMHouIZruIZruIZrNGQNJ4uY2feSTgOeABoDN5vZG9nWkZllO07HcRzHcRzHceo53rXJcRzHcRzH\ncZxq44aE4ziO4ziO4zjVxg0Jx3Ecx2lgSHX8qSzHceoFbkg4juM4BUGuK7+VxZ9vle8NSY+kEZI2\nNzPLt+txHKfwcEPCaXDU9cszpS+pdV2moyFR1Xte12WjKuRjGiXl/F0iqQNwtaS2OZTpU4F+C2BQ\nXN9KUq8cpqNSJMnibCmSulR2fILjgKckdagtYyLxzMu7slsVNjTdieveJLspqlS3Uab1fCeRX83q\nOi1O1SmYAuY42SDt5VvrL7WUvqTdgd9J2qxQXq6Jh3xPSe1yqZGD+JpX5fh4b8ZI2q+GeqntRsn/\nNUHSBEndLA+m2kuUhSEAZrauNmSBlsDvJW2a1YgDTYH7JB1TzmHdgDGSrgceAb7PZhqqS+I5djIw\nRVK7in4/khrH804B3gLuyLVnIhFvx/i/ICqIifK9BZTldXXjiHm7H3CbpM2znMzydJsBEyVtImkQ\ncEpN728iP/pK2jgb6cykEfNrb0Kas/obd3KHGxJOgyLx8v1/wN8lnSRpQG3qS9oDuA64x8w+J1SQ\n8p7ES/F6oCjb8SdeJHtK+pmkX8TK3QaTyO/bJJ0l6dgqnDYQ2C+mqcrf2kkzUn8iaSpwr6Simla0\nJf0U+DmhIv2DXk3irAkxX/cB7pa0XS7TJKlI0h/N7BPgHGAVcGU2KxoWWAv8FmgXdRulHfM2sAkw\nGXjIzN6Px9XZfZB0HDAJmGJmXwCtE/vWS5eZlcbwScBKoCswM5fGRIx3HHCrpMuAM3PVCJFNEuX7\nEUn/kLR3dctbosHoQuBSM/tEUrOUQZcrzGwNUAosBu4HHq9p40Oigv8w0CMX1xA1dgX+ALxqZiuz\nreHkBjcknAZB8iUpaTiwD/Ao4WV6cqpltRbS0RjYHzjLzJ6XdDBwV3y55zWSBgIXAOeY2aJsxx9f\nJGOBy4GZwNnA+TVpzZe0C+HFdA1QDOwlKaN3QtK2kloCXwGfxTRVudU5YUScAIwDpgCDgV8kNDak\nT/sw4EBgtJm9pdDHfUhd9nGX1B+YChxqZi9L6iRpkxx5S74DtpV0g5l9BPyOUBHOijEhabjKulK8\nDhwtqW/K+EvL478CpwLfSzpF0mbxPuSklTZDWpW2Xgz8Pm6eATwu6feQuRVd0g7AL4Fzzawf8Cww\nI1fGRHyu/g44Adgc2B5Yk02NXBDfET8CTgZeIjQsjN+A8tYfuAFYKelI4CHgxylPRw6ZB3xNMCi+\ngvUbRap7nyVtTbiPk8zs9ZRRmi0kNYppOhT4g5k9mzJWcm14OTXHDQmn3pPWUjwc2A6408xuAe4G\nVgCTJRXnSj/+7wO0AF4EbpI0DRgKPAf8RNKWudDPIh2BJWY2F8oe8LHynS32Iby8WwLvADdUtzU/\n7SXZBfg/QjeUXsDPzew7Sd3SzmkF/Aq4DTgEOFXSeZJ2kzQ47i9Pb0D0eqQoAk4itBQvIbTCNpXU\nfAMr2i0IXbOOk3QNcBEwS9LIOuzm1Bp4Hmgt6QLgPkLrdt9sC5nZZ8DBQJGkP6cZE1dIalNDiVOB\naZLOAf5L+ILvjyQ1Tj07JO0j6XzgROBmwm+4P7CvgpfulxWVkWyQ9hzbmVAxfxi4FriKUMZ/ChRL\n6pQ6Jy2aj2PavwYws5OBLwnlqX0OylN74CagB7A18GMzWyVp63ytIEraDDgXaGJms83sD8BcYBRw\nQEXlLUN+/xvYBbgd2Ax4ijAOJ+t1r8R7ZrsY/ziCsf+EpMHxK8fdJTXbgPu8FnjJzGZFr0qTqFWj\nZ38iv1rGNH0TtQBS3uh+CuOjnHzFzHzxpUEshMrdW4QK0NxE+ABC5ez3QPMcae8HPAMMidu7A73i\neldCy2D7us6jtDQrlT/x/0DgDkKFvHEMGxXztWkNtUYTWitPBv4CvJDIn8nA5GrGtxcwFjgSWEZo\noWuf2PcLoEXaOY0IxtIQ4ANCJW0qoRLRvRyd5sDpwN+APWLYZQTj8I5UeQLOBH5WzWvoD7SJ6z8B\nbgHGxO0Lq5snWSoLHeM1i2B0PQQcRTD8rgJOyrZmYrs9YWzCn+P2FsCfYt433oBr6Q20BZoQKnf/\nR/BI3BPLS+q4beNz4wSCwfR0PG9v4ArgPWD/XOd9YvtMgseud+I6Wsb13Qkt6JsmzwPaxDS3jOfu\nl9j3f4RnYucslpNeBGOzF/BqzL/N475xBM/OJrVVfqt5DS0IjQlvAicmwk8hGEWdKjl/H+Bi4PS4\n3QHoGtf7AguIz9QcpH1v4H1g+0TYGfFajicYMltV4z4OiPewF8Eo2j1xzM4Ew7VRDcvKWMK7txFw\nDKFhb+u4rxhYVJU0+1J3S50nwBdfamMhtAo9kXjhPgHcndi/NbBZjrT7x4rJsAz79gfeAA6o6zxK\nS1ej+H/vWAkYRBgoeQOhcv2juO9dYJcaag0BXia0Wk4AlgJ7xn3bxBfJHtWIbzihwjmK0Kp1M3Bj\n3LdjzO99Ei+yRolzUwbSRcB2cX3zSvS6xUrGn6N2N+BDQut1U+BoQgW1bzWu4RTgFYIxch+JSh6h\n4r6YWJGshbKQyqd94n26hrLKfJP4f1C8TztkWXMXQqXu0Li9Wby3N8TtLYH+GxD/3sBr8T6/SpnB\ntlX8TS4ieD1GEAyV0xPnXg/8M663AIqSac5B/jdJrI8EZgGbZii7p8SyPTjt/DOBBwjG7aExTxcQ\nuileEeOrsRGR0BsX83YAwYD5dSwzBxAaDBaQMGTqekmUte2AMUBx3N6P0JhwQuLYLpXEsU28Bz8B\n7gWmJ47ZN/5uc3LtQGdgITA8bg8CdiAYkEcC04F9qhHfhPh7Tz0HDyQ0sJwCHBa1anQthEadd4gN\nJDHs2JiHNwHzgQl1XUZ8qeQ+1nUCfPElFwvrt8Y1Bc4jtNSMT4Q/CjyaA+0i4LeJ7R2BBxLbqcpq\n2/hw3ic9zXWYb5sk1ofEh/zQuN2M0NL5Y+CPwJ3VeTGVozcAuBW4KhH2M2Aa8I/4Iqvyy4pQ0XyD\nMJAdQmvzdoTW8pcIrbH7xn0dKojnauD3qfJTUfmK211ivtxC8NwMJFTc/kZoBaxyCyShsrgoxllE\naFF8ieC5GkkOWzTT0tE4sV5M8MxsBfw/QoVok7hvJ0LFMasVJEKF9A1CBfRT4OwY3i7ex5s3MN5+\nwGxCK+sxMa83Y/1KeT9Cl6HesTJzM4mGBkLFvKgW7sHWhK5WqYrqSIIXqHna/dkilvO+aecfDswA\nGgN3EZ93BIN3EsFYqrYhVkF6B8RyMjIR1iem43GCJ2K/GJ4Pz7vUs3gfQqv9/xHGR+0Vw8cTPFCV\netoIRuefgePjdiNCQ8AjcXtHYFSurp1gtE0ljM26Nqb7CcK4BoBWFWkTvFWpBqRBsdz3jNtbEp5H\nQ+I9vD6RR1W+lhjP6Fgem8ff1a5x3wGE52UxwYvTDxiYL2XFlwrua10nwBdfsr2wvhFxFKECtgWh\nO8v1JFrQCbNaZLVCQHDpD6TMnV1E6C7Rj7IW3FGELjHN0tNch/m2KWFWnI5xe0dCi+UwwgDNBYQK\nfre4v8IXUxU1e8cX08PAtonwvkB3oE91NYAjCP2+D00L34yylue9CS3+ncjsmdiDDB6kDOVrZ0KF\ndHNgY8q6IA0idP9pCrStJL2NEuWiOWHczF/idpP40r2OMgMo513g4ov8NGKXrliBOIRQsXoF6BHD\nh8brTHV/y0o5JlQ4ZsZysC/Bo/cF8LvEvRxejfiUFvephIrLbMq6CO1BqEyJ4E1aE38DQ2JafhR/\nw8MJ3Zl61MJ9aBKvdaf4vz3hWTKEsm5zRxB+t5kM3oMJBsbPCRX5ZvH6uuYovQMo8/61pKyi3iyW\n87x43pF45sff7xzCs2g/wrimNcAhcf8Eopeigviaxt/Cq4RGix+ejcCDwGM5uIbUc2tLYOO4fjhw\nJWWV/FOAa+N6uV2QCI1a11D2fBxM8GAcSfAoPQK8DeycKQ3VSPNEgnHcOm6fSujKdD9hko0pBE9H\nTnoH+JKbpc4T4IsvuVoILbnzKavkdAPOIlTK9syxdiNCi/oDcfs3hP7cp8UX07+A3eo6j9LSvCnB\n4CoidIFoSWjReoTwEasOBO/BUalr3ACN1MtvENCTUPluR2h1nUJsgdqA+IYBu1FW8T2A0NJ8UIZz\nxsSX4o5xe6PEvsHAqVXU/n+ESvUUQvejvpSNmfh7Ve4vwUjYl9DSfDahQlhEqKiemjjuOuDk5DXn\nuCzsEu/JTwktkdsQBuouJlZYCS2L95Elwya9PBEMyaHAgri9PbAOOL8acSZb7UcTxtsUxd/fikT5\n2T6W9ZSBtDdwPrGPPKEf97OEcU63UGbU5ao7U7rH68+EcQwtCWOS7iZUvM4hdC/slykthH7x77F+\nN85UX/+WNU1/hnQOJFQEuybCdiSMfWpSE61s5i1hDMPARNhWBIPr1bg9OZa1SrvVEDwulxAaE7Yh\nGJ3HESv38Zhts3kNiXj3IRjZVwB/TNs3gmDYVOldR+ga1SP+9lsTPKxzCQ0InQljhI7JQprbE8ZX\nHRC396VsTERX4EkqaXzxJb+WOk+AL75ka4kv03ZxfWvCgN2NCa16Y+KLond8+f4R2CibFYEML9VN\nCMZEqoXuqPjAvxXYu67zq5xraBxfgn8HxsawlLu7X6wkjKyhRqqFeWqs0AwitLZeF1/I1eq2Q6j0\nvRPv62eEaVJTOu/FF2GyRfpHhH643QndO+YRKo1NY9hOVdDci1CxbEZogXwZ+CdhPMxGhMrellVM\n/xGEMRRvAtvEsGEEY+cygkE8j1oaE5FI15h4bT8nGEhHEirfOxFaPmvcRzrqNE+sD46/09TA3J2A\n6xPr11LF8TKUeVVSg/Z/BhwZ17cHSgjG4IkEb9uEZNkjGB4vUzZIf/t4z3+c43xfr1tmYv0qgleh\nRUzLjwkNFOndmU6O5fnwuH0LwUgaRDByXycL3ZkoM8J2Icx4Np7w/DgdWB5/d0cTxliNq82yW4W0\nNyZ4I5NdTo8kTD0KYdD6PVShEk7wLl9BGG+zGaEC/xShm9TGuUh/1N0x3st+sWx/DDwc9/UgdL+b\nkF6mMsST9MSeSZjVK/UMTTUabE/oZrhB4+HS9SnrBjqeOOkFofFnIXBgXZcPX6p5f+s6Ab74ko2F\n0D/0dELrdjNCq+MzhErQTYTxEMsIlaPNiAZHlrRbE1tQCN0jfkHsU0swZB4kGhMxrGVd51da+lMV\ngpQ7PjVA+BbgiBi2K6GFv0YD3whdMl6hrKvZovjCHkZo0buJ2J2pivH1JvTP7w3sSRjkvIRoqBEG\nzu4U10fHl1UxodL+FKEiOSGuD6ksjxLb/QkVkRMIBsRmhP7Q84mta9W4hs0JfYUfiOUzVXHtRejD\nfj61MCYirSwMIBgPYwhdJX5OMIyPJIyNuZUN6COdQa8jodtc7/jbWR7jX0Iw1sYQjNor477tq6pJ\nmVflrJjHv2D9gbMjCYbJb6P2YEJF5i5CF7V2hGlAp1DWRWc0oZX3KDZwtpqq5H9cP50wucHVibBr\nCc+y1PNGaefsSDA6zyT8fn8bwy8ljPu5q7rls5L0jo/5cQrByLma4Ok4NN6zWymbOKGuuzNtTFm3\nzX7x9/sAcFfi3v6d0JjxLrFrY3npJmGMESraF8ffa2vCIOcXyOIg9vRyQjCsB8aymxrz8yxwXzym\nQ2X5TtnvPTmZw2TC83AM0IpggC4geuFqkObdYplODWY/PpaP8YTn5yTiGMa6Liu+VPPe1nUCfPEl\nWwuh4jOA0HLXND6ofkdsQY+VgnOzrNma0JJ+JKH7w5L4sHyXsr6pGwGPJR7wVZ6qshbzbn/i9y0o\nczkfS+iOdRjBq5OaunaDHvIxjk6E7ipjCZXuEYTK0jOElugqTyNL6BvcnND6Noo4pS+hdW4toSKZ\nelEqbv+L0MWtE7EfLqHrzlzKaaVl/Yra4cQWsxjnJcSZiggtklcSx5BU8Rp2IvRN3ojQgns7Za2I\nvakDFz9hkPPsxAt/D4IH76eU9W3OSiWa0GXijph3tyby8nhCH+1+hO4iE6liV8C0+7UzoXJ7Sszn\nH8fw9iQ8krHsPU7wSO1OMJwWxLTNSIt/x+rc4xregzGEQftPJPbdEvOmEeu3Jg8meFdSg1f7E6ZS\nvpgyr2LWprcmNAbcEn9P4wmNAn8kPA9THqW8edYRGjH+QWgNfzSme1OCcfW3eMzuhOdHhZNIEBqr\nZgC3pZWLx2J5a0ecWSuL6U+V1RaUeQqasf4z+yKCl6LcRpEM8e5FaFi5Azg4hk2KebQrYfxEj2Qa\nNjDv34jl8c/AGTH8WEJD0n6UGetuRBTYUucJ8MWXmizpD534ML+WMM/+ponwo+ODrMpTcFYjDZPj\nC/UqysYPbEIwJq6K2xsTZz/KlyXxYmpFGPA8IT7Q5wOHxX0nEGbSKHeGoyroNCb0I55HWUvqr4jd\nHQiVtj9V8+W3OaHSPiCRzuvi+hhCP+XUtIV7ECuhBKPoAspa6w4meDQqnX6X0LL9MokW3Vhp+Hvc\n9xqVzzH/w8BugiGyK2ECgF/HfDo63oubCPO253xmoLT09SRD9zVCBetGwjiOGvetT+VB/N+VUPmc\nDxyd2P9rwgxF1TJaEnlcHH93u8bf5kJC16w/EbxicwlGSsqYPSktnu0IBk0JseJTS/dgLKFSd0wi\nbCZx9p+43THtnNNi/i2LeZnqLrI1wcOTmoEsa2MiYnntSmixnkuomI8hVEpvjnmfda9NDdN/HeGj\ng0nPVFuCMXFfedeaFt4t/i8iGHR/Suy7nND/PyffPSA8nx8keFImx7CrCB60AwnGTaUe3cRvpF1M\n7yjCM/Qq4Edx30mEmZ82yHuf0OhAMBhSsxPuTTDqfxK3JxO7dPpSmEudJ8AXXzZ0SXupDaVswFZ/\n4A8Ed/7mhIF0M8ly9xDWbw2cGB/iVxL7xhNauz5KvmjybYkvkF8AlyfCxhIq/UfH7Q2qzKa/iAn9\n/X8eKyC/IQzcnUAYC1DpuIvEiyk1h/7llH1PYBShZetKQoVxu8R5d8T78GuCgXkO0ftAqPjsXE56\nk+WrD/BkXG8dX4apD079kjDeo8ovw1RFg+Cl2TGm+4IYNpLQstyvDspDLxIt8Kw/fmEHsjRVaOJe\npjwcG8U8uJQyT8hogoem2oN0CYNQ36Jsus3RhErSFILXQQRjvzGhS9xigndpowxxbQdclMM8Ty93\nuxIqqNeQmBmK4CG5L8P5EwgVy0aELoJPEiqBqVmd+gJbZDG928f8TXUZ3Iay6Za3IxjCtV52q1DW\nWsXf1WWEWZqKE8e0j+mucKY2Qsv6h5QZZp0ILfd/J3g8Z1ON2cSqeR2jKZsG+irKBoaPJRj5zxA9\nClWMb1eC5+H2RNiRMe5T4naNumYRxqm9En9fqYaeZgQvyF+o5kc6fcnPpc4T4IsvNV0I/dxfiA/T\n+bFSMorQ9/kqgiu4dZY1Uy+WrQj9O5sSKoR3Elq9U679TUl8bCcflkTaR8YH/J2Efs77UjaN4F4E\nD06VBgxXoDWa0KrVPcZ/HmUf8JpKaCGs8rgLQuvxPymbG30GodtK45jmX1M2PiI1B3lvgjt9t/jC\n/zCmqdxuF6xvJB5IqCA9T5j+9iZCq/ZC4MJknlYh/Y0ILZnrgGNjWLP4Un+S0BWvRVXiynKZ6Ecw\n8toQpuI9JLFvNxLdY7KoOY7QnejG+BtuTfAMTCNUouexAQMvCZW7+SS+7hvD9yB4f84mdInbDbg4\n7ts/lqt9SeteR9nHAbN+X1jfWN2J0Lq/BcEzdGfU7pY4plva+W0Iz7gSyrx9o2NZOoM43Wq20hl/\nB8sIHodpBO+vCB+R/AdhHEveDKxOpHt/wkxXqfFHP45lpBvh2XQKlXT7iuX1QYLB+QVwSQxvQfBo\n3EkWv3BOaAQ7KbF9EMEzeCChG2qqu9EPxnh6maog7h3iPTsP+I7ETGiEyTaup5yP71Uj/dsQjKwB\nUe9fwHFxX1OCMVorY798ye1S5wnwxZeaLPGl+QShMvYz4PnEvtQAuA3ullOJ9l6Ej9zdTmiJ6kRo\n4f4b4UNXHRPH5lW/T0LL5YvAiLh9JqGyPY4yY6LGc3kTWuk+IoxP+Q3BM3ROYn+l88qzfqW+F8Ho\nWUboonRUzO8tEscoLn8jVFR7Evrfn0owOC4nGAGVehAoM4RSA/h/S5k3YUIsX9X5xkWqYrM78Dmx\nK1wM+1t8gVf4Je0sl4NUevYDborrk4nTSRI8L2+S5emSCZXmuQTv4cXA5Oz/PgAAIABJREFUazF8\ny1gO76KaY3IS19ITeDoR3iKxPpJQsRlLmOlrt8S+wwiG6cGsP1vSRHLcwh7L5oJ4/98lNIRsReh2\n8lMq+O4DoTJ8cyzjqVnrxhIMwjb/v73zDperqv7+Z6UHQgJIkR5q6D2U0EILEAgERAQUEZDeEaSJ\nEEASihQFFBApUgRpUgQUkKaA9A4CUkUR9ZWfIIrCev/4rsOcO9wyc+/MnXuT9X2e/cycMrucs2fv\n1VcD+7gRMuEs/FjmQKGh90TE9FZ0INFvZYn/2hOlfg+JdWAPRNx2Or9jLRmBGM3Ch2De+O3ppft6\nnFenqs3N450Wfj1fjP/MncR6h3xTLkbatVr/J2NiXn01jhcjNLalezo10ayhjdmRVUA5ceX6SPu8\nZ6vnRJbGlpZ3IEuWekr1Yhmb7S5IEv1LKoRpEf2hYc6FVe2uhIitIkzeYcAbyB70C0g61SNpfpOf\n40pIqnZW6dxB0e9JdC9HxOxUknstTSXU5gXIJ2JVJGX+L6FR6KK+0VRC0K6OIsHMiiRd+yIC9Cwk\n3f8GFUJyzlId30TmMkfHprZmEBGdmnrERj4+6t67nesF4bdcHc9na0S8FCFe1wX+gWyTdykTCL04\nDwqmcRjSuOyBzK3WQkTtaTRBwoykkeshYukBKvk/5kYavnoygRfvvZwP5Grgm6XjCYgIHhjjO50K\nUTikdN/28Rzm7s646u1zaZ4/RsX2flOUyXtZxHCdF/N+1fK7oG2OjMXiXV1ERere0OhwMec/Ab5U\njAExF+f25pytoZ/zEqZHcXwIMvVaGgkebo31Ylg805pMEmPObFw6Xh8lrmua/0ysC5dSMTO9EgnO\nPhf/nefowjG8nTonISbz0tKcWwT4ADihgX1fBwnZppbm5EZon5yfPiZcy9KDd93qDmTJUk+hEtmh\niFqxKJIsPlS6ZydElDU8OyYyTRmOIpQ8FURA0afTCGk7vewk282xFIT94aVzhwLLd6OuIp/CVCTx\nXR85Hx+CTEjuQ9GVVojNpUs7YmSO8FbUtTIyKToN2TjvEpvfAMTEFY7bm0RbFyHzmOFBQHwd+DDO\nt2vu0d7GhiSCb5aOh1HJVF5z8jxEhD2AmLU/UQmruw4iem+olaBp4PtfNJ5REYVqHNIaDSrd05BI\nKlSI/QXjGW6HtEq/oSKx3AgRzXXH3keMwk9iDm4Xc+dkZG7zBRTJZlLp/hNjLpV9QFaNedwUDWYH\n/V4DOUT/JNountPhhP044bSMGNvPUzI5oa22brF4fj+I+xv1ztamIs0/HHiPisBgK2RWWrNEvBee\n6cyICStMfzaP/+tjKKLeN5AJXV1rNNKq3kwlUMPKyP/qdZqQXBQx279GArK7gF3i/I+RFuIWupmP\nCGlpzkQmXvPHuUUJwU0j5k18H480wCeUnlvDQq9n6Rul5R3IkqWWQklljmx/b0JOrgshKfNf4/wp\nyPSlrgzJNbT/abSd+FwEqZnL6uD9CFV3X9lU2xnHwKrjVRAx9+0e1Fk8m/kQsX8o0k7MEhveobEZ\nFs7En3Fo7aTOPZCvRmHmsl5squ8iM6nZS79ZFqnO10cal1Oj3cLkYFNqSKiECNodqBBLFyDGsSwB\nrtn2HBGodyACZx9E1N5Nxc9jYD31NWgej0bE93bILO/byFfhLprE0CDzqZ9TMQ37QfxXZ0VS0mfp\nhvYDGIuYkonI5vtEZIK2ONJYHUlVzgtkEvKphBk50d5HN5joHjyP7ZFkeE5EMJ5TuvYt4OSqPg9A\npl/3EERlMX9K3+ehgdqU+M+8QgQkiHOHAx8jJvQy+lACsdKzGhb9uymO5yLMw5BpzxPUuEfQllk7\nN97ZeSjZ5VIx59ZtQN+HU9lfPodMT5dFGrTt0Nr6pdL9dYeXrRrLlsj86HDaMqeNMM0qz8n1kC/N\nNOQb0WdCAmdpTGl5B7JkqaUgqdcdiPD9OcoGfCyS5C0UhMAByISm5oRmNbZdbE7jkYlMEcZuIUQU\nXoXsyp+gAZl+m9T3NelAQ4McmB8nJPw9aGMccvL8c2zicyKn1g2QZPh9QsVdY71bIOL2TkRkFomt\nRiAi8FEkVSui/CxNRMhCxPlAJL3brr3+lo5HlL4figjrq5C08Vtx/jykku+WwzEy2dmSsN2Pufp/\nSHLeqyEyox/3E2ZZSEuwDgpl+w4yeWh0cII1kAavcIAfFHPjeOA65JRZOMnXRcggn4aC6C7m28Ux\nn8+grf9MQagNRgTUxWhdeZwaQgD38BmUJbWTY24WDM6IWD+uRMTdY4hILf+mIIS3QVGayqFyNwD2\na3B/543nUvhRrYak+wOoaPiKtbDm/C+9ML8non1hDsTo/Kx0bRIyB2p3naaylq1KW+aprKVbF/kO\nLYaEFs8Q2dN70OfZ0Jo5axzPHs++MIMcGfPiUSphUztdNzoZS5mZ2AoR+aN72P/V0R6zZOlc2c9o\nTRqYCDFL3yot70CWLJ2Vqo3058gsZFIcL45Us5dQCvfZpH4USXt2Rs6QJyJJ6vxIM3E7wcB0tcC3\n4BlugUJhrl86VxBURaSXHiVPQsmwnkfSvrVjAz+Okp8I9dm9F86Ma8X3HZBWoCA2C4Lxe0iavlzc\n9xalEIhIQ7VbJ+1sicyxRsb7vJmK38CaiBAtzKYuBBap87mUN+1dgUvie0EM9sipsRvvaZkgRtrN\n2osIpItpsI9AjP0CRNwfiDSKd1CJNFOzOROf1aqti4i51Uvnbor/7BxI+7Jz6VphFjkQSX5XLf13\nm6JJjPlVJECcO57Bs4ihLJ7BUBR+cyeq8hCgqFb3IYZjBCIAb6SSt2Ypehhlp3r8iNk7iYq51K2I\n6Tsyrh8C/I8GC2562P/lkDCjmN8jEWN8dRyvQyUkcEd5IrZEa81a7Vz7HBU/vCXiv9QQLRbaSxYm\n1mlksnkOFQ1ewXzWs452NpZi7e+WCTBtBUhvIRPNNoIbFA1uh1bPiyzNLS3vQJYsHZXYcOeN72vH\n551EBuM4XgRJFn9IKVNtg9q3Uj+uiY2jiDxxOSJSZ0EmPY8Dx7T6mbUzhkUQkVXkTViYIIYRkXUz\n0hz0RBMxDDEQd5SuLRft/oCKFHpA+Xed1LsQYhLLksSBiKh/AxG7Rds7IKblLMQIrIMYkP0RIfkE\nHZgdxLifRpqmeZCN+lNUMksPQEzKmQ16F8sj5uGmaKdXCLDSs5oZSVGviONBVHwgyiZid1CKJtXD\nNguifb4Y+1PIvGtVpE0aV76/izpnK31fD2kfxyLtwv5IezUREdWPU9G4bINMtr5cVV+HkZAa/PwH\nIcJ/V+S3cUuc3wtFFVuXToJCII3Lo5SYWLTWTUJmTjXnDuikjZHtnBuAhBDfoULcfpmI7hXHB9Kk\n5GvdGMMoJOB5m7aS8ZHAteX1pJM6FkYaycK/YgwR0hUxcMdQ0QwNpAGZ52krbDgE+VKti/zJjkLa\nqSNQuNYN6qi3q7EcS8/zRGwQc3r5mJNfRozbdnF9PepINJqlf5aWdyBLlo4KUpfeg2zdn6CSYOk3\nwK2l+0Y3YkHvoA8TkDnV/IiReJBKtI+/Eb4FSPLyIHWY7vTC8xuKCOTrkUnCdxEj9hqVZFI93Ug2\nQ34ISyLN0KZUJPpFJK3RNdRTlm7djpxK76MUEQZpgy6hEilrYmyyJ6M4/yciJmQsij71I0oOth20\nexAyq3k1jndFzM/40vF58SxrDa+4FyUnSNoyXIsjv49eleIiJ/QfIcf3X1DShMQz3yuIi8/FM+02\ncVga76bxTg4p/XeLZIIrIy1ZrRFzZkaM+8GIyHoWSWufQYzCSijk8oNIUv8F5OuyNrI9LyLVFNFv\nhiPGtKnmFqVnsRBKgvZnYELp+sGImd+QdhzbY9xfJjIx0zYy1VBE6Pc03v8IPmsC9hk7dqShe4w6\nowT18jxfBGm+vk/bZH4j6YCgLb2jIfEsrkFr5SkoCMKDSLtqPX3WnbQ9f+ncbkj7s070ezIK1lCL\nf1evjKXUztkokldhgjUPMju+hj7kO5OluaXlHciSpbOCNA3/ocqmFTknPtjktldFavI143hN4P74\nPgYRvMuW7u8TdsJIUrZYEC7DUcSMK6iEuzyK0J7QAw0OMpP5JbBSHB+BpMzHoVCnDxCapBrrWzd+\n/8U4/hyy5b8aJUl6EVg5rg1Hkq/Cxnz12DC/S1Uehs7GiMwH/h+SZlu0uR+KBHY+0m7UnM05NvxH\nacf8gF505K1qdwySyC4fx2cFobIT0ui8TCmOPiWfkR60uSnS9qwOvInMpZZAzNRa0eYWddQ3DOVz\nOB1pdDaI819EJo/ble4bgRjcF6j41cyEtALXUXFy/4wUvsHPvcwQzI+cq3+KpPgrlq4diQiv4VW/\n2RcxuocgTU5ZI7MLVQn3etDPYbRvAjaQCsFY5B7YqnpsLZrTs1JJxLY5Yhy+He95IWSueBpd+C7Q\nluk9Bjllj0dE9wSkcd4EOK6JYynMZi+nskZ/HTH8NUdR6o2xlNoYWTp3Jlonh8XxPIipT03EDFJa\n3oEsWcqleoNCTprfRA5y46qu/ZQmmSfEIvwicFEcD0BSnqsRE/MiFUdDa6/vfeDZnR6baVl1PhZJ\nccf3sK0RyKTsHSo+BIOQGcZJ8ZzqisAT7/mflHJMIKnrVMT8bFF1/wWIcShMpiYi06fvUKPPByKQ\nJqIoOadRCVG4fGzCNUvtYqO+uSBeqEiYByCziyuQmVyvzJNSu5chSXLZR+bgeE8/ISTkjehXtDki\n/psrBgHzCGFagjRXi9GNxGWI0N4aRbw6v3R+m/hP7hTtD0NE2AblcUW/tkVMVK/5piBtz9Xx/1gE\n2ZEfjXx6tkCReWar+s2eyCylyAJ/ItIsLoMk1k/TgCR5tGVcOjIBK/owW/VvWlFiTbgeMVirIW31\n3rEe3EOFmTgT+VC1m0+jNC/GI8HBOh1ce5IG51Ip1T87YtDWQszD94DdS/Pm19QQLrU3xkJbRuVq\nJBwowlh/F2kJC210r0Shy9I3Sss7kCVLUao2ta2CMCiIst1jgVwmNtljm9V+aTHcAyUMK0trRyEH\nttVa/bw6GMO6QYCsjkyyjqBCUK2JTIA6Nfep5f3E8eeCwLmQKulosXl3RnSUnvespXMHxIa0ZAf3\nLkJFKzEuNrDt43gJRCTWba4SG/m0KN1iTqlEWik0NAUjMSY+ezvEa/G5aBArRxAhbUv3NqRPfNYB\nehZko/1bKs6pf0fMWrc1HoiZn4TCcB5UOr8t0iAujZjAO4p5QEVSWiTFamqyuar+fglpE8r+DUsi\nM7OLURjjasfq4UiKvEn8x/aK/9lbiCm8gTocbmuYJ12ZgJ1VzOG+UpCA6XZk6rhX6fw5iNGcGWlY\nPtNvwqm5dHwmsG95Hse4F0EMaVO0MMi/YFfgstK5LyNmYp+ir13U0atjQSaEryJfwQORWemxce1S\npGkcQB8LOJKluaXlHciSpbogYvI3SLL8KBWpx+7I1Oi3NFhtWtpUV0cSriKk6FeR9G+TRrbXjL7H\n9zHAS0gbcUks7oeWrncrOk3p+UyMzWoKChs6K9IWnEMpMkhX9Zfqm4SkW2dTCTP5DUpx3qvafglJ\n1X6EtEa7x+9/hUxZ2jWXqXpG83Rwfg1kInE8JbOOGp7NWKRpGIyIvpuoZGr+KmLeeiVhV+lZTUCE\n6hnI/n5upH34BiX/jJ72CYWtLNpcB5nijI95MQeSqC6OiOcbqJEBpxRvniqJMhXNxPeBI0rnJyFG\nbt54hzcQ0lzEYPeaDxMykzOktStMqYZS0Z4VGdrb1XghIcbjMZdOjnl1MpK2N8yEkj5gAlZnf8va\n1UXjf38hbYURP453/RliNv7XO6NgEAWDeTRwSDHv4nO5mLNFpK1GMxHjEEF+DDLdPbZ07WvIT6tT\nbWhvjAVpdjanrdb33OJdxDiuopIhuyXmm1laW1regSxZygVJzX8ei2Qhnb6CULcj4qRZjtWbxqb0\nMDLZKUII7hiL/qatfj6d9H01KlKsE5A2Zw1EkH9CiZnoQRubI8ZuA0Sk/Q4RibMgxuIC6ggjG/U8\ngaRld1GKQBMb4gtUtENjkDlDId3/JdJGjEJE+gbUkGAKmUCcStsoRWVmYkWqfCy6qO+A6Pc0ZLqz\nQvT9dcTMPVlLvxo8F9ZFzO9Xgij5K3LYnDf6eBgdmHvU2U7ZAXo8Mvc7Azn0H48Yi8PjXb1CjVl4\nkdZhvSBSJiETloJQKpiWmZAW4gKk+Vgx5tKScX0p5OPycPTvWbqpiavjebSXGf1rSMJfdm7/MlVm\nmu38bhhiUAtGaEfElHWZzLGO/vYpE7Bany9iki9AjNoKMd/2p63dfocaG2RiNlu8lyIXxO+QtH3m\nqPNx6vCNqnMchb/JV+N4MeSEX05uWtNzb/ZYkGBt5WhjALIIeIqS70asKVvH99REzIBlEIlE38LD\nyMlwc+RgvYyZnQhMMbP/oXjg3uhGzWwhJGn/urvfb2bfBK43s23c/QozGwz8q9HtNhD/AfY1s2Eo\nKtP+iBj4IpJuPtSTys1sZkTcfRlJt2ZHG/g1SDp8OpL2vldDXUZFmrUr2lhHIP+CA81sAIqUtADw\nsZmNjPEsjAhIkNT0WmQG8HV3v6uGdndEZnFbuvvfi/Pu7ma2BvChuz/RVT2l+lZDYx+PCJv/Ak+5\n+5NmdjeKsf89d3+t1jq7AzMb6O4fl07ND1zr7pfF9WcR4bIBMi36t7t/2ICmP0aM5erIDOdAd7/N\nzFZGDMDSKFjCL5B09LEa6/0EMad7I0Loq+7+bzOz0n/f3P0aM7vX3f9iZkshRmK8mW0TY30LMVEv\nAXvG/9qatH58Wq+ZbYvm7qOIgXoC+LqZ3YAYnAMQY9Ah3P3fwMNmNsDMdkOanh3cvUdrUNFPM1sa\nEaFDUG4ekMbk34hpu8bM7nP3d3rSXiMR/V4XaT93j+f9ZKzVJwHDzOx8d3/P3Z8t/9bMZkImQM8i\nTcbsyHfnJKTtOQsJA/6BTKKmuPtzTRrKEkgjtZGZ3ePuL5vZOOBpMxvs7se4+9sd/bi3xhJz5SEz\nG46EAZcj07qzge3NrEhYOAbtObj7J/W2k5gO0GpOJkuWotA2e+j+wNnx/StIfVqzpLgbbY8ALkLx\n7gvJ13lIUlS2b26po2F1P5DkaRyS6g9FTsMHI+Llx3FucPk3tdZffT/asBZBZgOLxz1PIkl0l5JS\nKqYqg+JzMNpQb6Ti5PwAMslaNtpaEJkwLYGI9aOoaCWGI9OG5bp4RkW7pwG7FG0XfYlx7E/J5KmG\nseyDosQci6I83UbFL2QDGiDxr7Efw5DmaSTyEVgXmXvdUnXfD2iChJWKmdFvgEtL5zeJc7PWWV9h\nQjEB2VtfjTSFw0r3zIWIpnK40hHxHn6LnIaXRkxjU5NhleZY0e/9UaSxfZBGbTJiNL+NtG63UIf5\nB2Kcd6GBYWrpQyZg3ej70UQGb9rmQVkR+UaM7uB3i1Nxvr4DmezMHevJtTGnRiGpfrG+NG2tBzaO\n/hxI+EEgpqDLKE29OZbiP4b8aO5AUd6WiP/8PfH/zDCvM3hpeQeyzJiFtjatC7VzfSkUXehn8dlU\nZ79YeG9AUtXi3EZITfwUdRJETe5rQbxsHgTBxcjxsMhpMRoR3X8iIq7UWf+Q0vfxKGzlhnE8B2K4\n5kCmK6fQhb8KchgdGd83RgT9F+J4WGxIe8Y7v5OKf8pwRIDdHO0tGxvoEVQS7HWUnbZsrlTYBh+M\nTLDKROkW1JkzARGHVyMm7iHgmdK1PWMezdJLc2FuJIW8CkWsKoIT3B//nbmpZH6uKWdDd+ZLPMfz\ngcPi3BjESNTt2Bz9vRM5wH8FSVgL08Y5Yx505FtQOHYX0ck2bPLzX6z0fT7EvA9EkZXujPMFkzEL\n3TBN6miOd7O/LTcB62H/90SChnIo3LURU/SZZxvzsEimNwV4H5hWuj4/Cr97Gx0IJBrc/7KPx5bI\nPPPw8nzuZE3rlbFQ2V/GxhqyRxyvh8zriv/iEGoIqpFl+i9p2pTodZjZQGArM5sfmR+sbmaHufsH\nJROBF1FSqU2BI9395Sb2x9z9PTM7CviJmc2HzJi2Rr4GeyKi9h/N6kMtMLNZ0Ub0dzMbgTb+A939\n3ujzxWZ2oLufBexuZku4+x/rbGN24OdmtjOShl6KNAbzm9lq7j7VzIYiZmAikvB3aA4UavGDgMFm\ndhdiPK5GpmoLIMLrRMQc7IvyWzwSpioLICJnCFKn74cYpAOAbczsdeBDwKvaLJuZ7A9MMLPJSEK8\nFzDRzJ5DTojfRO+51uczH3LyvcfdHzezs4HNzWwqyqi7C4rF/89a6+wJ3P0dM/szYip/hp4H7r62\nmf0Mhc5dGjklP9mTtsK87xN3/9jMhnuYR7n7R2Z2J5IQ72dm2wH/B5zqdZrGmNlyyHzuSnf/jZk9\ngZj8sWa2OmLuJ7v7mx1U8bGZrYLmy1Hufmd3xlpDPw3Ny/vN7BJ3P9zd/2hmf0OmXB59BZk1/dbd\nn+lOW8VcbhD+QwtNwBqAp5C520Zmdj+Svn8XaSnaMweaA/CYu5ej/8eqZrabu1/o7m+Z2TVxfkgj\nO1oyI1sV+Xrd4+6fmNkAd//E3W+MeTQBMZ9Ap++76WMp+mZmm6PADO8DB5nZh+7+EzM7FviumQ11\n9x/X0OfEjIBWczJZZqxCmAkhwvwV4C9U4pOXTZuakvmX9h0ijYqpy6KIcTgeST7XR46rvRYysoN+\nz4LMOQ6jEsbyFtomxNsaOLmzsdbY1lS0YZ9MJcPz2GhvzzgeTVVUpY6eNzJzmYoYkiLZ3OrIqX4/\nJGWeiUqkI0O+E3fH95Eogtdlce9ydGCmQymiDWIaHqSt1HgLFDr0RmQWVXeUEWQ68xdE0ILCjZ6D\npIFNcdDsbB4jomEsMrOaQkWjMxIR4T2OPENtDtBDkbbmR1QCFdQbHazITv09KuZuwxGjNIUaktgh\nJ9OFezrmGvu7EHKsPzGO90YZ2YvM8TsizcjCzexHHf1tiQlYA/pdluTvHPPjPqT12rqd+xemkm15\nFCEYiuNtUDSsbZH5zzdpXgCPLdH+sVY714p973Nd1NH0sZT7gJize6hE0dsJCX8Ks9D1gTVaPSey\n9J3S8g5kmTEKIgiHoQyeJyH7+B8D9wKnVN07K7KVn50mRIFAWo5zgUNLG/6nycNK962JpHS9GnWn\nk35vjTQBhY3wkci0qbBv/gIyqxnWHQKqauzfAP5IJZ78IGSD/2siK3ad734DFFLyxtIGOhbZ3R5G\nhRhdE1gvvk8DfhLf50LRln5GB+EvkRP4AVRMSaaiXBorIVvkp2PjHRlzrNvEA2JIniIYo16eB+Vw\nuBci6eTYICROQyElD4zNvyF+RfH+t0DM5FtURfqJ70Wm4XqiXhVjWRAxAAOCULkSEUefScbVnbnd\npOdfrBkLIW3UUaV59/P4L/Z61K4ax9CrJmDdfL5z0dYMsWxyORQJM+Yv/6Z0fR8UeW/FOJ4M/I0K\n878VElS8SSRkbMI4FkamsQVTO6bU/gjE+HeaJ6LZYyGEaEioMrp0/loigl4cfwv5LG3V6vmRpe+V\nlncgy4xRqBB3i8XmemAcz4IiQhSO1esHUTG0we0Xm9PSSIp1KHJOe4pKZuaFEFE7eyywn6dJmbPr\n7HtBsIwvESd7UPEheCXG8gI9DFFLW2biMEoZdBExOZaQeNfz3OP72ii60xTC5wSFrV21NMbfIN+O\nc+NdnUgl8/JCdKKpQvbfs6MQhbMhjcSTyMdiN5Q59lYaR1xvFpv3F1owJ9ZCJipbIpOwV2N+LBDv\n7W7akdT2ZE5QpwN0HfVvihjiS5FgYY44dymS6Pc5/6TSfCv+G/MiZqKI4b8A0t7U7MDfy+MYiJjs\nh1AUs5b3qap/W8R6dh5VyUeJkNM11HFw1FFo6CYik7siKducNJjJo7LPDEHMwjXI9OoUtHY/CByH\n9pdO80T0xliI5JBII7w0cFwcH4T2loJ5WRlpKfqsI36W1pWWdyDL9F/4rLRo/iDwjo3jBZBz6D3A\nY5SiJDW4H6sFwbVL6dyWyFF5XmTW1KcyuJb6WWhH1kVmPj+gkjdiYoxjnQa1NbD0/VtB5HV7w6VC\niBaaiVNjYy07TC4Qn6ORpO5cRPTfRCkaUGf1x/dZkdnDKUjSNpqKpHw8IrA7NSWoc2wbN2u+dtHu\n14Dvlo63RJqCheK4IBAaIr2nBw7QXdS7CGLm10HE1zGIMR6FmIjL6QZz0gvP/zAkxf0ZCnE7PNax\n18vvpS8XetEErM5+LRHPdCISPvwU+dsU148hstvXUNdBwO+pEOCbofDCDTflosJEbBp9nIuK8GcC\nEpptQhDr3ai/oWOJ9/8Y8KU4Xhz50OyPtNpnxv/vQuC5uH4hsHqr50iWvlVa3oEs03ehrQRvf0QA\nH4Ck0M8jR2qQqnpPIupMs/qC1PgPlc7NjGzv645u1MvPcTfgvNLxjshGeH96ZqJTbH7LUoqeRVtm\n4gRKyeFqrG8dYGI75w05oX4/CAZDkrvHUWjHrZFZzgpRpsVmuURXxA4ipifEMzkY+XgU4QsPjU2z\nX2ZerR57jPVy2obAPIcmRJ5BPinnoXwdxX9mXyohKJ+jzshXpbrnBi6O7wXDeTZwcHyvmznphXcx\nDrg1vp+PmInCVGjBeB5zdjVfs3zmuQ5AQqa3gB/HucFIw3glyskCJROnqt8Xa8zKyK6/8CvYFRHg\nhd/O5kQm7wb2vWh7fLS1TgfXniQ04DXW19SxoDxDv6eSCHRx4F1g3zheHfnULI3W9Bfp43tllt4v\nLe9AlhmjIDvPe2Kj+FsQeZNiYZ3apDaLxXh5YN3S+SdRluTZEEPzOp1kQm3R86qOTz8WSYTHle65\nBYViXaiHbW0Wm8nKVHI8VBOui9VR3yREtH8mJjoVx99Zq84vgiRuJ6DkSr+k4pjfrkSatkzq9siO\n+ERklzwFMSTT4j1PoI9qm+p4rpvEmLaL458jYn4F5Aj9e5rAKNGn/bZOAAAgAElEQVQAB+jqd4ZM\nKSzqeYTQrsW1A6nTD6eX38PqSCt4LNKaDY3z68bnoFb1rT+WdtaanYE/EA7KiMFYHvlYdfofjrXn\nORQitjArGo5yq/wRGNtRu93s+/yUHOnj/1gQ4QWDPzzWt19QMUXqsu1mjiWeabG3bI009UUgjCWQ\nmd6xpfvXREK4Pufvk6X1JcO/JpqOyEy8MiL2vogIhwWQ4+txwJFmNoe7/7WR7bq7m9nGiAD6wMzu\nQ2YHK5jZo2iRvgyF63y2s7p6G9H39YCVzexJd78rsiVPjBCkzyHzj2Pc/fXutmNmi6B3sLOXMg9H\n+0UUkH3Qxl5LfaOQb8Je7v67qlCsswPfMLPzyn2OzMx/MLPzkGnSUMRUHG9muyMGoTqsa/n7gijc\n5jh3f8XMnkImWdcgO+L93f347j6jvgAzK8w9fgxsYmbLo/9PkdRqcSTFf6oBbRVhKxdETP9lyBdk\nD2A9M7vLlRn8liht3kdniHq3RFGfXkUmjVsAj0Q46NdQCN3DejqOJuIvSEszK9K6/cfM9gG2M7Ot\n0JxL1IDSXBuHzOZeQo6+HwCXmNlO7v6AmT2DMpy/30ldQ5Aj8m7xmyLD+q7ufk6sPzMV99cyX7vo\n+0BgQ+AxMxvmykb+Llq/QMT6x8gv8L8ocMXfavmvNHMs0f4nZraImf3V3a83s/eAC6Nr15jZ+sBv\nzewKd38p+rCJ1xlOPDGDoNWcTJYZo6DFdQXg13FswHvIRGJEg9sqpJ6DkK3qckiSczayzy8ifdwB\n3Fz9uxY/p0KKtQ5ybP0OIua2QZKiXVGG3DtogDMtYVoSz2oIlTC4hTN0p8nm2qlvNiS9XqN4B/E5\nX9Fee++LquhcyMxtlc7eb3w/ADmMPoecqYuQpNsh4vdI2on80x9KaR4vijQq28TxPMiv56Tye2xw\n201xgI45fAMiktZC/hDfiPqPRQ7bDTU76cEzWJWSGQoVCe4AZDr3feAMZDb3NH1Mq9lfCvKFeDbW\n5vNjfswDfAkFXhhXYz0DkU/VUaVzXwF+VnVfIxP8DYo170bEMKyPNKIrIRPAFeJ/VFdY6GaPBZlG\n3oP2xDOQCdmWSKu5Q9wzUzHGVs+RLH27tLwDWWacgqSm9yHCfguUjXehJrU1GbgiiMyJcW7+2Py/\nT8XJ8DXCHrfFz2bO0vcxQbBNiuMNkTnW9nE8jG7mBqBCnBY23aNQ9J+tSveshUJYthtmtaP6qDAh\nR0dZpFTfzUS0JNoPv1sQaTX7e8Q7vhQRp6cgB+DxVJiXbWiBI3SD5kPxXCcgJulZ4CdUmODPIwK/\nsCVvWJhkmuQAjRiiN4gIbaX59zRVcenrnddNeP4DYi59nrZZh4u5NSj+l0cgZnXJVs+Z/lIQ07ho\n6fhUYLP4PhcSLhX+EPsT4aDbqaf4jyyJ9pShsdacS8X8b1VE5H+uUXMKaQKWie9jkNnPt1E0s9lR\nQsXb0f72EBGmtYs6e20sKJv5I1HPd5HJbCE42goJYD5PyU8uS5bOSss7kGXGKbE4Ho6inTxLk5J3\nIee8e5FUukiuVjinLYBMRMqJ3FqaLAoxBscXmyuSxP0GEcYj49z6yLzo6w1obxIi7M9DDN1ySOtx\nFJLwP0MN8cJLm99WiKC/DhH1qyDJ8j2xwb5MhZnrKvzuodSgQQDmQ0TphaVneAJiEjdmOpCiIafe\ni+KZrRfPeA9g3rg+DyU76Qa22zAH6NIcKZIoTkVBFkaX7jmDJsXy7+b4P/VPimd8D20jvQ1u7/4s\nNT3bQUjIsDAV35KLgHNL96yOHNiHls5V+1EU72gTtJfci4INHIHW/d8ik7yXaXB4WyQQKwIN3BHr\n1tyxnl2LmKFRSEMxpqs50htjoa0Wd/FYKyeifE3FvlPskQ0Jj51lxikt70CWGasgFeoCNCnyAyJk\nf1q1MR0APEyErSttYF1K3HvpmQxEoQHnpxLHe6vYTL5GJZTnBvQwxGtsIrcjZmUr4M/xfVFkWjKV\ncJKuhUBCjtqPxGZ6C1KNj42NdBJynCycJhsafhdpHN6mooofhPJUnEao5ftribFcAPy1NF8nIsfL\nAxv5/ykRMg11gC7VuyWS1hbalDPi/7gdYpDf6um8bvSziO8LlubZ9URyxji3PhXGKhmJ+p7xsPi/\nT0UMxULxfL8R11ciAnN0Uc8qyIF5iVhvto3//0aIAdyQiGLWiHeEtA/rx/cpwPvAtNL1+ZF26jbq\njJ7WzLFU/Q+vQhHF7kGMSWFyuikKHJDRxrLUXdLZOtGrcPf/ItVpw1DlvPY2IlaXCwe+B939e2Y2\nFDmTjUOOfEVfWoro+8fhuDcfsJKZHeXuJ5nZMKTeHmZml7n7XaXfeGf1dtDWMkhrcJ27XxXnnkfq\n8i+4+5Ty/TW2sTKwN0rONTPaqG5ARNdNVfc+jJ79XkgKCVKrb4c2r1fqGY+7X2dm/wGmmhnufqWZ\nfROZR/2rnrr6Gtz9f2Z2KJoTP0X+ML8ws0EoaMH1DWyrKQ7QUe96iOja1d3fivMHx3v6PjLX2trd\nH+7uvG4kivbNbH/kPL0ZiiD2MbBbdPEyxIBfU/5NonOU3u9A4H9obu+C1qRpwNmxPi8DfLOYLx3U\nNRzN0fUA3P09M7sHrZcrufsdyL+CuN6IdzQH4GY2GJn3fQisama7ufuF7v6WmV0T54fUWmmzxxL/\nw1VRCPFT3P1dM7saPecdzexpZBr6LXd/t566EwkgNRJZ+nehIm1ZE0nYN4jjKUj1vEbpntGt7m8H\nY1gESeeGIee8n6JFHeRg90O6kM7V0dYlyPlv9tK5c4A163zeg0rn5kEO4IvF8W+QRHtUHDc1/C6V\nDNNfbPW7bMLcGIWIlqtL5+ZscBtNc4BGTsnTkNR5D0Q0XoDMho5HjqmzledWqwuSBj9Kyb8GaWsm\nIUnutq3uY38rpXVjyZgP8yIToB/GWr0wMn0dQ2Sv72o+IA3ABbGWFOZ+u6HgEYMbNZ+ib0Ueh1HA\nv4hAF0hbdVPMmSLKXd15fZo5FqRhPA5FEyvW6HmRtuNS5CdRmJf2if9glv5VWt6BLFl6WpBatpCq\n/Bb4UZw/FkUBqYlI7uU+FxvrQMRA3EmYjiAp/0+AE+J4nga0V04wdzmSsk5ADrVvU+XsWsPzPhk4\nrBgLYlB2RJloL6YStWljZBP/CDJpKcxbHkVStlOB8Q0YX0syTDd4LnSUL2MUIvRviONGOlY31QEa\nMfJXR30HBcF1FpVgBxcgO/OW+bTQ1pxpZuQsWyTfm6l0bSiSHPe5JHn9oSCG/9aYb2eirPNzIf+b\n0+mGzxxKADgNJUrbD9n815zXpMY29kEhqFeM48nIp2xyHG8F3I2EGd329WnmWBCjcj7y4Zi36lrh\nB5VMRJZulZZ3IEuWnhQk2byKCI0Z5x5AYVOHxAa1VKv72UHfN0CRpVYFRqIEY1vEtdWRZqLmRHA1\ntFfWIlwYG980woehxjoWQ7a1OyEG4WzEDO2Nchy8TkW61e/C77ZwLkxEmqIFqTAWA0rXZ0ZmDo1o\nq9ccoGMOjKSSYXylaGv50j0NDV3bnWcR3/dFzM4hKAjAbKVru9AHBRL9paCAAc8jjcQEJAU/CUnG\n50EMZVfJ5lainVDhUcePYt3fsph3De7/wcALwKpxPBFJ+IsEc3NSR7K2Vowl1pYTkeN2ZqfO0rDS\n8g5kyVJvKRFC41GCu+8Dm5auLwVcFN/7VAi7KsJlfaQNeCCIuZ2QFmUAYoJG9fD5LEiVORcR9jW+\nXwDc0V7fOqhvGeRIvXscj0LhDU9C6vchVFTn/Sr8bovnxGooUtb4dq6tABzY6PlHgx2g6VqrMjTq\nfYkKs9xnImuhvCW/o+J8eiIyM1kGmZg8TYZ4red5LkgEjojj9avWmlWR9vj7KOLRZ+YC8qEowkMP\nQaHDFy9dLzPacyGhxU00QIPbwZgOQsEkCmZiM+ATIthDF7/t1bHQMaMyGjFxV1PaC7Jk6UkZQCLR\nz+DubmaTEPHzBpIU/TAyPoMW7dGRZdlb1M12EX1fy8xWQurw/ZCUfiZExB0NbOjuH7n7ez1oYxLS\ncNxqZvua2cxx7SMzGxPfdwf+bWbXmNkAd//Msyplnl0fmV+dAuxtZqtF/zZGqv2zo88vh1P3Icgc\n5y5gmpmt4nKePAUxHUV/RqNQhDMyRqFEU3eb2RAzK6/L7wOPdfC7uhHvcj1klz7VSw7QKOTm94HN\nkQ34fWZmddQ7Ec23BYvfFZ/u/h8kwf2qu98c5/7XqHH1BOHsuhki3v5tZnvFpbEoCs8klMPlhRZ1\nsT/ij8ANkR0dxKT9zcx2NLNB7v4ICgM+J8qX87925trGKJDC+u7+EXJ4f7/4f7iyM48xsyPc/S9U\nBBc9pmtK83dlM9vJzFZw9zORBveKWM9uRXPj7zVU2dSxmNl8ZrZOfB+C/APnKV0v2nktxnBM9COR\n6DEyalOi38HMRqAMz/u6+0PAQ2Y2B3C7md2O1M6HdpcQ7wWsiIi12xDT8w4iWOYDvoVMhepGiegf\ngAijHaiYdw00s4sRYXq0mZ3u7k+4+xZmNq+7f9JenVHfGsgxbzIiEHYCvmpmn7j7I2a2JrBs9GEJ\nRJA94+5XA1eb2Z8Qo7efuz9kZge6+3/MbLC7/9fdX+3OePsr2olO5MDXzOxid3897tkISQx/gZyf\nG4mVUcjdv5vZHsjn5W9IKj8ijr9TT4VmthpiEvdz9zeK8zF/VkDmc+c2qP8Nhbt/aGa/QATWWygB\n4KvIl2gK8F/vAxHe+gMiytcOwA/d/Qkzu9vM3o1zt6Iwpyub2R3I3+QyYLKZXeERac3MFkMS+ouD\nKD7UzBxJ8f9SrFVxDTSXcfdXzGxaIxjUkjDmZKSp28rMXqeyPt9oZpPd/ZboS7sRx3pxLBsDXzez\nY9z912b2KaPi7p8UjAoSEEwDMjpTomFIRiLRH+FIkjUCPl3Ep5jZa8iJ9wp3f7QvhJNsD+5+jpk9\nRCXR2N4octP+7r4zdC/Ea2x+WyAH6tHAv9z9BTM7CplqDHX3U81s15AADnL3/7n7211UfQRifi53\n9zfN7CbECO1jZj8EHnb338a9/Sr8bm+jxOxtgN79yyjc6hnAuWZ2HNLWnIWS8zUDDyCN0c3IV+YK\nNGcWcvdvm9k8wM/MbNM6CJk2WhXgfyXm9H1kGtSXcSnyUXnF3f9uZjsik7MZcp72AINRJKNBRE4a\nxECchZKRvoSc2b+KGNfBiAguS943Bs4xs6Xd/fzQDpyIEjQubgqB/Gekxd3f3T8o/leN0nLFHJ4M\n7ObuD5jZykj7sGus37NH+0CnIVmbOpZuMCq31f80Eoku4H3AvipLlnoLsD8KabdUHK+JNqyWOW7W\n2O8h8VnYlC+EIig9RA8zbCOtwCMoBOH1wJWEjW08n3u62wYK2/nT0vEKSPOwTKn+fhl+twVzYHNk\nrvQVZN5xKvA5RGjdjQj8hkaeqWq/xw7QVPnTIILxVcSMlM9NbPXzrvPZDKDiE1Gz82yWNtF/FkcE\n65Q4HhTz+gele4fE/+BRIhpSVV0HIal5sb7vHuvXUchhezlg7SaOZSDyUTiqdO4riFku31dL0s6m\njQUJoT4h/HcQc3Z/nPspynVyNgqEMXOr50iW6bO0vANZsnSnIDOg45HkZSoy/9i81f2q6mPZSXkU\nbZmHe4koJcCslBzvutnWyrFpHBjHo+O5/ISKA2ndztu0jfR0G9JKFMdFxu1+F363hXNiAeDbyPF8\nY8T4zVO6PhAYVp4/DZh/DXWALtW7AWIYd0Kx9g9A2c3HokAIzwKbtfqZ1/nMZkIRmvpkpLe+Wkpz\nosjCvkDMhTIz8VDV+rEDnURqQgKRd4mwsIjB+zXyIWtW/wvCfijKqXIusF1cWxUl7/xcvf/NZo6F\nFjNdWbIUf55Eot/B5EA8FkX9eM3lL9GnYMqMezbKHP1E9Pk84Fl3n9pd86vq35nZwohofxfYx93/\nYWYLobCF8yJp2sfu/nFXdZrZ54F3ivoLE6j4fi9SmW8bxwOQ5uMqd78uzj2AnKynILvzC9z9+XrH\nOL2g9FzXRsn/HkeE1lBgR3d/w8w2R8TWzZ29o260PRH5O2wFvBn9sNK7XQVpyR6os97NkZP86cDO\nwBPoXX8d+ee8j+zkb27UWHoLfdUksq+iNL83RmZLTyKm4VWUwf5+dz8xzHhWaW+dLtUxBknOH4vz\nhyLTyrVdZpr7Ar9z94eb0P9N0Hz+GxKMvAn8ARHqf0Da1UPc/cYa6+u1sZgyxR8GrOfuz5nZbmjN\nP9Hd7+xp/YlEp2g1J5Mly/RakCToeSqJjOZHfhGrle7pTpKvcgjZVdAGtwAyV7kJaQKKrNKj6SI+\ne7lO5AD5MJ2YIRX9px+G323hXFgd5c1YE/g80kQcHNfWRlqB9RrcZldhZffpZr29plXJ0j8KMmN7\nCtgEmVVeFucXRdLxE6rut+rvsfa8gAQtj1LRpB4M/IduJKyro/+rAL9AWd5HocSJp8e45gE2BJar\n7ns79TR9LKU2xgArl84fCvyVipnTvsDYVs+NLNN/SWfrRKJ5+DcKwTo2IoBsjTaXS0DSfO8gWlJH\nMLO5o841zGx5tGnfiza7a1B26UuBIyLqx2u11OvubgofeDIK0fmamY1ETrP/irbGu/v33P13MZ7j\ngb2ohN9dy93/SNvwu/+sZ3zTMUYhM6K1XM6bpwEHhJZiCcRU3NOENhviAN2BVuUSpFXZxt3/VKVV\n+Td06oSa6Oeo0twsjkzcZkEM5jZx/jWksZqz/NuYS0XUtmLtORIxIishzcY1Zraju58R2oz5UESt\nRo9jOCL814u+vWdm9yDTppXc/Q7gT+W+t1NHr4yl9D/cAjgNuMfMVkXJ604zRWt60sxWcvdz6q0/\nkegOkpFIJBqE0iI/F3J2ewP4H9qkLgUuBvZAEZroBhMx0N3fMbM3zOxF5KS9nbs/aAq7ejEKJbtf\nfJ8TqCkEbhCaM8XvRppi6e8C3Gtm5wH/QCYL00P43V6Hu//SzLYBTjWzl9z9p2Z2GyK6PnSFe+yR\nSU07v3caFFY25vXqKMDBXshs5WZk8/5GMBhnAl/3BppmJfouYk5sjNYGA65C683m7v6XMOtcwt3P\nQgzFpwhiejszewUJGyYjaf3iyLZ/TmQWdZeZbeDup8bvGm525gr/eyH6L55sZvu6+9tm9gywjpkN\nRkx4u+32xlj6CtOVSLSHTEiXSDQIschPRiYslwN7u/u3gMnufi1yqp4IvFhv3eG3MCU2n+2QCdPe\nxH/Y3X+PQiyuGVqBrdz95RrrXhoxDf9A5ljTgI9QxuqZgbnc/Y2SxNxpJ/wuij50EfAVd7/FrLZk\nZjMK3P3niMD4tpnt7O7/cPdn3P2VuN5jJsLMNjCzKWa2E2IUirCyY81sPJoj3W2nrFX5M5KIftHM\nrgV+QHO0Kok+ipCEHxGHlyMN1wPBRKyFTIPaXetcPlfPIo3qHSii0++Qmd9N7v4h0rC+h9bN4ndN\n0XC5EjOeEP39tZnth/x9rikI+E5+29SxlBiVNUzJPttjVF5HjMrC7n6qu/8q199EbyE1EolED1Ei\n4gahePx7o9jgd5vZXO5+VGysU5Gt8F31tuHufzazy4CFzOxjdz80TI8uN7Mxriylg4BlzGwYssOt\nFfMD30B2/HsDg13O2ksgh+lLqvrygZldBYwzszfd/XlTUrrtgdvc/Z24L81aqhAM1iCU5faXwJ8b\n8Zxi/lU7QC+PmMLhiMl7HzjclZG3O200XauS6B8wmVheCdzh7g+HedA5wG5mdh9ai77p7p3lLXgJ\nMbtzI1+Kl4HfA5ub2ZEoQMDu7v54E/q/EvCSu79fnAvN2veAOZCUf6q732ylYBOtGIsr58+zKLw5\nwDru/rIp58xNoVG5BkVoajrTlUh8Bt4HHDWyZOnvBZiACLhLgQXj3PxIpX9SHC8Wn/WGDhxQ+n4B\nknwVznsXoUhNxwA3IE1ErfUOLX0/HkUnMRTjfQKKxNNuffSD8Lt9uQBzNri+3gwrW8T/37nVzzFL\n6wpy5v0HVc78KJjAHPG907mGmNy1kKP2NnHuO8hsc9MG9nU+RIAT69t9lEJuV62xc8V6elP5P1RD\nG00bC9IM348Ylk3i3I5IE3Qk8CDpWJ2lRSXDvyYS3URJE7EMSvhzH4qGcxtwrctheUFEdK3hYcLS\ngHbPRRGajnD3t8zsfBTqbzV3f6YWqbCZLYek1u8C30X2tlu6+55xfTVkF/xYR/VZPwi/Oz2jNP9a\nEVZ2K8RAbkiDtCqJ/gdTmNEDUVbmbpu1mZyHz0Lazw1RPpwnGqXhMrOvIVOlY9z912Z2N8pj8Y5X\nsj+PAbZ292lmtmhcv8hlKtrysYTWZ2VkRnicu19nZt9Ba/Dp3rn2J5FoGpKRSCR6ADNbAzERh7nM\nVrZC0T/eBG5w91fNbKi712Nq1FFbAwti0OQAPRT4dhCMS7r7C138viA8FwP+juzdN0Z5Jq5GzrJH\nu/t5Pe1roncQDtDfQRLUsgP0GcFgXIQcoBvuu2Bmc7r7u42uN9G30BUBHP44U4Bd3f3uHrSzJiL2\nr3b327tbT1WdiyFtw+/NbA9kYnQqIvCPK62nQ1BSxZk8zI9qNGnqqN2Gj6VUd1OZrkSiXiQjkUj0\nAOGP8CLwnLtvFucmIfOPV1COhY+8/ghNHSWHKzMTlwKDkaP0f+L+rjb9zVBs843d/cU4ty3BlAB3\nufveuSn1D5jZBGQ7fbgr/OP2KMP0n1BY2SO9HyaFS/QNlNahIjzqEaX1p5zYcHfgBXe/r4ftDXL5\nBDRKE7E30tYt7UoCtycKUzsOCU8GIX+2mZBW5YMGtt3QsVTV3TRGJZGoF8lIJBLdRGmjGI7sYu91\n993i2mTgRe9GRueqzftY4IveQT4IM1vW3Z+psd4V0Oa5Q5gsLYBMmwomZAVkc7uXu99fb78TrUFo\nwU5FWrGfm9mspAN0okEIZvVkxJTeVnVteeAtd/97HPe5uWZmBwFHA+u6AkPsjkxBbweuQ8KYUf1t\nzWsmo5JI1IMM/5pI1AizSji9WLz/F4v5hyhCzlgzuxLA3W/oDhMRvy0nh9srfC1GmtlM0fbyZrZ/\n3FsTExEYgmK9zxORRG5CkVdWjLqeBO5CmakT/QTexLCyiQTKYH+iu99myqlQxkRg6eKgL841dz8T\nMdr3mtnS7n4BCoqxMQpa8XR/YyLg07CzffKZJ2YsJCORSNSAkpZgfTNbpli8g5kYHMzEGsBqQeh3\nO4a3tZ8c7lcoj8RiKFLKU92o+k8oNOHuKO74pDi3fLQ7F8qQ3e821Rkd7n4LiqJ1uJnN05P5l0gU\niHk0H1rbcPf/xvmVwsxyWl8jwou5b2ZjzGxlAHc/BQlm7g1/sgtRbof/a11PE4npA2nalEjUCFME\nnDOBfdz9V1XXFgyn5wH1+kNU1bM0ykXxBAqvOAb5NPwNRVa6zN1/2416B7j7J6ZIS/9194/MbCng\nCmQbfH9swMOCKUr0Q6QDdKInKAlMlkYmP2+h3ARnoshf55nZOCTR39bdn2hhdz+DKrPQ04B7gFVR\nRLo/mtnBKLfKSu6emZ8TiQYgGYlEogaYEjD9AtjD3R8N2+BZgedQoq/7gW2RvXC3In1EOxOAs1Fy\nuI9omxzuamBP7yLMans2s8EkDCg5Sm4EnAhMc/cbesoAJRKJ6QNmtiWKAvYntLY9izI2n4fWu+WR\nc/8tLetkFUIrXGhL1kHMwo7ASsBlKOv2jq4oeocBT1QLgxKJRPeQjEQiUQPMbA6UcO5xYBmUO2FB\n4DR3/4mZzV44HHaz/k9DxJrZ8Sg861lIKjgeOAU4Nuzha6lvU2BL4A/AQ+5+X0krMRhFaRrtNead\nSCQS0z/MbBbgRuRz8zzyn9oPCTHuRsnm3N1fblUfq2HKFP8lFCXvn8CuyBdsJHASijZ1ERLObODu\nr8bvct1LJBqA9JFIJNpByc520Yhu9B7aYOcFrnH3ScD3gHXNbCA9sLU1JYf7jpkdHpviQ8BSLnyE\nfCJ2jYg8Hdq+l/q8NJIo/gE5WJ9jZpsHE7EQcAhUHLVzM00kZlyU1o2lkCnlIOAfIdh4BplZruTu\n/3T3l/oSEwGfOh0/C1yPNCc/cPffAWsDN4Wp5jVoDZ+19Ltc9xKJBiAZiUSiHYSd7aYowddUpIl4\n2t0Pi+gl44CDgJ+5+8f1mjOVNu/FgD8CD6CESNcBswFfMMU8x91/5+6PFf3qos+rAbcAP3L309z9\nJOBbwAFmNi8iEm5w9/fr6W8ikZg+EevGJOQv9RbyK/iumc3h7h+gPAuLmtngPuzE/xLSSLyPAkoA\n/B5YMiLUHQrs7pFsLpFINA7JSCQS7cDM5geOQ5vPV5Bq/OehoZgXOBzFVf9ld+qPzXszFG51Tne/\n1t33Qk6MjpyrV4y+1LN5Pwx8AOxVOncnyhdh7v6KRyK6RCKRMLMVgRNQfpk/o5DQ7wK/CGHG8Shb\n+n/7qhQ/GJ6Nga8Bp5rZNu5+BfAasD7KYv1w63qYSEy/SB+JRKIdRM6GcxEz8XoQ/icDs7j7PmY2\nr7u/3V07W2tQcrhSlJLlgVnd/d44/yQybdoVSeiuBSa6+7P19jWRSEy/CJOmw4EHgdmBDZCWdH7g\nQuBtd7+7ZR2sExGx6SzgEmBD4EB3fyJ9IhKJ5iA1EolE+xgEjAK2Km0+DwP/AnD3t+OzuxtTQ5LD\nBROxcdR1upmdYWbzu/sKyBn8OeSIuHMyEYlEoh28CTwCfBV4ATgA+C1wgbtf0Z+YCAB3vxllrl4I\nOKkIUZtMRCLRHCQjkUhUISRX/wccBexkZqeb2THIgfnXDWqmR8nhSj4Wg1CyqO1Q/onBwIHBTKyC\nnBCXKoiBPmzjnEgkWgB3f9/dzwbGu/t1wMzA/khD2i/h7g+gUNm355qXSDQXadqUmKHRSc6FQe7+\nXzNbBKn650BhVH/dUxV5o5LDmdlkxEAsCkxx91+Eb8fhcWTC0lsAAAhsSURBVMvpETf9NeAud9+1\nu31OJBLTNyL63IrIpPM77n5ji7uUSCT6AZKRSMzw6CDnwkB3/7j47EHdTUkOZ2bLAD9AyetWAjYH\ndnEly1sAOBo4uwjxamYLF/HTE4lEoj2EcGOuEECkT0EikegSyUgkZkiUnJSXBi5AMciHANujaEy3\nRM6FLwI/Bv5fdzfVRieHM2W5Ph74u7vvE+cOAHYC9nP3h4oEd1bK+JpIJBKJRCLRSKSPRGKGRB05\nF25y97/Xy0Q0OjlclZ3v28CrwIJmNi6Yku+hKFAXmtlI4H9RXzIRiUQikUgkmoLUSCRmWARx/jTw\ngbuvHudmBs4DDnf3P/aw/tVQNKXj3f2iOLclsC+wCzAc+WJ0mtehpD1ZE5gL+Ke732VmU1DyuiuQ\npsPNbLS7v9aTficSiUQikUjUgtRIJGYYlLQEy5vZui4sCwwzs+vNbDYUbnUdYNYGNNmQ5HDBIGwK\nnA+sBZxoZj9y92NR4rpdUeQmkolIJBKJRCLRW0hGIjHDoNk5F5rFqJjZAKTBONbdv+nu44BlzOw7\nwFTgfeAf9fY3kUgkEolEoidIRiIx3aO3ci40klEp9Xk88AXgL0QyvMCuwLzu/hFwmLs/X09fE4lE\nIpFIJHqKZCQS0z2CwJ8MXApsASwQ+RmmAcOAwyM86kbAsmb24+J3tdTfDEYl+jwJOAN4A2Wc/aGZ\nzRe3zAeMNrNRQDo6JRKJRCKR6HUkI5GY7hE5Fw4BbgDuAqaZ2Sru/hZwCiL4ZwZw99HACfXU3wxG\nxcxGIK3Dvu7+kLufA1wE3G5m3wW+D5zm7u91lXMikUgkEolEohkY1OoOJBLNRORcOAZ4xt2vBq42\nsz8h6X6Rc+HAcs6FehO3lRiVIjncNDN7J5LDnYKSw33KqJjZwjVU68CcwIhow9x9SmSpfhS4IurP\npFGJRCKRSCRagtRIJKY79GbOhWpGxd2PBH6EGJXV3f1N4MBINjc42umSUXH3D5CvxTgzW6oU/nV7\n4F13fzTuSyYikUgkEolES5B5JBLTFXoj50JZCxAmSEcDywEnAQ9GsrnDgJ2BcShPxcfdaGc+YE9g\nfeB+5HtxgLvfUm9diUQikUgkEo1GMhKJ6Q6Rc+FU4FZgbeA5d/+6mR2LnJQvcvcHull3ryaHiwR5\nY4G5gdfc/aGe1JdIJBKJRCLRKCQjkZiuEDkXrgSucvfr4twDyMl6CnKAvqAn4VKbyagkEolEIpFI\n9Bekj0Si36M3cy5kcrhEIpFIJBIJIRmJRL9Hs3MuZHK4RCKRSCQSic8iw78m+j2qcy4AD5nZHCjn\nwu3AROBQd3+vO/WXGJXjgb2oMCprufsfacuo/LMBQ0okEolEIpHo80hGIjE9oKk5F5rNqCQSiUQi\nkUj0RyQjkej3cPcPzKzIufCmuz9fyrlwm7u/E/d1N7JAJodLJBKJRCKRqEL6SCSmF1yH5vP5ZjYV\nuAw4u2AieoJMDpdIJBKJRCLxWWT418R0g2bmXMjkcIlEIpFIJBJtkYxEIlEjMjlcIpFIJBKJRAXJ\nSCQSiUQikUgkEom6kT4SiUQikUgkEolEom4kI5FIJBKJRCKRSCTqRjISiUQikUgkEolEom4kI5FI\nJBKJRCKRSCTqRjISiUQikUgkEolEom4kI5FIJBLTMczsYzN7wsyeMbOfmdlMPahrvJndHN+3NLMj\nOrl3VjPbpxttHGdmh9Z6vuqei81s2zraGm1mz9Tbx0QikUgIyUgkEonE9I0P3X1Fd18W+AjYq3zR\nhLr3Ane/0d2ndXLLrEDdjEQikUgk+g+SkUgkEokZB/cBi4Uk/kUzuxR4BljAzCaY2QNm9lhoLkYA\nmNmmZvaCmT0GbFNUZGZfM7Oz4/vcZna9mT0ZZRwwDVg0tCGnxn2HmdnDZvaUmU0p1XW0mf3ezO4H\nxnQ1CDPbPep50syurdKybGRmj0R9W8T9A83s1FLbe/b0QSYSiUQiGYlEIpGYIWBmg4DNgKfj1OLA\nue6+DPAB8C1gI3dfGXgEOMTMhgEXAJOAVYDPd1D994B73H0FYGXgWeAI4JXQhhxmZhOizdWAFYFV\nzGxdM1sF2D7OTUTZ47vCde4+Ntp7HtitdG10tLE58MMYw27Ae+4+Nurf3cwWrqGdRCKRSHSCQa3u\nQCKRSCSaiuFm9kR8vw+4EJgXeN3dH4zzawBLA78xM4AhwAPAksCr7v4SgJldBuzRThsbAF8FcPeP\ngffMbLaqeyZEeTyORyDGYhbgenf/V7RxYw1jWtbMTkTmUyOA20vXrnb3T4CXzOwPMYYJwPIl/4lR\n0fbva2grkUgkEh0gGYlEIpGYvvGhu69YPhHMwgflU8Cv3H2Hqvva/K6HMGCqu59X1cZB3ajrYmCy\nuz9pZl8DxpeuedW9Hm3v7+5lhgMzG92NthOJRCIRSNOmRCKRSDwIrGVmiwGY2cxmtgTwAjDazBaN\n+3bo4Pd3AnvHbwea2Sjgn0jbUOB2YNeS78V8ZjYXcC8w2cyGm9ksyIyqK8wC/MnMBgNfrrr2RTMb\nEH1eBHgx2t477sfMljCzmWtoJ5FIJBKdIDUSiUQiMYPD3d8Nyf6VZjY0Tn/L3X9vZnsAt5jZv5Bp\n1CztVHEgcL6Z7QZ8DOzt7g+Y2W8ivOqt4SexFPBAaETeB77i7o+Z2VXAk8BfgIdr6PIxwEPAu/FZ\n7tMbwO+AkcBe7v5vM/sR8p14zNT4u8Dk2p5OIpFIJDqCuVdrgROJRCKRSCQSiUSic6RpUyKRSCQS\niUQikagbyUgkEolEIpFIJBKJupGMRCKRSCQSiUQikagbyUgkEolEIpFIJBKJupGMRCKRSCQSiUQi\nkagbyUgkEolEIpFIJBKJupGMRCKRSCQSiUQikagb/x8nQSKSAQJihQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff1b5aea748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 9))\n",
    "plot_confusion_matrix(cm, newsgroup.target_names, normalize=False, title=\"Confusion matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
